{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea09b84",
   "metadata": {},
   "source": [
    "## Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9132e952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neehanth/Documents/Masters/Adv Database Systems/Project/rag-qa-system/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pdf documents loaded: 49\n",
      "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 0}, page_content='Efﬁcient Estimation of Word Representations in\\nVector Space\\nTomas Mikolov\\nGoogle Inc., Mountain View, CA\\ntmikolov@google.com\\nKai Chen\\nGoogle Inc., Mountain View, CA\\nkaichen@google.com\\nGreg Corrado\\nGoogle Inc., Mountain View, CA\\ngcorrado@google.com\\nJeffrey Dean\\nGoogle Inc., Mountain View, CA\\njeff@google.com\\nAbstract\\nWe propose two novel model architectures for computing continuous vector repre-\\nsentations of words from very large data sets. The quality of these representations\\nis measured in a word similarity task, and the results are compared to the previ-\\nously best performing techniques based on different types of neural networks. We\\nobserve large improvements in accuracy at much lower computational cost, i.e. it\\ntakes less than a day to learn high quality word vectors from a 1.6 billion words\\ndata set. Furthermore, we show that these vectors provide state-of-the-art perfor-\\nmance on our test set for measuring syntactic and semantic word similarities.\\n1\\nIntroduction\\nMany current NLP systems and techniques treat words as atomic units - there is no notion of similar-\\nity between words, as these are represented as indices in a vocabulary. This choice has several good\\nreasons - simplicity, robustness and the observation that simple models trained on huge amounts of\\ndata outperform complex systems trained on less data. An example is the popular N-gram model\\nused for statistical language modeling - today, it is possible to train N-grams on virtually all available\\ndata (trillions of words [3]).\\nHowever, the simple techniques are at their limits in many tasks. For example, the amount of\\nrelevant in-domain data for automatic speech recognition is limited - the performance is usually\\ndominated by the size of high quality transcribed speech data (often just millions of words). In\\nmachine translation, the existing corpora for many languages contain only a few billions of words\\nor less. Thus, there are situations where simple scaling up of the basic techniques will not result in\\nany signiﬁcant progress, and we have to focus on more advanced techniques.\\nWith progress of machine learning techniques in recent years, it has become possible to train more\\ncomplex models on much larger data set, and they typically outperform the simple models. Probably\\nthe most successful concept is to use distributed representations of words [10]. For example, neural\\nnetwork based language models signiﬁcantly outperform N-gram models [1, 27, 17].\\n1.1\\nGoals of the Paper\\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word\\nvectors from huge data sets with billions of words, and with millions of words in the vocabulary. As\\nfar as we know, none of the previously proposed architectures has been successfully trained on more\\n1\\narXiv:1301.3781v3  [cs.CL]  7 Sep 2013'), Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='than a few hundred of millions of words, with a modest dimensionality of the word vectors between\\n50 - 100.\\nWe use recently proposed techniques for measuring the quality of the resulting vector representa-\\ntions, with the expectation that not only will similar words tend to be close to each other, but that\\nwords can have multiple degrees of similarity [20]. This has been observed earlier in the context\\nof inﬂectional languages - for example, nouns can have multiple word endings, and if we search for\\nsimilar words in a subspace of the original vector space, it is possible to ﬁnd words that have similar\\nendings [13, 14].\\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple\\nsyntactic regularities. Using a word offset technique where simple algebraic operations are per-\\nformed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vec-\\ntor(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20].\\nIn this paper, we try to maximize accuracy of these vector operations by developing new model\\narchitectures that preserve the linear regularities among words. We design a new comprehensive test\\nset for measuring both syntactic and semantic regularities1, and show that many such regularities\\ncan be learned with high accuracy. Moreover, we discuss how training time and accuracy depends\\non the dimensionality of the word vectors and on the amount of the training data.\\n1.2\\nPrevious Work\\nRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular model\\narchitecture for estimating neural network language model (NNLM) was proposed in [1], where a\\nfeedforward neural network with a linear projection layer and a non-linear hidden layer was used to\\nlearn jointly the word vector representation and a statistical language model. This work has been\\nfollowed by many others.\\nAnother interesting architecture of NNLM was presented in [13, 14], where the word vectors are\\nﬁrst learned using neural network with a single hidden layer. The word vectors are then used to train\\nthe NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this\\nwork, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are\\nlearned using a simple model.\\nIt was later shown that the word vectors can be used to signiﬁcantly improve and simplify many\\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word\\nvectors were made available for future research and comparison2. However, as far as we know, these\\narchitectures were signiﬁcantly more computationally expensive for training than the one proposed\\nin [13], with the exception of certain version of log-bilinear model where diagonal weight matrices\\nare used [23].\\n2\\nModel Architectures\\nMany different types of models were proposed for estimating continuous representations of words,\\nincluding the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\\nIn this paper, we focus on distributed representations of words learned by neural networks, as it was\\npreviously shown that they perform signiﬁcantly better than LSA for preserving linear regularities\\namong words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.\\nSimilar to [18], to compare different model architectures we deﬁne ﬁrst the computational complex-\\nity of a model as the number of parameters that need to be accessed to fully train the model. Next,\\nwe will try to maximize the accuracy, while minimizing the computational complexity.\\n1The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt\\n2http://ronan.collobert.com/senna/\\nhttp://metaoptimize.com/projects/wordreprs/\\nhttp://www.fit.vutbr.cz/˜imikolov/rnnlm/\\nhttp://ai.stanford.edu/˜ehhuang/\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='For all the following models, the training complexity is proportional to\\nO = E × T × Q,\\n(1)\\nwhere E is number of the training epochs, T is the number of the words in the training set and Q is\\ndeﬁned further for each model architecture. Common choice is E = 3 −50 and T up to one billion.\\nAll models are trained using stochastic gradient descent and backpropagation [26].\\n2.1\\nFeedforward Neural Net Language Model (NNLM)\\nThe probabilistic feedforward neural network language model has been proposed in [1]. It consists\\nof input, projection, hidden and output layers. At the input layer, N previous words are encoded\\nusing 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a\\nprojection layer P that has dimensionality N × D, using a shared projection matrix. As only N\\ninputs are active at any given time, composition of the projection layer is a relatively cheap operation.\\nThe NNLM architecture becomes complex for computation between the projection and the hidden\\nlayer, as values in the projection layer are dense. For a common choice of N = 10, the size of the\\nprojection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000\\nunits. Moreover, the hidden layer is used to compute probability distribution over all the words in the\\nvocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity\\nper each training example is\\nQ = N × D + N × D × H + H × V,\\n(2)\\nwhere the dominating term is H × V . However, several practical solutions were proposed for\\navoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized\\nmodels completely by using models that are not normalized during training [4, 9]. With binary tree\\nrepresentations of the vocabulary, the number of output units that need to be evaluated can go down\\nto around log2(V ). Thus, most of the complexity is caused by the term N × D × H.\\nIn our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary\\ntree. This follows previous observations that the frequency of words works well for obtaining classes\\nin neural net language models [16]. Huffman trees assign short binary codes to frequent words, and\\nthis further reduces the number of output units that need to be evaluated: while balanced binary tree\\nwould require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires\\nonly about log2(Unigram perplexity(V )). For example when the vocabulary size is one million\\nwords, this results in about two times speedup in evaluation. While this is not crucial speedup for\\nneural network LMs as the computational bottleneck is in the N ×D×H term, we will later propose\\narchitectures that do not have hidden layers and thus depend heavily on the efﬁciency of the softmax\\nnormalization.\\n2.2\\nRecurrent Neural Net Language Model (RNNLM)\\nRecurrent neural network based language model has been proposed to overcome certain limitations\\nof the feedforward NNLM, such as the need to specify the context length (the order of the model N),\\nand because theoretically RNNs can efﬁciently represent more complex patterns than the shallow\\nneural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and\\noutput layer. What is special for this type of model is the recurrent matrix that connects hidden\\nlayer to itself, using time-delayed connections. This allows the recurrent model to form some kind\\nof short term memory, as information from the past can be represented by the hidden layer state that\\ngets updated based on the current input and the state of the hidden layer in the previous time step.\\nThe complexity per training example of the RNN model is\\nQ = H × H + H × V,\\n(3)\\nwhere the word representations D have the same dimensionality as the hidden layer H. Again, the\\nterm H × V can be efﬁciently reduced to H × log2(V ) by using hierarchical softmax. Most of the\\ncomplexity then comes from H × H.\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='2.3\\nParallel Training of Neural Networks\\nTo train models on huge data sets, we have implemented several models on top of a large-scale\\ndistributed framework called DistBelief [6], including the feedforward NNLM and the new models\\nproposed in this paper. The framework allows us to run multiple replicas of the same model in\\nparallel, and each replica synchronizes its gradient updates through a centralized server that keeps\\nall the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with\\nan adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use\\none hundred or more model replicas, each using many CPU cores at different machines in a data\\ncenter.\\n3\\nNew Log-linear Models\\nIn this section, we propose two new model architectures for learning distributed representations\\nof words that try to minimize computational complexity. The main observation from the previous\\nsection was that most of the complexity is caused by the non-linear hidden layer in the model. While\\nthis is what makes neural networks so attractive, we decided to explore simpler models that might\\nnot be able to represent the data as precisely as neural networks, but can possibly be trained on much\\nmore data efﬁciently.\\nThe new architectures directly follow those proposed in our earlier work [13, 14], where it was\\nfound that neural network language model can be successfully trained in two steps: ﬁrst, continuous\\nword vectors are learned using simple model, and then the N-gram NNLM is trained on top of these\\ndistributed representations of words. While there has been later substantial amount of work that\\nfocuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.\\nNote that related models have been proposed also much earlier [26, 8].\\n3.1\\nContinuous Bag-of-Words Model\\nThe ﬁrst proposed architecture is similar to the feedforward NNLM, where the non-linear hidden\\nlayer is removed and the projection layer is shared for all words (not just the projection matrix);\\nthus, all words get projected into the same position (their vectors are averaged). We call this archi-\\ntecture a bag-of-words model as the order of words in the history does not inﬂuence the projection.\\nFurthermore, we also use words from the future; we have obtained the best performance on the task\\nintroduced in the next section by building a log-linear classiﬁer with four future and four history\\nwords at the input, where the training criterion is to correctly classify the current (middle) word.\\nTraining complexity is then\\nQ = N × D + D × log2(V ).\\n(4)\\nWe denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous\\ndistributed representation of the context. The model architecture is shown at Figure 1. Note that the\\nweight matrix between the input and the projection layer is shared for all word positions in the same\\nway as in the NNLM.\\n3.2\\nContinuous Skip-gram Model\\nThe second architecture is similar to CBOW, but instead of predicting the current word based on the\\ncontext, it tries to maximize classiﬁcation of a word based on another word in the same sentence.\\nMore precisely, we use each current word as an input to a log-linear classiﬁer with continuous\\nprojection layer, and predict words within a certain range before and after the current word. We\\nfound that increasing the range improves quality of the resulting word vectors, but it also increases\\nthe computational complexity. Since the more distant words are usually less related to the current\\nword than those close to it, we give less weight to the distant words by sampling less from those\\nwords in our training examples.\\nThe training complexity of this architecture is proportional to\\nQ = C × (D + D × log2(V )),\\n(5)\\nwhere C is the maximum distance of the words. Thus, if we choose C = 5, for each training word\\nwe will select randomly a number R in range < 1; C >, and then use R words from history and\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 4}, page_content='w(t-2)\\nw(t+1)\\nw(t-1)\\nw(t+2)\\nw(t)\\nSUM\\n       INPUT         PROJECTION         OUTPUT\\nw(t)\\n          INPUT         PROJECTION      OUTPUT\\nw(t-2)\\nw(t-1)\\nw(t+1)\\nw(t+2)\\n                   CBOW                                                   Skip-gram\\nFigure 1: New model architectures. The CBOW architecture predicts the current word based on the\\ncontext, and the Skip-gram predicts surrounding words given the current word.\\nR words from the future of the current word as correct labels. This will require us to do R × 2\\nword classiﬁcations, with the current word as input, and each of the R + R words as output. In the\\nfollowing experiments, we use C = 10.\\n4\\nResults\\nTo compare the quality of different versions of word vectors, previous papers typically use a table\\nshowing example words and their most similar words, and understand them intuitively. Although\\nit is easy to show that word France is similar to Italy and perhaps some other countries, it is much\\nmore challenging when subjecting those vectors in a more complex similarity task, as follows. We\\nfollow previous observation that there can be many different types of similarities between words, for\\nexample, word big is similar to bigger in the same sense that small is similar to smaller. Example\\nof another type of relationship can be word pairs big - biggest and small - smallest [20]. We further\\ndenote two pairs of words with the same relationship as a question, as we can ask: ”What is the\\nword that is similar to small in the same sense as biggest is similar to big?”\\nSomewhat surprisingly, these questions can be answered by performing simple algebraic operations\\nwith the vector representation of words. To ﬁnd a word that is similar to small in the same sense as\\nbiggest is similar to big, we can simply compute vector X = vector(”biggest”)−vector(”big”)+\\nvector(”small”). Then, we search in the vector space for the word closest to X measured by cosine\\ndistance, and use it as the answer to the question (we discard the input question words during this\\nsearch). When the word vectors are well trained, it is possible to ﬁnd the correct answer (word\\nsmallest) using this method.\\nFinally, we found that when we train high dimensional word vectors on a large amount of data, the\\nresulting vectors can be used to answer very subtle semantic relationships between words, such as\\na city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors\\nwith such semantic relationships could be used to improve many existing NLP applications, such\\nas machine translation, information retrieval and question answering systems, and may enable other\\nfuture applications yet to be invented.\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 5}, page_content='Table 1: Examples of ﬁve types of semantic and nine types of syntactic questions in the Semantic-\\nSyntactic Word Relationship test set.\\nType of relationship\\nWord Pair 1\\nWord Pair 2\\nCommon capital city\\nAthens\\nGreece\\nOslo\\nNorway\\nAll capital cities\\nAstana\\nKazakhstan\\nHarare\\nZimbabwe\\nCurrency\\nAngola\\nkwanza\\nIran\\nrial\\nCity-in-state\\nChicago\\nIllinois\\nStockton\\nCalifornia\\nMan-Woman\\nbrother\\nsister\\ngrandson\\ngranddaughter\\nAdjective to adverb\\napparent\\napparently\\nrapid\\nrapidly\\nOpposite\\npossibly\\nimpossibly\\nethical\\nunethical\\nComparative\\ngreat\\ngreater\\ntough\\ntougher\\nSuperlative\\neasy\\neasiest\\nlucky\\nluckiest\\nPresent Participle\\nthink\\nthinking\\nread\\nreading\\nNationality adjective\\nSwitzerland\\nSwiss\\nCambodia\\nCambodian\\nPast tense\\nwalking\\nwalked\\nswimming\\nswam\\nPlural nouns\\nmouse\\nmice\\ndollar\\ndollars\\nPlural verbs\\nwork\\nworks\\nspeak\\nspeaks\\n4.1\\nTask Description\\nTo measure quality of the word vectors, we deﬁne a comprehensive test set that contains ﬁve types\\nof semantic questions, and nine types of syntactic questions. Two examples from each category are\\nshown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions\\nin each category were created in two steps: ﬁrst, a list of similar word pairs was created manually.\\nThen, a large list of questions is formed by connecting two word pairs. For example, we made a\\nlist of 68 large American cities and the states they belong to, and formed about 2.5K questions by\\npicking two word pairs at random. We have included in our test set only single token words, thus\\nmulti-word entities are not present (such as New York).\\nWe evaluate the overall accuracy for all question types, and for each question type separately (se-\\nmantic, syntactic). Question is assumed to be correctly answered only if the closest word to the\\nvector computed using the above method is exactly the same as the correct word in the question;\\nsynonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely\\nto be impossible, as the current models do not have any input information about word morphology.\\nHowever, we believe that usefulness of the word vectors for certain applications should be positively\\ncorrelated with this accuracy metric. Further progress can be achieved by incorporating information\\nabout structure of words, especially for the syntactic questions.\\n4.2\\nMaximization of Accuracy\\nWe have used a Google News corpus for training the word vectors. This corpus contains about\\n6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we\\nare facing time constrained optimization problem, as it can be expected that both using more data\\nand higher dimensional word vectors will improve the accuracy. To estimate the best choice of\\nmodel architecture for obtaining as good as possible results quickly, we have ﬁrst evaluated models\\ntrained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.\\nThe results using the CBOW architecture with different choice of word vector dimensionality and\\nincreasing amount of the training data are shown in Table 2.\\nIt can be seen that after some point, adding more dimensions or adding more training data provides\\ndiminishing improvements. So, we have to increase both vector dimensionality and the amount\\nof the training data together. While this observation might seem trivial, it must be noted that it is\\ncurrently popular to train word vectors on relatively large amounts of data, but with insufﬁcient size\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 6}, page_content='Table 2:\\nAccuracy on subset of the Semantic-Syntactic Word Relationship test set, using word\\nvectors from the CBOW architecture with limited vocabulary. Only questions containing words from\\nthe most frequent 30k words are used.\\nDimensionality / Training words\\n24M\\n49M\\n98M\\n196M\\n391M\\n783M\\n50\\n13.4\\n15.7\\n18.6\\n19.1\\n22.5\\n23.2\\n100\\n19.4\\n23.1\\n27.8\\n28.7\\n33.4\\n32.2\\n300\\n23.2\\n29.2\\n35.3\\n38.6\\n43.7\\n45.9\\n600\\n24.0\\n30.1\\n36.5\\n40.8\\n46.6\\n50.4\\nTable 3: Comparison of architectures using models trained on the same data, with 640-dimensional\\nword vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set,\\nand on the syntactic relationship test set of [20]\\nModel\\nSemantic-Syntactic Word Relationship test set\\nMSR Word Relatedness\\nArchitecture\\nSemantic Accuracy [%]\\nSyntactic Accuracy [%]\\nTest Set [20]\\nRNNLM\\n9\\n36\\n35\\nNNLM\\n23\\n53\\n47\\nCBOW\\n24\\n64\\n61\\nSkip-gram\\n55\\n59\\n56\\n(such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the\\nsame increase of computational complexity as increasing vector size twice.\\nFor the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradi-\\nent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so\\nthat it approaches zero at the end of the last training epoch.\\n4.3\\nComparison of Model Architectures\\nFirst we compare different model architectures for deriving the word vectors using the same training\\ndata and using the same dimensionality of 640 of the word vectors. In the further experiments, we\\nuse full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to\\nthe 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic\\nsimilarity between words3.\\nThe training data consists of several LDC corpora and is described in detail in [18] (320M words,\\n82K vocabulary). We used these data to provide a comparison to a previously trained recurrent\\nneural network language model that took about 8 weeks to train on a single CPU. We trained a feed-\\nforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6],\\nusing a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the\\nprojection layer has size 640 × 8).\\nIn Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly\\non the syntactic questions. The NNLM vectors perform signiﬁcantly better than the RNN - this is\\nnot surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden\\nlayer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the\\nsame on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic\\ntask than the CBOW model (but still better than the NNLM), and much better on the semantic part\\nof the test than all the other models.\\nNext, we evaluated our models trained using one CPU only and compared the results against publicly\\navailable word vectors. The comparison is given in Table 4. The CBOW model was trained on subset\\n3We thank Geoff Zweig for providing us the test set.\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 7}, page_content='Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relation-\\nship test set, and word vectors from our models. Full vocabularies are used.\\nModel\\nVector\\nTraining\\nAccuracy [%]\\nDimensionality\\nwords\\nSemantic\\nSyntactic\\nTotal\\nCollobert-Weston NNLM\\n50\\n660M\\n9.3\\n12.3\\n11.0\\nTurian NNLM\\n50\\n37M\\n1.4\\n2.6\\n2.1\\nTurian NNLM\\n200\\n37M\\n1.4\\n2.2\\n1.8\\nMnih NNLM\\n50\\n37M\\n1.8\\n9.1\\n5.8\\nMnih NNLM\\n100\\n37M\\n3.3\\n13.2\\n8.8\\nMikolov RNNLM\\n80\\n320M\\n4.9\\n18.4\\n12.7\\nMikolov RNNLM\\n640\\n320M\\n8.6\\n36.5\\n24.6\\nHuang NNLM\\n50\\n990M\\n13.3\\n11.6\\n12.3\\nOur NNLM\\n20\\n6B\\n12.9\\n26.4\\n20.3\\nOur NNLM\\n50\\n6B\\n27.9\\n55.8\\n43.2\\nOur NNLM\\n100\\n6B\\n34.2\\n64.5\\n50.8\\nCBOW\\n300\\n783M\\n15.5\\n53.1\\n36.1\\nSkip-gram\\n300\\n783M\\n50.0\\n55.9\\n53.3\\nTable 5: Comparison of models trained for three epochs on the same data and models trained for\\none epoch. Accuracy is reported on the full Semantic-Syntactic data set.\\nModel\\nVector\\nTraining\\nAccuracy [%]\\nTraining time\\nDimensionality\\nwords\\n[days]\\nSemantic\\nSyntactic\\nTotal\\n3 epoch CBOW\\n300\\n783M\\n15.5\\n53.1\\n36.1\\n1\\n3 epoch Skip-gram\\n300\\n783M\\n50.0\\n55.9\\n53.3\\n3\\n1 epoch CBOW\\n300\\n783M\\n13.8\\n49.9\\n33.6\\n0.3\\n1 epoch CBOW\\n300\\n1.6B\\n16.1\\n52.6\\n36.1\\n0.6\\n1 epoch CBOW\\n600\\n783M\\n15.4\\n53.3\\n36.2\\n0.7\\n1 epoch Skip-gram\\n300\\n783M\\n45.6\\n52.2\\n49.2\\n1\\n1 epoch Skip-gram\\n300\\n1.6B\\n52.2\\n55.1\\n53.8\\n2\\n1 epoch Skip-gram\\n600\\n783M\\n56.7\\n54.5\\n55.5\\n2.5\\nof the Google News data in about a day, while training time for the Skip-gram model was about three\\ndays.\\nFor experiments reported further, we used just one training epoch (again, we decrease the learning\\nrate linearly so that it approaches zero at the end of training). Training a model on twice as much\\ndata using one epoch gives comparable or better results than iterating over the same data for three\\nepochs, as is shown in Table 5, and provides additional small speedup.\\n4.4\\nLarge Scale Parallel Training of Models\\nAs mentioned earlier, we have implemented various models in a distributed framework called Dis-\\ntBelief. Below we report the results of several models trained on the Google News 6B data set,\\nwith mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Ada-\\ngrad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 8}, page_content='Table 6:\\nComparison of models trained using the DistBelief distributed framework. Note that\\ntraining of NNLM with 1000-dimensional vectors would take too long to complete.\\nModel\\nVector\\nTraining\\nAccuracy [%]\\nTraining time\\nDimensionality\\nwords\\n[days x CPU cores]\\nSemantic\\nSyntactic\\nTotal\\nNNLM\\n100\\n6B\\n34.2\\n64.5\\n50.8\\n14 x 180\\nCBOW\\n1000\\n6B\\n57.3\\n68.9\\n63.7\\n2 x 140\\nSkip-gram\\n1000\\n6B\\n66.1\\n65.1\\n65.6\\n2.5 x 125\\nTable 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.\\nArchitecture\\nAccuracy [%]\\n4-gram [32]\\n39\\nAverage LSA similarity [32]\\n49\\nLog-bilinear model [24]\\n54.8\\nRNNLMs [19]\\n55.4\\nSkip-gram\\n48.0\\nSkip-gram + RNNLMs\\n58.9\\nestimate since the data center machines are shared with other production tasks, and the usage can\\nﬂuctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of\\nthe CBOW model and the Skip-gram model are much closer to each other than their single-machine\\nimplementations. The result are reported in Table 6.\\n4.5\\nMicrosoft Research Sentence Completion Challenge\\nThe Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing\\nlanguage modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one\\nword is missing in each sentence and the goal is to select word that is the most coherent with the\\nrest of the sentence, given a list of ﬁve reasonable choices. Performance of several techniques has\\nbeen already reported on this set, including N-gram models, LSA-based model [32], log-bilinear\\nmodel [24] and a combination of recurrent neural networks that currently holds the state of the art\\nperformance of 55.4% accuracy on this benchmark [19].\\nWe have explored the performance of Skip-gram architecture on this task. First, we train the 640-\\ndimensional model on 50M words provided in [32]. Then, we compute score of each sentence in\\nthe test set by using the unknown word at the input, and predict all surrounding words in a sentence.\\nThe ﬁnal sentence score is then the sum of these individual predictions. Using the sentence scores,\\nwe choose the most likely sentence.\\nA short summary of some previous results together with the new results is presented in Table 7.\\nWhile the Skip-gram model itself does not perform on this task better than LSA similarity, the scores\\nfrom this model are complementary to scores obtained with RNNLMs, and a weighted combination\\nleads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and\\n58.7% on the test part of the set).\\n5\\nExamples of the Learned Relationships\\nTable 8 shows words that follow various relationships. We follow the approach described above: the\\nrelationship is deﬁned by subtracting two word vectors, and the result is added to another word. Thus\\nfor example, Paris - France + Italy = Rome. As it can be seen, accuracy is quite good, although\\nthere is clearly a lot of room for further improvements (note that using our accuracy metric that\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 9}, page_content='Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-\\ngram model trained on 783M words with 300 dimensionality).\\nRelationship\\nExample 1\\nExample 2\\nExample 3\\nFrance - Paris\\nItaly: Rome\\nJapan: Tokyo\\nFlorida: Tallahassee\\nbig - bigger\\nsmall: larger\\ncold: colder\\nquick: quicker\\nMiami - Florida\\nBaltimore: Maryland\\nDallas: Texas\\nKona: Hawaii\\nEinstein - scientist\\nMessi: midﬁelder\\nMozart: violinist\\nPicasso: painter\\nSarkozy - France\\nBerlusconi: Italy\\nMerkel: Germany\\nKoizumi: Japan\\ncopper - Cu\\nzinc: Zn\\ngold: Au\\nuranium: plutonium\\nBerlusconi - Silvio\\nSarkozy: Nicolas\\nPutin: Medvedev\\nObama: Barack\\nMicrosoft - Windows\\nGoogle: Android\\nIBM: Linux\\nApple: iPhone\\nMicrosoft - Ballmer\\nGoogle: Yahoo\\nIBM: McNealy\\nApple: Jobs\\nJapan - sushi\\nGermany: bratwurst\\nFrance: tapas\\nUSA: pizza\\nassumes exact match, the results in Table 8 would score only about 60%). We believe that word\\nvectors trained on even larger data sets with larger dimensionality will perform signiﬁcantly better,\\nand will enable the development of new innovative applications. Another way to improve accuracy is\\nto provide more than one example of the relationship. By using ten examples instead of one to form\\nthe relationship vector (we average the individual vectors together), we have observed improvement\\nof accuracy of our best models by about 10% absolutely on the semantic-syntactic test.\\nIt is also possible to apply the vector operations to solve different tasks. For example, we have\\nobserved good accuracy for selecting out-of-the-list words, by computing average vector for a list of\\nwords, and ﬁnding the most distant word vector. This is a popular type of problems in certain human\\nintelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.\\n6\\nConclusion\\nIn this paper we studied the quality of vector representations of words derived by various models on\\na collection of syntactic and semantic language tasks. We observed that it is possible to train high\\nquality word vectors using very simple model architectures, compared to the popular neural network\\nmodels (both feedforward and recurrent). Because of the much lower computational complexity, it\\nis possible to compute very accurate high dimensional word vectors from a much larger data set.\\nUsing the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram\\nmodels even on corpora with one trillion words, for basically unlimited size of the vocabulary. That\\nis several orders of magnitude larger than the best previously published results for similar models.\\nAn interesting task where the word vectors have recently been shown to signiﬁcantly outperform the\\nprevious state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were\\nused together with other techniques to achieve over 50% increase in Spearman’s rank correlation\\nover the previous best result [31]. The neural network based word vectors were previously applied\\nto many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can\\nbe expected that these applications can beneﬁt from the model architectures described in this paper.\\nOur ongoing work shows that the word vectors can be successfully applied to automatic extension\\nof facts in Knowledge Bases, and also for veriﬁcation of correctness of existing facts. Results\\nfrom machine translation experiments also look very promising. In the future, it would be also\\ninteresting to compare our techniques to Latent Relational Analysis [30] and others. We believe that\\nour comprehensive test set will help the research community to improve the existing techniques for\\nestimating the word vectors. We also expect that high quality word vectors will become an important\\nbuilding block for future NLP applications.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 10}, page_content='7\\nFollow-Up Work\\nAfter the initial version of this paper was written, we published single-machine multi-threaded C++\\ncode for computing the word vectors, using both the continuous bag-of-words and skip-gram archi-\\ntectures4. The training speed is signiﬁcantly higher than reported earlier in this paper, i.e. it is in the\\norder of billions of words per hour for typical hyperparameter choices. We also published more than\\n1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of\\nour follow-up work will be published in an upcoming NIPS 2013 paper [21].\\nReferences\\n[1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-\\nchine Learning Research, 3:1137-1155, 2003.\\n[2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-\\nchines, MIT Press, 2007.\\n[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine\\ntranslation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language\\nProcessing and Computational Language Learning, 2007.\\n[4] R. Collobert and J. Weston. A Uniﬁed Architecture for Natural Language Processing: Deep\\nNeural Networks with Multitask Learning. In International Conference on Machine Learning,\\nICML, 2008.\\n[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan-\\nguage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-\\n2537, 2011.\\n[6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A.\\nSenior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.\\n[7] J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and\\nstochastic optimization. Journal of Machine Learning Research, 2011.\\n[8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.\\n[9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representations\\nvia Global Context and Multiple Word Prototypes. In: Proc. Association for Computational\\nLinguistics, 2012.\\n[10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis-\\ntributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations,\\nMIT Press, 1986.\\n[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring\\ndegrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic\\nEvaluation (SemEval 2012), 2012.\\n[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for\\nsentiment analysis. In Proceedings of ACL, 2011.\\n[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-\\nversity of Technology, 2007.\\n[14] T. Mikolov, J. Kopeck´y, L. Burget, O. Glembek and J. ˇCernock´y. Neural network based lan-\\nguage models for higly inﬂective languages, In: Proc. ICASSP 2009.\\n[15] T. Mikolov, M. Karaﬁ´at, L. Burget, J. ˇCernock´y, S. Khudanpur. Recurrent neural network\\nbased language model, In: Proceedings of Interspeech, 2010.\\n[16] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock´y, S. Khudanpur. Extensions of recurrent neural\\nnetwork language model, In: Proceedings of ICASSP 2011.\\n[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ˇCernock´y. Empirical Evaluation and Com-\\nbination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.\\n4The code is available at https://code.google.com/p/word2vec/\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 11}, page_content='[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. ˇCernock´y. Strategies for Training Large Scale\\nNeural Network Language Models, In: Proc. Automatic Speech Recognition and Understand-\\ning, 2011.\\n[19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-\\nsity of Technology, 2012.\\n[20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-\\ntations. NAACL HLT 2013.\\n[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of\\nWords and Phrases and their Compositionality. Accepted to NIPS 2013.\\n[22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,\\n2007.\\n[23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural\\nInformation Processing Systems 21, MIT Press, 2009.\\n[24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language\\nmodels. ICML, 2012.\\n[25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,\\n2005.\\n[26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-\\npropagating errors. Nature, 323:533.536, 1986.\\n[27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,\\n2007.\\n[28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and\\nUnfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.\\n[29] J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for\\nSemi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.\\n[30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-\\ntional Joint Conference on Artiﬁcial Intelligence, 2005.\\n[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for\\nMeasuring Relational Similarity. NAACL HLT 2013.\\n[32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft\\nResearch Technical Report MSR-TR-2011-129, 2011.\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='Billion-scale similarity search with GPUs\\nJeff Johnson\\nFacebook AI Research\\nNew York\\nMatthijs Douze\\nFacebook AI Research\\nParis\\nHerv´e J´egou\\nFacebook AI Research\\nParis\\nABSTRACT\\nSimilarity search ﬁnds application in specialized database\\nsystems handling complex data such as images or videos,\\nwhich are typically represented by high-dimensional features\\nand require speciﬁc indexing structures. This paper tackles\\nthe problem of better utilizing GPUs for this task. While\\nGPUs excel at data-parallel tasks, prior approaches are bot-\\ntlenecked by algorithms that expose less parallelism, such as\\nk-min selection, or make poor use of the memory hierarchy.\\nWe propose a design for k-selection that operates at up\\nto 55% of theoretical peak performance, enabling a nearest\\nneighbor implementation that is 8.5× faster than prior GPU\\nstate of the art. We apply it in diﬀerent similarity search\\nscenarios, by proposing optimized design for brute-force, ap-\\nproximate and compressed-domain search based on product\\nquantization. In all these setups, we outperform the state of\\nthe art by large margins. Our implementation enables the\\nconstruction of a high accuracy k-NN graph on 95 million\\nimages from the Yfcc100M dataset in 35 minutes, and of\\na graph connecting 1 billion vectors in less than 12 hours\\non 4 Maxwell Titan X GPUs. We have open-sourced our\\napproach1 for the sake of comparison and reproducibility.\\n1.\\nINTRODUCTION\\nImages and videos constitute a new massive source of data\\nfor indexing and search. Extensive metadata for this con-\\ntent is often not available. Search and interpretation of this\\nand other human-generated content, like text, is diﬃcult and\\nimportant. A variety of machine learning and deep learn-\\ning algorithms are being used to interpret and classify these\\ncomplex, real-world entities. Popular examples include the\\ntext representation known as word2vec [32], representations\\nof images by convolutional neural networks [39, 19], and im-\\nage descriptors for instance search [20]. Such representations\\nor embeddings are usually real-valued, high-dimensional vec-\\ntors of 50 to 1000+ dimensions. Many of these vector repre-\\nsentations can only eﬀectively be produced on GPU systems,\\n1https://github.com/facebookresearch/faiss\\nas the underlying processes either have high arithmetic com-\\nplexity and/or high data bandwidth demands [28], or cannot\\nbe eﬀectively partitioned without failing due to communi-\\ncation overhead or representation quality [38].\\nOnce pro-\\nduced, their manipulation is itself arithmetically intensive.\\nHowever, how to utilize GPU assets is not straightforward.\\nMore generally, how to exploit new heterogeneous architec-\\ntures is a key subject for the database community [9].\\nIn this context, searching by numerical similarity rather\\nthan via structured relations is more suitable. This could be\\nto ﬁnd the most similar content to a picture, or to ﬁnd the\\nvectors that have the highest response to a linear classiﬁer\\non all vectors of a collection.\\nOne of the most expensive operations to be performed on\\nlarge collections is to compute a k-NN graph. It is a directed\\ngraph where each vector of the database is a node and each\\nedge connects a node to its k nearest neighbors.\\nThis is\\nour ﬂagship application. Note, state of the art methods like\\nNN-Descent [15] have a large memory overhead on top of\\nthe dataset itself and cannot readily scale to the billion-sized\\ndatabases we consider.\\nSuch applications must deal with the curse of dimension-\\nality [46], rendering both exhaustive search or exact index-\\ning for non-exhaustive search impractical on billion-scale\\ndatabases.\\nThis is why there is a large body of work on\\napproximate search and/or graph construction. To handle\\nhuge datasets that do not ﬁt in RAM, several approaches\\nemploy an internal compressed representation of the vec-\\ntors using an encoding.\\nThis is especially convenient for\\nmemory-limited devices like GPUs. It turns out that accept-\\ning a minimal accuracy loss results in orders of magnitude\\nof compression [21]. The most popular vector compression\\nmethods can be classiﬁed into either binary codes [18, 22],\\nor quantization methods [25, 37]. Both have the desirable\\nproperty that searching neighbors does not require recon-\\nstructing the vectors.\\nOur paper focuses on methods based on product quanti-\\nzation (PQ) codes, as these were shown to be more eﬀective\\nthan binary codes [34]. In addition, binary codes incur im-\\nportant overheads for non-exhaustive search methods [35].\\nSeveral improvements were proposed after the original prod-\\nuct quantization proposal known as IVFADC [25]; most are\\ndiﬃcult to implement eﬃciently on GPU. For instance, the\\ninverted multi-index [4], useful for high-speed/low-quality\\noperating points, depends on a complicated “multi-sequence”\\nalgorithm. The optimized product quantization or OPQ [17]\\nis a linear transformation on the input vectors that improves\\nthe accuracy of the product quantization; it can be applied\\n1\\narXiv:1702.08734v1  [cs.CV]  28 Feb 2017'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='as a pre-processing. The SIMD-optimized IVFADC imple-\\nmentation from [2] operates only with sub-optimal parame-\\nters (few coarse quantization centroids). Many other meth-\\nods, like LOPQ and the Polysemous codes [27, 16] are too\\ncomplex to be implemented eﬃciently on GPUs.\\nThere are many implementations of similarity search on\\nGPUs, but mostly with binary codes [36], small datasets [44],\\nor exhaustive search [14, 40, 41]. To the best of our knowl-\\nedge, only the work by Wieschollek et al. [47] appears suit-\\nable for billion-scale datasets with quantization codes. This\\nis the prior state of the art on GPUs, which we compare\\nagainst in Section 6.4.\\nThis paper makes the following contributions:\\n• a GPU k-selection algorithm, operating in fast register\\nmemory and ﬂexible enough to be fusable with other\\nkernels, for which we provide a complexity analysis;\\n• a near-optimal algorithmic layout for exact and ap-\\nproximate k-nearest neighbor search on GPU;\\n• a range of experiments that show that these improve-\\nments outperform previous art by a large margin on\\nmid- to large-scale nearest-neighbor search tasks, in\\nsingle or multi-GPU conﬁgurations.\\nThe paper is organized as follows. Section 2 introduces\\nthe context and notation.\\nSection 3 reviews GPU archi-\\ntecture and discusses problems appearing when using it for\\nsimilarity search. Section 4 introduces one of our main con-\\ntributions, i.e., our k-selection method for GPUs, while Sec-\\ntion 5 provides details regarding the algorithm computation\\nlayout. Finally, Section 6 provides extensive experiments for\\nour approach, compares it to the state of the art, and shows\\nconcrete use cases for image collections.\\n2.\\nPROBLEM STATEMENT\\nWe are concerned with similarity search in vector collec-\\ntions. Given the query vector x ∈Rd and the collection2\\n[yi]i=0:ℓ(yi ∈Rd), we search:\\nL = k-argmini=0:ℓ∥x −yi∥2,\\n(1)\\ni.e., we search the k nearest neighbors of x in terms of L2\\ndistance. The L2 distance is used most often, as it is op-\\ntimized by design when learning several embeddings (e.g.,\\n[20]), due to its attractive linear algebra properties.\\nThe lowest distances are collected by k-selection. For an\\narray [ai]i=0:ℓ, k-selection ﬁnds the k lowest valued elements\\n[asi]i=0:k, asi ≤asi+1, along with the indices [si]i=0:k, 0 ≤\\nsi < ℓ, of those elements from the input array. The ai will be\\n32-bit ﬂoating point values; the si are 32- or 64-bit integers.\\nOther comparators are sometimes desired; e.g., for cosine\\nsimilarity we search for highest values. The order between\\nequivalent keys asi = asj is not speciﬁed.\\nBatching.\\nTypically, searches are performed in batches\\nof nq query vectors [xj]j=0:nq (xj ∈Rd) in parallel, which\\nallows for more ﬂexibility when executing on multiple CPU\\nthreads or on GPU. Batching for k-selection entails selecting\\nnq × k elements and indices from nq separate arrays, where\\neach array is of a potentially diﬀerent length ℓi ≥k.\\n2To avoid clutter in 0-based indexing, we use the array no-\\ntation 0 : ℓto denote the range {0, ..., ℓ−1} inclusive.\\nExact search. The exact solution computes the full pair-\\nwise distance matrix D = [∥xj −yi∥2\\n2]j=0:nq,i=0:ℓ∈Rnq×ℓ.\\nIn practice, we use the decomposition\\n∥xj −yi∥2\\n2 = ∥xj∥2 + ∥yi∥2 −2⟨xj, yi⟩.\\n(2)\\nThe two ﬁrst terms can be precomputed in one pass over\\nthe matrices X and Y whose rows are the [xj] and [yi]. The\\nbottleneck is to evaluate ⟨xj, yi⟩, equivalent to the matrix\\nmultiplication XY ⊤. The k-nearest neighbors for each of\\nthe nq queries are k-selected along each row of D.\\nCompressed-domain search. From now on, we focus on\\napproximate nearest-neighbor search. We consider, in par-\\nticular, the IVFADC indexing structure [25]. The IVFADC\\nindex relies on two levels of quantization, and the database\\nvectors are encoded. The database vector y is approximated\\nas:\\ny ≈q(y) = q1(y) + q2(y −q1(y))\\n(3)\\nwhere q1 : Rd →C1 ⊂Rd and q2 : Rd →C2 ⊂Rd are quan-\\ntizers; i.e., functions that output an element from a ﬁnite\\nset. Since the sets are ﬁnite, q(y) is encoded as the index of\\nq1(y) and that of q2(y −q1(y)). The ﬁrst-level quantizer is a\\ncoarse quantizer and the second level ﬁne quantizer encodes\\nthe residual vector after the ﬁrst level.\\nThe Asymmetric Distance Computation (ADC) search\\nmethod returns an approximate result:\\nLADC = k-argmini=0:ℓ∥x −q(yi)∥2.\\n(4)\\nFor IVFADC the search is not exhaustive.\\nVectors for\\nwhich the distance is computed are pre-selected depending\\non the ﬁrst-level quantizer q1:\\nLIVF = τ-argminc∈C1∥x −c∥2.\\n(5)\\nThe multi-probe parameter τ is the number of coarse-level\\ncentroids we consider.\\nThe quantizer operates a nearest-\\nneighbor search with exact distances, in the set of reproduc-\\ntion values. Then, the IVFADC search computes\\nLIVFADC =\\nk-argmin\\ni=0:ℓs.t. q1(yi)∈LIVF\\n∥x −q(yi)∥2.\\n(6)\\nHence, IVFADC relies on the same distance estimations as\\nthe two-step quantization of ADC, but computes them only\\non a subset of vectors.\\nThe corresponding data structure, the inverted ﬁle, groups\\nthe vectors yi into |C1| inverted lists I1, ..., I|C1| with homo-\\ngeneous q1(yi). Therefore, the most memory-intensive op-\\neration is computing LIVFADC, and boils down to linearly\\nscanning τ inverted lists.\\nThe quantizers. The quantizers q1 and q2 have diﬀerent\\nproperties. q1 needs to have a relatively low number of repro-\\nduction values so that the number of inverted lists does not\\nexplode. We typically use |C1| ≈\\n√\\nℓ, trained via k-means.\\nFor q2, we can aﬀord to spend more memory for a more ex-\\ntensive representation. The ID of the vector (a 4- or 8-byte\\ninteger) is also stored in the inverted lists, so it makes no\\nsense to have shorter codes than that; i.e., log2 |C2| > 4 × 8.\\nProduct quantizer. We use a product quantizer [25] for q2,\\nwhich provides a large number of reproduction values with-\\nout increasing the processing cost. It interprets the vector y\\nas b sub-vectors y = [y0...yb−1], where b is an even divisor of\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='the dimension d. Each sub-vector is quantized with its own\\nquantizer, yielding the tuple (q0(y0), ..., qb−1(yb−1)). The\\nsub-quantizers typically have 256 reproduction values, to ﬁt\\nin one byte. The quantization value of the product quantizer\\nis then q2(y) = q0(y0) + 256 × q1(y1) + ... + 256b−1 × qb−1,\\nwhich from a storage point of view is just the concatena-\\ntion of the bytes produced by each sub-quantizer. Thus, the\\nproduct quantizer generates b-byte codes with |C2| = 256b\\nreproduction values. The k-means dictionaries of the quan-\\ntizers are small and quantization is computationally cheap.\\n3.\\nGPU: OVERVIEW AND K-SELECTION\\nThis section reviews salient details of Nvidia’s general-\\npurpose GPU architecture and programming model [30]. We\\nthen focus on one of the less GPU-compliant parts involved\\nin similarity search, namely the k-selection, and discuss the\\nliterature and challenges.\\n3.1\\nArchitecture\\nGPU lanes and warps. The Nvidia GPU is a general-\\npurpose computer that executes instruction streams using\\na 32-wide vector of CUDA threads (the warp); individual\\nthreads in the warp are referred to as lanes, with a lane\\nID from 0 – 31. Despite the “thread” terminology, the best\\nanalogy to modern vectorized multicore CPUs is that each\\nwarp is a separate CPU hardware thread, as the warp shares\\nan instruction counter. Warp lanes taking diﬀerent execu-\\ntion paths results in warp divergence, reducing performance.\\nEach lane has up to 255 32-bit registers in a shared register\\nﬁle. The CPU analogy is that there are up to 255 vector\\nregisters of width 32, with warp lanes as SIMD vector lanes.\\nCollections of warps. A user-conﬁgurable collection of 1\\nto 32 warps comprises a block or a co-operative thread ar-\\nray (CTA). Each block has a high speed shared memory, up\\nto 48 KiB in size. Individual CUDA threads have a block-\\nrelative ID, called a thread id, which can be used to parti-\\ntion and assign work. Each block is run on a single core of\\nthe GPU called a streaming multiprocessor (SM). Each SM\\nhas functional units, including ALUs, memory load/store\\nunits, and various special instruction units. A GPU hides\\nexecution latencies by having many operations in ﬂight on\\nwarps across all SMs. Each individual warp lane instruction\\nthroughput is low and latency is high, but the aggregate\\narithmetic throughput of all SMs together is 5 – 10× higher\\nthan typical CPUs.\\nGrids and kernels. Blocks are organized in a grid of blocks\\nin a kernel. Each block is assigned a grid relative ID. The\\nkernel is the unit of work (instruction stream with argu-\\nments) scheduled by the host CPU for the GPU to execute.\\nAfter a block runs through to completion, new blocks can\\nbe scheduled. Blocks from diﬀerent kernels can run concur-\\nrently. Ordering between kernels is controllable via ordering\\nprimitives such as streams and events.\\nResources and occupancy. The number of blocks execut-\\ning concurrently depends upon shared memory and register\\nresources used by each block. Per-CUDA thread register us-\\nage is determined at compilation time, while shared memory\\nusage can be chosen at runtime. This usage aﬀects occu-\\npancy on the GPU. If a block demands all 48 KiB of shared\\nmemory for its private usage, or 128 registers per thread as\\nopposed to 32, then only 1 – 2 other blocks can run concur-\\nrently on the same SM, resulting in low occupancy. Under\\nhigh occupancy more blocks will be present across all SMs,\\nallowing more work to be in ﬂight at once.\\nMemory types. Diﬀerent blocks and kernels communicate\\nthrough global memory, typically 4 – 32 GB in size, with 5 –\\n10× higher bandwidth than CPU main memory.\\nShared\\nmemory is analogous to CPU L1 cache in terms of speed.\\nGPU register ﬁle memory is the highest bandwidth memory.\\nIn order to maintain the high number of instructions in ﬂight\\non a GPU, a vast register ﬁle is also required: 14 MB in the\\nlatest Pascal P100, in contrast with a few tens of KB on\\nCPU. A ratio of 250 : 6.25 : 1 for register to shared to global\\nmemory aggregate cross-sectional bandwidth is typical on\\nGPU, yielding 10 – 100s of TB/s for the register ﬁle [10].\\n3.2\\nGPU register ﬁle usage\\nStructured register data. Shared and register memory\\nusage involves eﬃciency tradeoﬀs; they lower occupancy but\\ncan increase overall performance by retaining a larger work-\\ning set in a faster memory. Making heavy use of register-\\nresident data at the expense of occupancy or instead of\\nshared memory is often proﬁtable [43].\\nAs the GPU register ﬁle is very large, storing structured\\ndata (not just temporary operands) is useful. A single lane\\ncan use its (scalar) registers to solve a local task, but with\\nlimited parallelism and storage.\\nInstead, lanes in a GPU\\nwarp can instead exchange register data using the warp shuf-\\nﬂe instruction, enabling warp-wide parallelism and storage.\\nLane-stride register array. A common pattern to achieve\\nthis is a lane-stride register array. That is, given elements\\n[ai]i=0:ℓ, each successive value is held in a register by neigh-\\nboring lanes. The array is stored in ℓ/32 registers per lane,\\nwith ℓa multiple of 32. Lane j stores {aj, a32+j, ..., aℓ−32+j},\\nwhile register r holds {a32r, a32r+1, ..., a32r+31}.\\nFor manipulating the [ai], the register in which ai is stored\\n(i.e., ⌊i/32⌋) and ℓmust be known at assembly time, while\\nthe lane (i.e., i mod 32) can be runtime knowledge. A wide\\nvariety of access patterns (shift, any-to-any) are provided;\\nwe use the butterﬂy permutation [29] extensively.\\n3.3\\nk-selection on CPU versus GPU\\nk-selection algorithms, often for arbitrarily large ℓand\\nk, can be translated to a GPU, including radix selection\\nand bucket selection [1], probabilistic selection [33], quick-\\nselect [14], and truncated sorts [40]. Their performance is\\ndominated by multiple passes over the input in global mem-\\nory. Sometimes for similarity search, the input distances are\\ncomputed on-the-ﬂy or stored only in small blocks, not in\\ntheir entirety. The full, explicit array might be too large to\\nﬁt into any memory, and its size could be unknown at the\\nstart of the processing, rendering algorithms that require\\nmultiple passes impractical. They suﬀer from other issues\\nas well.\\nQuickselect requires partitioning on a storage of\\nsize O(ℓ), a data-dependent memory movement. This can\\nresult in excessive memory transactions, or requiring parallel\\npreﬁx sums to determine write oﬀsets, with synchronization\\noverhead. Radix selection has no partitioning but multiple\\npasses are still required.\\nHeap parallelism. In similarity search applications, one\\nis usually interested only in a small number of results, k <\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='1000 or so. In this regime, selection via max-heap is a typi-\\ncal choice on the CPU, but heaps do not expose much data\\nparallelism (due to serial tree update) and cannot saturate\\nSIMD execution units. The ad-heap [31] takes better advan-\\ntage of parallelism available in heterogeneous systems, but\\nstill attempts to partition serial and parallel work between\\nappropriate execution units.\\nDespite the serial nature of\\nheap update, for small k the CPU can maintain all of its\\nstate in the L1 cache with little eﬀort, and L1 cache latency\\nand bandwidth remains a limiting factor. Other similarity\\nsearch components, like PQ code manipulation, tend to have\\ngreater impact on CPU performance [2].\\nGPU heaps.\\nHeaps can be similarly implemented on a\\nGPU [7]. However, a straightforward GPU heap implemen-\\ntation suﬀers from high warp divergence and irregular, data-\\ndependent memory movement, since the path taken for each\\ninserted element depends upon other values in the heap.\\nGPU parallel priority queues [24] improve over the serial\\nheap update by allowing multiple concurrent updates, but\\nthey require a potential number of small sorts for each insert\\nand data-dependent memory movement. Moreover, it uses\\nmultiple synchronization barriers through kernel launches in\\ndiﬀerent streams, plus the additional latency of successive\\nkernel launches and coordination with the CPU host.\\nOther more novel GPU algorithms are available for small\\nk, namely the selection algorithm in the fgknn library [41].\\nThis is a complex algorithm that may suﬀer from too many\\nsynchronization points, greater kernel launch overhead, us-\\nage of slower memories, excessive use of hierarchy, partition-\\ning and buﬀering. However, we take inspiration from this\\nparticular algorithm through the use of parallel merges as\\nseen in their merge queue structure.\\n4.\\nFAST K-SELECTION ON THE GPU\\nFor any CPU or GPU algorithm, either memory or arith-\\nmetic throughput should be the limiting factor as per the\\nrooﬂine performance model [48]. For input from global mem-\\nory, k-selection cannot run faster than the time required to\\nscan the input once at peak memory bandwidth. We aim to\\nget as close to this limit as possible. Thus, we wish to per-\\nform a single pass over the input data (from global memory\\nor produced on-the-ﬂy, perhaps fused with a kernel that is\\ngenerating the data).\\nWe want to keep intermediate state in the fastest memory:\\nthe register ﬁle. The major disadvantage of register memory\\nis that the indexing into the register ﬁle must be known at\\nassembly time, which is a strong constraint on the algorithm.\\n4.1\\nIn-register sorting\\nWe use an in-register sorting primitive as a building block.\\nSorting networks are commonly used on SIMD architec-\\ntures [13], as they exploit vector parallelism. They are eas-\\nily implemented on the GPU, and we build sorting networks\\nwith lane-stride register arrays.\\nWe use a variant of Batcher’s bitonic sorting network [8],\\nwhich is a set of parallel merges on an array of size 2k. Each\\nmerge takes s arrays of length t (s and t a power of 2) to s/2\\narrays of length 2t, using log2(t) parallel steps. A bitonic\\nsort applies this merge recursively: to sort an array of length\\nℓ, merge ℓarrays of length 1 to ℓ/2 arrays of length 2, to ℓ/4\\narrays of length 4, successively to 1 sorted array of length ℓ,\\nleading to 1\\n2(log2(ℓ)2 + log2(ℓ)) parallel merge steps.\\nAlgorithm 1 Odd-size merging network\\nfunction merge-odd([Li]i=0:ℓL, [Ri]i=0:ℓR)\\nparallel for i ←0 : min(ℓL, ℓR) do\\n▷inverted 1st stage; inputs are already sorted\\ncompare-swap(LℓL−i−1, Ri)\\nend for\\nparallel do\\n▷If ℓL = ℓR and a power-of-2, these are equivalent\\nmerge-odd-continue([Li]i=0:ℓL, left)\\nmerge-odd-continue([Ri]i=0:ℓR, right)\\nend do\\nend function\\nfunction merge-odd-continue([xi]i=0:ℓ, p)\\nif ℓ> 1 then\\nh ←2⌈log2 ℓ⌉−1\\n▷largest power-of-2 < ℓ\\nparallel for i ←0 : ℓ−h do\\n▷Implemented with warp shuﬄe butterﬂy\\ncompare-swap(xi, xi+h)\\nend for\\nparallel do\\nif p = left then\\n▷left side recursion\\nmerge-odd-continue([xi]i=0:ℓ−h, left)\\nmerge-odd-continue([xi]i=ℓ−h:ℓ, right)\\nelse\\n▷right side recursion\\nmerge-odd-continue([xi]i=0:h, left)\\nmerge-odd-continue([xi]i=h:ℓ, right)\\nend if\\nend do\\nend if\\nend function\\nOdd-size merging and sorting networks. If some input\\ndata is already sorted, we can modify the network to avoid\\nmerging steps. We may also not have a full power-of-2 set of\\ndata, in which case we can eﬃciently shortcut to deal with\\nthe smaller size.\\nAlgorithm 1 is an odd-sized merging network that merges\\nalready sorted left and right arrays, each of arbitrary length.\\nWhile the bitonic network merges bitonic sequences, we start\\nwith monotonic sequences: sequences sorted monotonically.\\nA bitonic merge is made monotonic by reversing the ﬁrst\\ncomparator stage.\\nThe odd size algorithm is derived by considering arrays to\\nbe padded to the next highest power-of-2 size with dummy\\n1\\n3\\n4\\n8\\n9\\n0\\n3\\n7\\n1\\n3\\n4\\n3\\n0\\n9\\n8\\n7\\n0\\n3\\n4\\n3\\n1\\n7\\n8\\n9\\n0\\n3\\n1\\n3\\n4\\n7\\n8\\n9\\n0\\n1\\n3\\n3\\n4\\n7\\n8\\n9\\nstep 1\\nstep 2\\nstep 3\\nstep 4\\nFigure 1:\\nOdd-size network merging arrays of sizes\\n5 and 3.\\nBullets indicate parallel compare/swap.\\nDashed lines are elided elements or comparisons.\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='input \\ninsertion\\nthread queue\\nmerging  network\\nwarp queue\\nlane 0\\nlane 1\\nlane 31\\ncoalesced \\nread\\n. . . . .\\n. . . . .\\n. . . . .\\n. . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . .  \\n. . . . . . . . . . . . . . . . . . . . \\n. . . . . . . . . \\n. . . . . . . . . . . .\\nFigure 2:\\nOverview of WarpSelect. The input val-\\nues stream in on the left, and the warp queue on the\\nright holds the output result.\\nelements that are never swapped (the merge is monotonic)\\nand are already properly positioned; any comparisons with\\ndummy elements are elided. A left array is considered to\\nbe padded with dummy elements at the start; a right ar-\\nray has them at the end.\\nA merge of two sorted arrays\\nof length ℓL and ℓR to a sorted array of ℓL + ℓR requires\\n⌈log2(max(ℓL, ℓR))⌉+1 parallel steps. Figure 1 shows Algo-\\nrithm 1’s merging network for arrays of size 5 and 3, with 4\\nparallel steps.\\nThe compare-swap is implemented using warp shuﬄes on\\na lane-stride register array. Swaps with a stride a multiple\\nof 32 occur directly within a lane as the lane holds both\\nelements locally. Swaps of stride ≤16 or a non-multiple of\\n32 occur with warp shuﬄes. In practice, used array lengths\\nare multiples of 32 as they are held in lane-stride arrays.\\nAlgorithm 2 Odd-size sorting network\\nfunction sort-odd([xi]i=0:ℓ)\\nif ℓ> 1 then\\nparallel do\\nsort-odd([xi]i=0:⌊ℓ/2⌋)\\nsort-odd([xi]i=⌊ℓ/2⌋:ℓ)\\nend do\\nmerge-odd([xi]i=0:⌊ℓ/2⌋, [xi]i=⌊ℓ/2⌋:ℓ)\\nend if\\nend function\\nAlgorithm 2 extends the merge to a full sort. Assuming no\\nstructure present in the input data, 1\\n2(⌈log2(ℓ)⌉2+⌈log2(ℓ)⌉)\\nparallel steps are required for sorting data of length ℓ.\\n4.2\\nWarpSelect\\nOur k-selection implementation, WarpSelect, maintains\\nstate entirely in registers, requires only a single pass over\\ndata and avoids cross-warp synchronization. It uses merge-\\nodd and sort-odd as primitives. Since the register ﬁle pro-\\nvides much more storage than shared memory, it supports\\nk ≤1024. Each warp is dedicated to k-selection to a single\\none of the n arrays [ai]. If n is large enough, a single warp\\nper each [ai] will result in full GPU occupancy. Large ℓper\\nwarp is handled by recursive decomposition, if ℓis known in\\nadvance.\\nOverview. Our approach (Algorithm 3 and Figure 2) oper-\\nates on values, with associated indices carried along (omit-\\nted from the description for simplicity). It selects the k least\\nvalues that come from global memory, or from intermediate\\nvalue registers if fused into another kernel providing the val-\\nues. Let [ai]i=0:ℓbe the sequence provided for selection.\\nThe elements (on the left of Figure 2) are processed in\\ngroups of 32, the warp size. Lane j is responsible for pro-\\ncessing {aj, a32+j, ...}; thus, if the elements come from global\\nmemory, the reads are contiguous and coalesced into a min-\\nimal number of memory transactions.\\nData structures.\\nEach lane j maintains a small queue\\nof t elements in registers, called the thread queues [T j\\ni ]i=0:t,\\nordered from largest to smallest (T j\\ni ≥T j\\ni+1). The choice of\\nt is made relative to k, see Section 4.3. The thread queue is\\na ﬁrst-level ﬁlter for new values coming in. If a new a32i+j\\nis greater than the largest key currently in the queue, T j\\n0 , it\\nis guaranteed that it won’t be in the k smallest ﬁnal results.\\nThe warp shares a lane-stride register array of k smallest\\nseen elements, [Wi]i=0:k, called the warp queue. It is ordered\\nfrom smallest to largest (Wi ≤Wi+1); if the requested k is\\nnot a multiple of 32, we round it up. This is a second level\\ndata structure that will be used to maintain all of the k\\nsmallest warp-wide seen values. The thread and warp queues\\nare initialized to maximum sentinel values, e.g., +∞.\\nUpdate. The three invariants maintained are:\\n• all per-lane T j\\n0 are not in the min-k\\n• all per-lane T j\\n0 are greater than all warp queue keys\\nWi\\n• all ai seen so far in the min-k are contained in either\\nsome lane’s thread queue ([T j\\ni ]i=0:t,j=0:32), or in the\\nwarp queue.\\nLane j receives a new a32i+j and attempts to insert it into\\nits thread queue. If a32i+j > T j\\n0 , then the new pair is by\\ndeﬁnition not in the k minimum, and can be rejected.\\nOtherwise, it is inserted into its proper sorted position\\nin the thread queue, thus ejecting the old T j\\n0 .\\nAll lanes\\ncomplete doing this with their new received pair and their\\nthread queue, but it is now possible that the second invariant\\nhave been violated. Using the warp ballot instruction, we\\ndetermine if any lane has violated the second invariant. If\\nnot, we are free to continue processing new elements.\\nRestoring the invariants. If any lane has its invariant\\nviolated, then the warp uses odd-merge to merge and sort\\nthe thread and warp queues together. The new warp queue\\nAlgorithm 3 WarpSelect pseudocode for lane j\\nfunction WarpSelect(a)\\nif a < T j\\n0 then\\ninsert a into our [T j\\ni ]i=0:t\\nend if\\nif warp-ballot(T j\\n0 < Wk−1) then\\n▷Reinterpret thread queues as lane-stride array\\n[αi]i=0:32t ←cast([T j\\ni ]i=0:t,j=0:32)\\n▷concatenate and sort thread queues\\nsort-odd([αi]i=0:32t)\\nmerge-odd([Wi]i=0:k, [αi]i=0:32t)\\n▷Reinterpret lane-stride array as thread queues\\n[T j\\ni ]i=0:t,j=0:32 ←cast([αi]i=0:32t)\\nreverse-array([Ti]i=0:t)\\n▷Back in thread queue order, invariant restored\\nend if\\nend function\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='will be the min-k elements across the merged, sorted queues,\\nand the new thread queues will be the remainder, from min-\\n(k +1) to min-(k +32t+1). This restores the invariants and\\nwe are free to continue processing subsequent elements.\\nSince the thread and warp queues are already sorted, we\\nmerge the sorted warp queue of length k with 32 sorted\\narrays of length t. Supporting odd-sized merges is important\\nbecause Batcher’s formulation would require that 32t = k\\nand is a power-of-2; thus if k = 1024, t must be 32. We\\nfound that the optimal t is way smaller (see below).\\nUsing odd-merge to merge the 32 already sorted thread\\nqueues would require a struct-of-arrays to array-of-structs\\ntransposition in registers across the warp, since the t succes-\\nsive sorted values are held in diﬀerent registers in the same\\nlane rather than a lane-stride array. This is possible [12],\\nbut would use a comparable number of warp shuﬄes, so we\\njust reinterpret the thread queue registers as an (unsorted)\\nlane-stride array and sort from scratch. Signiﬁcant speedup\\nis realizable by using odd-merge for the merge of the ag-\\ngregate sorted thread queues with the warp queue.\\nHandling the remainder. If there are remainder elements\\nbecause ℓis not a multiple of 32, those are inserted into the\\nthread queues for the lanes that have them, after which we\\nproceed to the output stage.\\nOutput. A ﬁnal sort and merge is made of the thread and\\nwarp queues, after which the warp queue holds all min-k\\nvalues.\\n4.3\\nComplexity and parameter selection\\nFor each incoming group of 32 elements, WarpSelect\\ncan perform 1, 2 or 3 constant-time operations, all happen-\\ning in warp-wide parallel time:\\n1. read 32 elements, compare to all thread queue heads\\nT j\\n0 , cost C1, happens N1 times;\\n2. if ∃j ∈{0, ..., 31}, a32n+j < T j\\n0 , perform insertion sort\\non those speciﬁc thread queues, cost C2 = O(t), hap-\\npens N2 times;\\n3. if ∃j, T j\\n0 < Wk−1, sort and merge queues, cost C3 =\\nO(t log(32t)2 + k log(max(k, 32t))), happens N3 times.\\nThus, the total cost is N1C1 + N2C2 + N3C3. N1 = ℓ/32,\\nand on random data drawn independently, N2 = O(k log(ℓ))\\nand N3 = O(k log(ℓ)/t), see the Appendix for a full deriva-\\ntion. Hence, the trade-oﬀis to balance a cost in N2C2 and\\none in N3C3. The practical choice for t given k and ℓwas\\nmade by experiment on a variety of k-NN data. For k ≤32,\\nwe use t = 2, k ≤128 uses t = 3, k ≤256 uses t = 4, and\\nk ≤1024 uses t = 8, all irrespective of ℓ.\\n5.\\nCOMPUTATION LAYOUT\\nThis section explains how IVFADC, one of the indexing\\nmethods originally built upon product quantization [25], is\\nimplemented eﬃciently. Details on distance computations\\nand articulation with k-selection are the key to understand-\\ning why this method can outperform more recent GPU-\\ncompliant approximate nearest neighbor strategies [47].\\n5.1\\nExact search\\nWe brieﬂy come back to the exhaustive search method,\\noften referred to as exact brute-force. It is interesting on its\\nown for exact nearest neighbor search in small datasets. It\\nis also a component of many indexes in the literature. In\\nour case, we use it for the IVFADC coarse quantizer q1.\\nAs stated in Section 2, the distance computation boils\\ndown to a matrix multiplication. We use optimized GEMM\\nroutines in the cuBLAS library to calculate the −2⟨xj, yi⟩\\nterm for L2 distance, resulting in a partial distance matrix\\nD′. To complete the distance calculation, we use a fused\\nk-selection kernel that adds the ∥yi∥2 term to each entry of\\nthe distance matrix and immediately submits the value to\\nk-selection in registers. The ∥xj∥2 term need not be taken\\ninto account before k-selection. Kernel fusion thus allows\\nfor only 2 passes (GEMM write, k-select read) over D′, com-\\npared to other implementations that may require 3 or more.\\nRow-wise k-selection is likely not fusable with a well-tuned\\nGEMM kernel, or would result in lower overall eﬃciency.\\nAs D′ does not ﬁt in GPU memory for realistic problem\\nsizes, the problem is tiled over the batch of queries, with\\ntq ≤nq queries being run in a single tile. Each of the ⌈nq/tq⌉\\ntiles are independent problems, but we run two in parallel\\non diﬀerent streams to better occupy the GPU, so the eﬀec-\\ntive memory requirement of D is O(2ℓtq). The computation\\ncan similarly be tiled over ℓ. For very large input coming\\nfrom the CPU, we support buﬀering with pinned memory\\nto overlap CPU to GPU copy with GPU compute.\\n5.2\\nIVFADC indexing\\nPQ lookup tables. At its core, the IVFADC requires com-\\nputing the distance from a vector to a set of product quanti-\\nzation reproduction values. By developing Equation (6) for\\na database vector y, we obtain:\\n∥x −q(y)∥2\\n2 = ∥x −q1(y) −q2(y −q1(y))∥2\\n2.\\n(7)\\nIf we decompose the residual vectors left after q1 as:\\ny −q1(y)\\n=\\n[ ey1 · · · eyb] and\\n(8)\\nx −q1(y)\\n=\\n[f\\nx1 · · · exb]\\n(9)\\nthen the distance is rewritten as:\\n∥x −q(y)∥2\\n2 = ∥f\\nx1 −q1( ey1)∥2\\n2 + ... + ∥exb −qb( eyb)∥2\\n2.\\n(10)\\nEach quantizer q1, ..., qb has 256 reproduction values, so\\nwhen x and q1(y) are known all distances can be precom-\\nputed and stored in tables T1, ..., Tb each of size 256 [25].\\nComputing the sum (10) consists of b look-ups and addi-\\ntions. Comparing the cost to compute n distances:\\n• Explicit computation: n × d mutiply-adds;\\n• With lookup tables: 256 × d multiply-adds and n × b\\nlookup-adds.\\nThis is the key to the eﬃciency of the product quantizer.\\nIn our GPU implementation, b is any multiple of 4 up to\\n64. The codes are stored as sequential groups of b bytes per\\nvector within lists.\\nIVFADC lookup tables.\\nWhen scanning over the ele-\\nments of the inverted list IL (where by deﬁnition q1(y) is\\nconstant), the look-up table method can be applied, as the\\nquery x and q1(y) are known.\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='Moreover, the computation of the tables T1 . . . Tb is fur-\\nther optimized [5]. The expression of ∥x−q(y)∥2\\n2 in Equation\\n(7) can be decomposed as:\\n∥q2(...)∥2\\n2 + 2⟨q1(y), q2(...)⟩\\n|\\n{z\\n}\\nterm 1\\n+ ∥x −q1(y)∥2\\n2\\n|\\n{z\\n}\\nterm 2\\n−2 ⟨x, q2(...)⟩\\n|\\n{z\\n}\\nterm 3\\n.\\n(11)\\nThe objective is to minimize inner loop computations.\\nThe computations we can do in advance and store in lookup\\ntables are as follows:\\n• Term 1 is independent of the query. It can be precom-\\nputed from the quantizers, and stored in a table T of\\nsize |C1| × 256 × b;\\n• Term 2 is the distance to q1’s reproduction value. It is\\nthus a by-product of the ﬁrst-level quantizer q1;\\n• Term 3 can be computed independently of the inverted\\nlist. Its computation costs d × 256 multiply-adds.\\nThis decomposition is used to produce the lookup tables\\nT1 . . . Tb used during the scan of the inverted list.\\nFor a\\nsingle query, computing the τ × b tables from scratch costs\\nτ × d × 256 multiply-adds, while this decomposition costs\\n256×d multiply-adds and τ×b×256 additions. On the GPU,\\nthe memory usage of T can be prohibitive, so we enable the\\ndecomposition only when memory is a not a concern.\\n5.3\\nGPU implementation\\nAlgorithm 4 summarizes the process as one would im-\\nplement it on a CPU. The inverted lists are stored as two\\nseparate arrays, for PQ codes and associated IDs. IDs are\\nresolved only if k-selection determines k-nearest member-\\nship. This lookup yields a few sparse memory reads in a\\nlarge array, thus the IDs can optionally be stored on CPU\\nfor tiny performance cost.\\nList scanning. A kernel is responsible for scanning the τ\\nclosest inverted lists for each query, and calculating the per-\\nvector pair distances using the lookup tables Ti. The Ti are\\nstored in shared memory: up to nq×τ ×maxi |Ii|×b lookups\\nare required for a query set (trillions of accesses in practice),\\nand are random access.\\nThis limits b to at most 48 (32-\\nbit ﬂoating point) or 96 (16-bit ﬂoating point) with current\\narchitectures. In case we do not use the decomposition of\\nEquation (11), the Ti are calculated by a separate kernel\\nbefore scanning.\\nMulti-pass kernels. Each nq × τ pairs of query against\\ninverted list can be processed independently.\\nAt one ex-\\ntreme, a block is dedicated to each of these, resulting in up\\nto nq × τ × maxi |Ii| partial results being written back to\\nglobal memory, which is then k-selected to nq × k ﬁnal re-\\nsults. This yields high parallelism but can exceed available\\nGPU global memory; as with exact search, we choose a tile\\nsize tq ≤nq to reduce memory consumption, bounding its\\ncomplexity by O(2tqτ maxi |Ii|) with multi-streaming.\\nA single warp could be dedicated to k-selection of each\\ntq set of lists, which could result in low parallelism.\\nWe\\nintroduce a two-pass k-selection, reducing tq × τ × maxi |Ii|\\nto tq × f × k partial results for some subdivision factor f.\\nThis is reduced again via k-selection to the ﬁnal tq×k results.\\nFused kernel. As with exact search, we experimented with\\na kernel that dedicates a single block to scanning all τ lists\\nfor a single query, with k-selection fused with distance com-\\nputation. This is possible as WarpSelect does not ﬁght for\\nthe shared memory resource which is severely limited. This\\nreduces global memory write-back, since almost all interme-\\ndiate results can be eliminated. However, unlike k-selection\\noverhead for exact computation, a signiﬁcant portion of the\\nruntime is the gather from the Ti in shared memory and lin-\\near scanning of the Ii from global memory; the write-back is\\nnot a dominant contributor. Timing for the fused kernel is\\nimproved by at most 15%, and for some problem sizes would\\nbe subject to lower parallelism and worse performance with-\\nout subsequent decomposition. Therefore, and for reasons\\nof implementation simplicity, we do not use this layout.\\nAlgorithm 4 IVFPQ batch search routine\\nfunction ivfpq-search([x1, ..., xnq], I1, ..., I|C1|)\\nfor i ←0 : nq do ▷batch quantization of Section 5.1\\nLi\\nIVF ←τ-argminc∈C1∥x −c∥2\\nend for\\nfor i ←0 : nq do\\nL ←[]\\n▷distance table\\nCompute term 3 (see Section 5.2)\\nfor L in Li\\nIVF do\\n▷τ loops\\nCompute distance tables T1, ..., Tb\\nfor j in IL do\\n▷distance estimation, Equation (10)\\nd ←∥xi −q(yj)∥2\\n2\\nAppend (d, L, j) to L\\nend for\\nend for\\nRi ←k-select smallest distances d from L\\nend for\\nreturn R\\nend function\\n5.4\\nMulti-GPU parallelism\\nModern servers can support several GPUs. We employ\\nthis capability for both compute power and memory.\\nReplication. If an index instance ﬁts in the memory of a\\nsingle GPU, it can be replicated across R diﬀerent GPUs. To\\nquery nq vectors, each replica handles a fraction nq/R of the\\nqueries, joining the results back together on a single GPU\\nor in CPU memory. Replication has near linear speedup,\\nexcept for a potential loss in eﬃciency for small nq.\\nSharding. If an index instance does not ﬁt in the memory\\nof a single GPU, an index can be sharded across S diﬀer-\\nent GPUs. For adding ℓvectors, each shard receives ℓ/S of\\nthe vectors, and for query, each shard handles the full query\\nset nq, joining the partial results (an additional round of k-\\nselection is still required) on a single GPU or in CPU mem-\\nory. For a given index size ℓ, sharding will yield a speedup\\n(sharding has a query of nq against ℓ/S versus replication\\nwith a query of nq/R against ℓ), but is usually less than\\npure replication due to ﬁxed overhead and cost of subse-\\nquent k-selection.\\nReplication and sharding can be used together (S shards,\\neach with R replicas for S × R GPUs in total). Sharding or\\nreplication are both fairly trivial, and the same principle can\\nbe used to distribute an index across multiple machines.\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='\\x01\\x01\\x02\\x03\\n\\x01\\x03\\n\\x01\\x03\\x01\\n\\x01\\x03\\x01\\x01\\n\\x01\\x03\\x01\\x04\\x05\\n\\x01\\x05\\x01\\x06\\x07\\n\\x01\\x03\\x07\\x08\\t\\x05\\n\\x01\\x07\\n\\n\\x08\\x07\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x01\\x08\\x06\\t\\n\\x0b\\x0c\\x0c\\x0b\\r\\x01\\x0e\\x0f\\x10\\x11\\x12\\x13\\n\\x12\\x0c\\x14\\x10\\x15\\x0b\\x12\\x0f\\x16\\x01\\x17\\x18\\x12\\x19\\x10\\x18\\x15\\x01\\x1a\\x19\\x0c\\x12\\n\\x1b\\x11\\x1c\\x10\\x10\\x01\\x1a\\x0f\\x0e\\x0f\\x15\\x12\\n\\x1d\\x0b\\x0c\\x1e\\x1f\\x0f\\x0e\\x0f\\x15\\x12\\n \\x0f \\x19\\x0c\\r\\x01\\x17\\x0b\\x10\\x16!\\x18\\x16\\x12\\x13\\x01\\x0e\\x18 \\x18\\x12\\nFigure 3:\\nRuntimes for diﬀerent k-selection meth-\\nods, as a function of array length ℓ. Simultaneous\\narrays processed are nq = 10000. k = 100 for full lines,\\nk = 1000 for dashed lines.\\n6.\\nEXPERIMENTS & APPLICATIONS\\nThis section compares our GPU k-selection and nearest-\\nneighbor approach to existing libraries. Unless stated other-\\nwise, experiments are carried out on a 2×2.8GHz Intel Xeon\\nE5-2680v2 with 4 Maxwell Titan X GPUs on CUDA 8.0.\\n6.1\\nk-selection performance\\nWe compare against two other GPU small k-selection im-\\nplementations: the row-based Merge Queue with Buﬀered\\nSearch and Hierarchical Partition extracted from the fgknn\\nlibrary of Tang et al. [41] and Truncated Bitonic Sort (TBiS)\\nfrom Sismanis et al. [40]. Both were extracted from their re-\\nspective exact search libraries.\\nWe evaluate k-selection for k = 100 and 1000 of each row\\nfrom a row-major matrix nq × ℓof random 32-bit ﬂoating\\npoint values on a single Titan X. The batch size nq is ﬁxed\\nat 10000, and the array lengths ℓvary from 1000 to 128000.\\nInputs and outputs to the problem remain resident in GPU\\nmemory, with the output being of size nq × k, with corre-\\nsponding indices. Thus, the input problem sizes range from\\n40 MB (ℓ= 1000) to 5.12 GB (ℓ= 128k). TBiS requires large\\nauxiliary storage, and is limited to ℓ≤48000 in our tests.\\nFigure 3 shows our relative performance against TBiS and\\nfgknn. It also includes the peak possible performance given\\nby the memory bandwidth limit of the Titan X. The rela-\\ntive performance of WarpSelect over fgknn increases for\\nlarger k; even TBiS starts to outperform fgknn for larger ℓ\\nat k = 1000. We look especially at the largest ℓ= 128000.\\nWarpSelect is 1.62× faster at k = 100, 2.01× at k = 1000.\\nPerformance against peak possible drops oﬀfor all imple-\\nmentations at larger k. WarpSelect operates at 55% of\\npeak at k = 100 but only 16% of peak at k = 1000. This\\nis due to additional overhead assocated with bigger thread\\nqueues and merge/sort networks for large k.\\nDiﬀerences from fgknn. WarpSelect is inﬂuenced by\\nfgknn, but has several improvements: all state is maintained\\nin registers (no shared memory), no inter-warp synchroniza-\\ntion or buﬀering is used, no “hierarchical partition”, the k-\\nselection can be fused into other kernels, and it uses odd-size\\nnetworks for eﬃcient merging and sorting.\\n# centroids\\nmethod\\n# GPUs\\n256\\n4096\\nBIDMach [11]\\n1\\n320 s\\n735 s\\nOurs\\n1\\n140 s\\n316 s\\nOurs\\n4\\n84 s\\n100 s\\nTable 1:\\nMNIST8m k-means performance\\n6.2\\nk-means clustering\\nThe exact search method with k = 1 can be used by a k-\\nmeans clustering method in the assignment stage, to assign\\nnq training vectors to |C1| centroids. Despite the fact that\\nit does not use the IVFADC and k = 1 selection is trivial (a\\nparallel reduction is used for the k = 1 case, not WarpSe-\\nlect), k-means is a good benchmark for the clustering used\\nto train the quantizer q1.\\nWe apply the algorithm on MNIST8m images. The 8.1M\\nimages are graylevel digits in 28x28 pixels, linearized to vec-\\ntors of 784-d. We compare this k-means implementation to\\nthe GPU k-means of BIDMach [11], which was shown to be\\nmore eﬃcient than several distributed k-means implemen-\\ntations that require dozens of machines3. Both algorithms\\nwere run for 20 iterations. Table 1 shows that our imple-\\nmentation is more than 2× faster, although both are built\\nupon cuBLAS. Our implementation receives some beneﬁt\\nfrom the k-selection fusion into L2 distance computation.\\nFor multi-GPU execution via replicas, the speedup is close\\nto linear for large enough problems (3.16× for 4 GPUs with\\n4096 centroids). Note that this benchmark is somewhat un-\\nrealistic, as one would typically sub-sample the dataset ran-\\ndomly when so few centroids are requested.\\nLarge scale. We can also compare to [3], an approximate\\nCPU method that clusters 108 128-d vectors to 85k cen-\\ntroids. Their clustering method runs in 46 minutes, but re-\\nquires 56 minutes (at least) of pre-processing to encode the\\nvectors. Our method performs exact k-means on 4 GPUs in\\n52 minutes without any pre-processing.\\n6.3\\nExact nearest neighbor search\\nWe consider a classical dataset used to evaluate nearest\\nneighbor search: Sift1M [25]. Its characteristic sizes are\\nℓ= 106, d = 128, nq = 104. Computing the partial distance\\nmatrix D′ costs nq × ℓ× d = 1.28 Tﬂop, which runs in less\\nthan one second on current GPUs. Figure 4 shows the cost\\nof the distance computations against the cost of our tiling\\nof the GEMM for the −2 ⟨xj, yi⟩term of Equation 2 and\\nthe peak possible k-selection performance on the distance\\nmatrix of size nq×ℓ, which additionally accounts for reading\\nthe tiled result matrix D′ at peak memory bandwidth.\\nIn addition to our method from Section 5, we include\\ntimes from the two GPU libraries evaluated for k-selection\\nperformance in Section 6.1. We make several observations:\\n• for k-selection, the naive algorithm that sorts the full\\nresult array for each query using thrust::sort_by_key\\nis more than 10× slower than the comparison methods;\\n• L2 distance and k-selection cost is dominant for all but\\nour method, which has 85 % of the peak possible\\nperformance, assuming GEMM usage and our tiling\\n3BIDMach numbers from https://github.com/BIDData/\\nBIDMach/wiki/Benchmarks#KMeans\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='\\x01\\x01\\n\\x01\\x01\\x02\\x03\\n\\x01\\x04\\n\\x01\\x04\\x02\\x03\\n\\x01\\x05\\n\\x01\\x05\\x02\\x03\\n\\x01\\x06\\n\\x01\\x06\\x02\\x03\\n\\x01\\x07\\n\\x01\\x04\\n\\x01\\x07\\n\\x01\\x04\\x08\\n\\x01\\x08\\x07\\n\\x01\\x05\\x03\\x08\\n\\x01\\x04\\x01\\x05\\x07\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x01\\x08\\t\\n\\t\\x01\\n\\n\\x05\\x0b\\x0c\\x01\\r\\x0e\\x0f\\x10\\x10\\x01\\x11\\x12\\x13\\x01\\x14\\x15\\x16\\x17\\x18\\x19\\n\\x1a\\x17\\x12\\t\\x01\\x1a\\x1b\\x13\\x13\\x15\\x1c\\x16\\x17\\x01\\t\\n\\x13\\x17\\x16\\x17\\x1d\\x14\\n\\x1b\\x1e\\x1f\\x01 \\x17\\x14!\\x1b\\x18\\n\\x14\\x1f\\x1e\"\\x1d\\x12\\x14\\x17\\x18\\x01\\x1c\\x15\\x14\\x1b\"\\x15\\x1d\\x01\\x13\\x1b\\x1f\\x14\\n#$\\t\"\"\\nFigure 4:\\nExact search k-NN time for the SIFT1M\\ndataset with varying k on 1 Titan X GPU.\\nof the partial distance matrix D′ on top of GEMM is\\nclose to optimal. The cuBLAS GEMM itself has low\\neﬃciency for small reduction sizes (d = 128);\\n• Our fused L2/k-selection kernel is important.\\nOur\\nsame exact algorithm without fusion (requiring an ad-\\nditional pass through D′) is at least 25% slower.\\nEﬃcient k-selection is even more important in situations\\nwhere approximate methods are used to compute distances,\\nbecause the relative cost of k-selection with respect to dis-\\ntance computation increases.\\n6.4\\nBillion-scale approximate search\\nThere are few studies on GPU-based approximate nearest-\\nneighbor search on large datasets (ℓ≫106). We report a\\nfew comparison points here on index search, using standard\\ndatasets and evaluation protocol in this ﬁeld.\\nSIFT1M. For the sake of completeness, we ﬁrst compare\\nour GPU search speed on Sift1M with the implementation\\nof Wieschollek et al. [47]. They obtain a nearest neighbor re-\\ncall at 1 (fraction of queries where the true nearest neighbor\\nis in the top 1 result) of R@1 = 0.51, and R@100 = 0.86 in\\n0.02 ms per query on a Titan X. For the same time budget,\\nour implementation obtains R@1 = 0.80 and R@100 = 0.95.\\nSIFT1B. We compare again with Wieschollek et al., on the\\nSift1B dataset [26] of 1 billion SIFT image features at nq =\\n104. We compare the search performance in terms of same\\nmemory usage for similar accuracy (more accurate methods\\nmay involve greater search time or memory usage). On a\\nsingle GPU, with m = 8 bytes per vector, R@10 = 0.376 in\\n17.7 µs per query vector, versus their reported R@10 = 0.35\\nin 150 µs per query vector.\\nThus, our implementation is\\nmore accurate at a speed 8.5× faster.\\nDEEP1B. We also experimented on the Deep1B dataset [6]\\nof ℓ=1 billion CNN representations for images at nq = 104.\\nThe paper that introduces the dataset reports CPU results\\n(1 thread): R@1 = 0.45 in 20 ms search time per vector. We\\nuse a PQ encoding of m = 20, with d = 80 via OPQ [17],\\nand |C1| = 218, which uses a comparable dataset storage as\\nthe original paper (20 GB). This requires multiple GPUs as\\nit is too large for a single GPU’s global memory, so we con-\\nsider 4 GPUs with S = 2, R = 2. We obtain a R@1 = 0.4517\\nin 0.0133 ms per vector. While the hardware platforms are\\n\\x01\\x01\\n\\x01\\x02\\x01\\n\\x01\\x03\\x01\\n\\x01\\x04\\x01\\n\\x01\\x05\\x01\\n\\x01\\x06\\x01\\x01\\n\\x01\\x06\\x02\\x01\\n\\x01\\x01\\x07\\x06\\n\\x01\\x01\\x07\\x02\\n\\x01\\x01\\x07\\x08\\n\\x01\\x01\\x07\\x03\\n\\x01\\x01\\x07\\t\\n\\x01\\x01\\x07\\x04\\n\\x01\\x01\\x07\\n\\x01\\x01\\x07\\x05\\n\\x01\\x01\\x07\\x0b\\n\\x01\\x02\\x03\\x03\\x01\\x04\\x05\\x06\\x07\\x08\\x01\\t\\n\\x0b\\x0c\\r\\x01\\x0e\\x0b\\x0f\\x10\\x01\\x11\\x0f\\x0b\\x12\\x13\\n\\x06\\x01\\x0c\\r\\x0e\\x0f\\x10\\x11\\x12\\x10\\x13\\x0f\\r\\x14\\x0e\\x01\\x15\\x0f\\x01\\x06\\x01\\n\\x01\\x02\\x03\\x03\\x04\\x05\\x05\\x06\\n\\x03\\x01\\x16\\r\\x0f\\x15\\x0e\\x01\\x17\\x18\\x01\\x19\\x1a\\x04\\x03\\x1b\\x01\\x1c\\x1a\\x06\\x1b\\x01\\x1d\\x1a\\x03\\n\\x03\\x01\\x16\\r\\x0f\\x15\\x0e\\x01\\x17\\x18\\x01\\x19\\x1a\\x08\\x02\\x1b\\x01\\x1c\\x1a\\x06\\x1b\\x01\\x1d\\x1a\\x03\\n\\x03\\x01\\x16\\r\\x0f\\x15\\x0e\\x01\\x17\\x18\\x01\\x19\\x1a\\x06\\x04\\x1b\\x01\\x1c\\x1a\\x06\\x1b\\x01\\x1d\\x1a\\x03\\n\\x01\\x01\\n\\x01\\x02\\n\\x01\\x03\\n\\x01\\x04\\x05\\n\\x01\\x04\\x06\\n\\x01\\x05\\x01\\n\\x01\\x05\\x02\\n\\x01\\x01\\x07\\x04\\n\\x01\\x01\\x07\\x05\\n\\x01\\x01\\x07\\x08\\n\\x01\\x01\\x07\\x02\\n\\x01\\x01\\x07\\t\\n\\x01\\x01\\x07\\x06\\n\\x01\\x01\\x07\\n\\x01\\x01\\x07\\x03\\n\\x01\\x01\\x07\\x0b\\n\\x01\\x02\\x03\\x03\\x01\\x04\\x05\\x06\\x07\\x08\\x01\\t\\n\\x0b\\x0c\\r\\x01\\x0e\\x0b\\x0f\\x10\\x01\\x11\\x08\\x12\\n\\x05\\x13\\x14\\n\\x04\\x01\\x0c\\r\\x0e\\x0f\\x10\\x11\\x12\\x10\\x13\\x0f\\r\\x14\\x0e\\x01\\x15\\x0f\\x01\\x04\\x01\\n\\x01\\x02\\x02\\x03\\x04\\x05\\n\\x02\\x01\\x16\\r\\x0f\\x15\\x0e\\x01\\x17\\x18\\x01\\x19\\x1a\\x02\\x01\\x1b\\x01\\x1c\\x1a\\x02\\x1b\\x01\\x1d\\x1a\\x04\\n\\x02\\x01\\x16\\r\\x0f\\x15\\x0e\\x01\\x17\\x18\\x01\\x19\\x1a\\x05\\x01\\x1b\\x01\\x1c\\x1a\\x05\\x1b\\x01\\x1d\\x1a\\x05\\n\\x03\\x01\\x1e\\x02\\x01\\x18\\x01\\x19\\x1a\\x02\\x01\\x1b\\x01\\x1c\\x1a\\x02\\x1b\\x01\\x1d\\x1a\\x05\\n\\x03\\x01\\x1e\\x02\\x01\\x18\\x01\\x19\\x1a\\x05\\x01\\x1b\\x01\\x1c\\x1a\\x05\\x1b\\x01\\x1d\\x1a\\x02\\nFigure 5:\\nSpeed/accuracy trade-oﬀof brute-force\\n10-NN graph construction for the YFCC100M and\\nDEEP1B datasets.\\ndiﬀerent, it shows that making searches on GPUs is a game-\\nchanger in terms of speed achievable on a single machine.\\n6.5\\nThe k-NN graph\\nAn example usage of our similarity search method is to\\nconstruct a k-nearest neighbor graph of a dataset via brute\\nforce (all vectors queried against the entire index).\\nExperimental setup. We evaluate the trade-oﬀbetween\\nspeed, precision and memory on two datasets: 95 million\\nimages from the Yfcc100M dataset [42] and Deep1B. For\\nYfcc100M, we compute CNN descriptors as the one-before-\\nlast layer of a ResNet [23], reduced to d = 128 with PCA.\\nThe evaluation measures the trade-oﬀbetween:\\n• Speed: How much time it takes to build the IVFADC\\nindex from scratch and construct the whole k-NN graph\\n(k = 10) by searching nearest neighbors for all vectors\\nin the dataset. Thus, this is an end-to-end test that\\nincludes indexing as well as search time;\\n• Quality: We sample 10,000 images for which we com-\\npute the exact nearest neighbors. Our accuracy mea-\\nsure is the fraction of 10 found nearest neighbors that\\nare within the ground-truth 10 nearest neighbors.\\nFor Yfcc100M, we use a coarse quantizer (216 centroids),\\nand consider m = 16, 32 and 64 byte PQ encodings for each\\nvector. For Deep1B, we pre-process the vectors to d = 120\\nvia OPQ, use |C1| = 218 and consider m = 20, 40. For a\\ngiven encoding, we vary τ from 1 to 256, to obtain trade-\\noﬀs between eﬃciency and quality, as seen in Figure 5.\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='Figure 6:\\nPath in the k-NN graph of 95 million images from YFCC100M. The ﬁrst and the last image are\\ngiven; the algorithm computes the smoothest path between them.\\nDiscussion. For Yfcc100M we used S = 1, R = 4. An\\naccuracy of more than 0.8 is obtained in 35 minutes. For\\nDeep1B, a lower-quality graph can be built in 6 hours,\\nwith higher quality in about half a day.\\nWe also experi-\\nmented with more GPUs by doubling the replica set, us-\\ning 8 Maxwell M40s (the M40 is roughly equivalent in per-\\nformance to the Titan X). Performance is improved sub-\\nlinearly (∼1.6× for m = 20, ∼1.7× for m = 40).\\nFor comparison, the largest k-NN graph construction we\\nare aware of used a dataset comprising 36.5 million 384-\\nd vectors, which took a cluster of 128 CPU servers 108.7\\nhours of compute [45], using NN-Descent [15]. Note that\\nNN-Descent could also build or reﬁne the k-NN graph for\\nthe datasets we consider, but it has a large memory over-\\nhead over the graph storage, which is already 80 GB for\\nDeep1B. Moreover it requires random access across all vec-\\ntors (384 GB for Deep1B).\\nThe largest GPU k-NN graph construction we found is a\\nbrute-force construction using exact search with GEMM, of\\na dataset of 20 million 15,000-d vectors, which took a cluster\\nof 32 Tesla C2050 GPUs 10 days [14]. Assuming computa-\\ntion scales with GEMM cost for the distance matrix, this\\napproach for Deep1B would take an impractical 200 days\\nof computation time on their cluster.\\n6.6\\nUsing the k-NN graph\\nWhen a k-NN graph has been constructed for an image\\ndataset, we can ﬁnd paths in the graph between any two\\nimages, provided there is a single connected component (this\\nis the case). For example, we can search the shortest path\\nbetween two images of ﬂowers, by propagating neighbors\\nfrom a starting image to a destination image. Denoting by\\nS and D the source and destination images, and dij the\\ndistance between nodes, we search the path P = {p1, ..., pn}\\nwith p1 = S and pn = D such that\\nmin\\nP\\nmax\\ni=1..n dpipi+1,\\n(12)\\ni.e., we want to favor smooth transitions. An example re-\\nsult is shown in Figure 6 from Yfcc100M4. It was ob-\\ntained after 20 seconds of propagation in a k-NN graph with\\nk = 15 neighbors. Since there are many ﬂower images in the\\ndataset, the transitions are smooth.\\n4The mapping from vectors to images is not available for\\nDeep1B\\n7.\\nCONCLUSION\\nThe arithmetic throughput and memory bandwidth of\\nGPUs are well into the teraﬂops and hundreds of gigabytes\\nper second.\\nHowever, implementing algorithms that ap-\\nproach these performance levels is complex and counter-\\nintuitive. In this paper, we presented the algorithmic struc-\\nture of similarity search methods that achieves near-optimal\\nperformance on GPUs.\\nThis work enables applications that needed complex ap-\\nproximate algorithms before. For example, the approaches\\npresented here make it possible to do exact k-means cluster-\\ning or to compute the k-NN graph with simple brute-force\\napproaches in less time than a CPU (or a cluster of them)\\nwould take to do this approximately.\\nGPU hardware is now very common on scientiﬁc work-\\nstations, due to their popularity for machine learning algo-\\nrithms. We believe that our work further demonstrates their\\ninterest for database applications. Along with this work, we\\nare publishing a carefully engineered implementation of this\\npaper’s algorithms, so that these GPUs can now also be used\\nfor eﬃcient similarity search.\\n8.\\nREFERENCES\\n[1] T. Alabi, J. D. Blanchard, B. Gordon, and R. Steinbach.\\nFast k-selection algorithms for graphics processing units.\\nACM Journal of Experimental Algorithmics,\\n17:4.2:4.1–4.2:4.29, October 2012.\\n[2] F. Andr´e, A.-M. Kermarrec, and N. L. Scouarnec. Cache\\nlocality is not enough: High-performance nearest neighbor\\nsearch with product quantization fast scan. In Proc.\\nInternational Conference on Very Large DataBases, pages\\n288–299, 2015.\\n[3] Y. Avrithis, Y. Kalantidis, E. Anagnostopoulos, and I. Z.\\nEmiris. Web-scale image clustering revisited. In Proc.\\nInternational Conference on Computer Vision, pages\\n1502–1510, 2015.\\n[4] A. Babenko and V. Lempitsky. The inverted multi-index.\\nIn Proc. IEEE Conference on Computer Vision and\\nPattern Recognition, pages 3069–3076, June 2012.\\n[5] A. Babenko and V. Lempitsky. Improving bilayer product\\nquantization for billion-scale approximate nearest neighbors\\nin high dimensions. arXiv preprint arXiv:1404.1831, 2014.\\n[6] A. Babenko and V. Lempitsky. Eﬃcient indexing of\\nbillion-scale datasets of deep descriptors. In Proc. IEEE\\nConference on Computer Vision and Pattern Recognition,\\npages 2055–2063, June 2016.\\n[7] R. Barrientos, J. G´omez, C. Tenllado, M. Prieto, and\\nM. Marin. knn query processing in metric spaces using\\nGPUs. In International European Conference on Parallel\\nand Distributed Computing, volume 6852 of Lecture Notes\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='in Computer Science, pages 380–392, Bordeaux, France,\\nSeptember 2011. Springer.\\n[8] K. E. Batcher. Sorting networks and their applications. In\\nProc. Spring Joint Computer Conference, AFIPS ’68\\n(Spring), pages 307–314, New York, NY, USA, 1968. ACM.\\n[9] P. Boncz, W. Lehner, and T. Neumann. Special issue:\\nModern hardware. The VLDB Journal, 25(5):623–624,\\n2016.\\n[10] J. Canny, D. L. W. Hall, and D. Klein. A multi-teraﬂop\\nconstituency parser using GPUs. In Proc. Empirical\\nMethods on Natural Language Processing, pages 1898–1907.\\nACL, 2013.\\n[11] J. Canny and H. Zhao. Bidmach: Large-scale learning with\\nzero memory allocation. In BigLearn workshop, NIPS,\\n2013.\\n[12] B. Catanzaro, A. Keller, and M. Garland. A decomposition\\nfor in-place matrix transposition. In Proc. ACM\\nSymposium on Principles and Practice of Parallel\\nProgramming, PPoPP ’14, pages 193–206, 2014.\\n[13] J. Chhugani, A. D. Nguyen, V. W. Lee, W. Macy,\\nM. Hagog, Y.-K. Chen, A. Baransi, S. Kumar, and\\nP. Dubey. Eﬃcient implementation of sorting on multi-core\\nsimd cpu architecture. Proc. VLDB Endow.,\\n1(2):1313–1324, August 2008.\\n[14] A. Dashti. Eﬃcient computation of k-nearest neighbor\\ngraphs for large high-dimensional data sets on gpu clusters.\\nMaster’s thesis, University of Wisconsin Milwaukee, August\\n2013.\\n[15] W. Dong, M. Charikar, and K. Li. Eﬃcient k-nearest\\nneighbor graph construction for generic similarity measures.\\nIn WWW: Proceeding of the International Conference on\\nWorld Wide Web, pages 577–586, March 2011.\\n[16] M. Douze, H. J´egou, and F. Perronnin. Polysemous codes.\\nIn Proc. European Conference on Computer Vision, pages\\n785–801. Springer, October 2016.\\n[17] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product\\nquantization. IEEE Trans. PAMI, 36(4):744–755, 2014.\\n[18] Y. Gong and S. Lazebnik. Iterative quantization: A\\nprocrustean approach to learning binary codes. In Proc.\\nIEEE Conference on Computer Vision and Pattern\\nRecognition, pages 817–824, June 2011.\\n[19] Y. Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale\\norderless pooling of deep convolutional activation features.\\nIn Proc. European Conference on Computer Vision, pages\\n392–407, 2014.\\n[20] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep\\nimage retrieval: Learning global representations for image\\nsearch. In Proc. European Conference on Computer Vision,\\npages 241–257, 2016.\\n[21] S. Han, H. Mao, and W. J. Dally. Deep compression:\\nCompressing deep neural networks with pruning, trained\\nquantization and huﬀman coding. arXiv preprint\\narXiv:1510.00149, 2015.\\n[22] K. He, F. Wen, and J. Sun. K-means hashing: An\\naﬃnity-preserving quantization method for learning binary\\ncompact codes. In Proc. IEEE Conference on Computer\\nVision and Pattern Recognition, pages 2938–2945, June\\n2013.\\n[23] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual\\nlearning for image recognition. In Proc. IEEE Conference\\non Computer Vision and Pattern Recognition, pages\\n770–778, June 2016.\\n[24] X. He, D. Agarwal, and S. K. Prasad. Design and\\nimplementation of a parallel priority queue on many-core\\narchitectures. IEEE International Conference on High\\nPerformance Computing, pages 1–10, 2012.\\n[25] H. J´egou, M. Douze, and C. Schmid. Product quantization\\nfor nearest neighbor search. IEEE Trans. PAMI,\\n33(1):117–128, January 2011.\\n[26] H. J´egou, R. Tavenard, M. Douze, and L. Amsaleg.\\nSearching in one billion vectors: re-rank with source\\ncoding. In International Conference on Acoustics, Speech,\\nand Signal Processing, pages 861–864, May 2011.\\n[27] Y. Kalantidis and Y. Avrithis. Locally optimized product\\nquantization for approximate nearest neighbor search. In\\nProc. IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 2329–2336, June 2014.\\n[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\\nclassiﬁcation with deep convolutional neural networks. In\\nAdvances in Neural Information Processing Systems, pages\\n1097–1105, 2012.\\n[29] F. T. Leighton. Introduction to Parallel Algorithms and\\nArchitectures: Array, Trees, Hypercubes. Morgan\\nKaufmann Publishers Inc., San Francisco, CA, USA, 1992.\\n[30] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym.\\nNVIDIA Tesla: a uniﬁed graphics and computing\\narchitecture. IEEE Micro, 28(2):39–55, March 2008.\\n[31] W. Liu and B. Vinter. Ad-heap: An eﬃcient heap data\\nstructure for asymmetric multicore processors. In Proc. of\\nWorkshop on General Purpose Processing Using GPUs,\\npages 54:54–54:63. ACM, 2014.\\n[32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\\nJ. Dean. Distributed representations of words and phrases\\nand their compositionality. In Advances in Neural\\nInformation Processing Systems, pages 3111–3119, 2013.\\n[33] L. Monroe, J. Wendelberger, and S. Michalak. Randomized\\nselection on the GPU. In Proc. ACM Symposium on High\\nPerformance Graphics, pages 89–98, 2011.\\n[34] M. Norouzi and D. Fleet. Cartesian k-means. In Proc.\\nIEEE Conference on Computer Vision and Pattern\\nRecognition, pages 3017–3024, June 2013.\\n[35] M. Norouzi, A. Punjani, and D. J. Fleet. Fast search in\\nHamming space with multi-index hashing. In Proc. IEEE\\nConference on Computer Vision and Pattern Recognition,\\npages 3108–3115, 2012.\\n[36] J. Pan and D. Manocha. Fast GPU-based locality sensitive\\nhashing for k-nearest neighbor computation. In Proc. ACM\\nInternational Conference on Advances in Geographic\\nInformation Systems, pages 211–220, 2011.\\n[37] L. Paulev´e, H. J´egou, and L. Amsaleg. Locality sensitive\\nhashing: a comparison of hash function types and querying\\nmechanisms. Pattern recognition letters, 31(11):1348–1358,\\nAugust 2010.\\n[38] O. Shamir. Fundamental limits of online and distributed\\nalgorithms for statistical learning and estimation. In\\nAdvances in Neural Information Processing Systems, pages\\n163–171, 2014.\\n[39] A. Sharif Razavian, H. Azizpour, J. Sullivan, and\\nS. Carlsson. CNN features oﬀ-the-shelf: an astounding\\nbaseline for recognition. In CVPR workshops, pages\\n512–519, 2014.\\n[40] N. Sismanis, N. Pitsianis, and X. Sun. Parallel search of\\nk-nearest neighbors with synchronous operations. In IEEE\\nHigh Performance Extreme Computing Conference, pages\\n1–6, 2012.\\n[41] X. Tang, Z. Huang, D. M. Eyers, S. Mills, and M. Guo.\\nEﬃcient selection algorithm for fast k-nn search on GPUs.\\nIn IEEE International Parallel & Distributed Processing\\nSymposium, pages 397–406, 2015.\\n[42] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde,\\nK. Ni, D. Poland, D. Borth, and L.-J. Li. YFCC100M: The\\nnew data in multimedia research. Communications of the\\nACM, 59(2):64–73, January 2016.\\n[43] V. Volkov and J. W. Demmel. Benchmarking GPUs to tune\\ndense linear algebra. In Proc. ACM/IEEE Conference on\\nSupercomputing, pages 31:1–31:11, 2008.\\n[44] A. Wakatani and A. Murakami. GPGPU implementation of\\nnearest neighbor search with product quantization. In\\nIEEE International Symposium on Parallel and Distributed\\nProcessing with Applications, pages 248–253, 2014.\\n[45] T. Warashina, K. Aoyama, H. Sawada, and T. Hattori.\\nEﬃcient k-nearest neighbor graph construction using\\nmapreduce for large-scale data sets. IEICE Transactions,\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='97-D(12):3142–3154, 2014.\\n[46] R. Weber, H.-J. Schek, and S. Blott. A quantitative\\nanalysis and performance study for similarity-search\\nmethods in high-dimensional spaces. In Proc. International\\nConference on Very Large DataBases, pages 194–205, 1998.\\n[47] P. Wieschollek, O. Wang, A. Sorkine-Hornung, and\\nH. P. A. Lensch. Eﬃcient large-scale approximate nearest\\nneighbor search on the GPU. In Proc. IEEE Conference on\\nComputer Vision and Pattern Recognition, pages\\n2027–2035, June 2016.\\n[48] S. Williams, A. Waterman, and D. Patterson. Rooﬂine: An\\ninsightful visual performance model for multicore\\narchitectures. Communications of the ACM, 52(4):65–76,\\nApril 2009.\\nAppendix: Complexity analysis of WarpSelect\\nWe derive the average number of times updates are triggered\\nin WarpSelect, for use in Section 4.3.\\nLet the input to k-selection be a sequence {a1, a2, ..., aℓ}\\n(1-based indexing), a randomly chosen permutation of a set\\nof distinct elements.\\nElements are read sequentially in c\\ngroups of size w (the warp; in our case, w = 32); assume ℓ\\nis a multiple of w, so c = ℓ/w. Recall that t is the thread\\nqueue length.\\nWe call elements prior to or at position n\\nin the min-k seen so far the successive min-k (at n). The\\nlikelihood that an is in the successive min-k at n is:\\nα(n, k) :=\\n(\\n1\\nif n ≤k\\nk/n\\nif n > k\\n(13)\\nas each an, n > k has a k/n chance as all permutations are\\nequally likely, and all elements in the ﬁrst k qualify.\\nCounting the insertion sorts. In a given lane, an inser-\\ntion sort is triggered if the incoming value is in the successive\\nmin-k + t values, but the lane has “seen” only wc0 + (c−c0)\\nvalues, where c0 is the previous won warp ballot. The prob-\\nability of this happening is:\\nα(wc0 + (c −c0), k + t) ≈k + t\\nwc\\nfor c > k.\\n(14)\\nThe approximation considers that the thread queue has seen\\nall the wc values, not just those assigned to its lane. The\\nprobability of any lane triggering an insertion sort is then:\\n1 −\\n\\x12\\n1 −k + t\\nwc\\n\\x13w\\n≈k + t\\nc\\n.\\n(15)\\nHere the approximation is a ﬁrst-order Taylor expansion.\\nSumming up the probabilities over c gives an expected num-\\nber of insertions of N2 ≈(k + t) log(c) = O(k log(ℓ/w)).\\nCounting full sorts. We seek N3 = π(ℓ, k, t, w), the ex-\\npected number of full sorts required for WarpSelect.\\nSingle lane. For now, we assume w = 1, so c = ℓ. Let\\nγ(ℓ, m, k) be the probability that in an sequence {a1, ..., aℓ},\\nexactly m of the elements as encountered by a sequential\\nscanner (w = 1) are in the successive min-k. Given m, there\\nare\\n\\x00 ℓ\\nm\\n\\x01\\nplaces where these successive min-k elements can\\noccur. It is given by a recurrence relation:\\nγ(ℓ, m, k) :=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n1\\nℓ= 0 and m = 0\\n0\\nℓ= 0 and m > 0\\n0\\nℓ> 0 and m = 0\\n(γ(ℓ−1, m −1, k) · α(ℓ, k)+\\nγ(ℓ−1, m, k) · (1 −α(ℓ, k)))\\notherwise.\\n(16)\\nThe last case is the probability of: there is a ℓ−1 se-\\nquence with m −1 successive min-k elements preceding us,\\nand the current element is in the successive min-k, or the\\ncurrent element is not in the successive min-k, m ones are\\nbefore us. We can then develop a recurrence relationship for\\nπ(ℓ, k, t, 1). Note that\\nδ(ℓ, b, k, t) :=\\nmin((bt+max(0,t−1)),ℓ)\\nX\\nm=bt\\nγ(ℓ, m, k)\\n(17)\\nfor b where 0 ≤bt ≤ℓis the fraction of all sequences of\\nlength ℓthat will force b sorts of data by winning the thread\\nqueue ballot, as there have to be bt to (bt + max(0, t −1))\\nelements in the successive min-k for these sorts to happen (as\\nthe min-k elements will overﬂow the thread queues). There\\nare at most ⌊ℓ/t⌋won ballots that can occur, as it takes t\\nseparate sequential current min-k seen elements to win the\\nballot.\\nπ(ℓ, k, t, 1) is thus the expectation of this over all\\npossible b:\\nπ(ℓ, k, t, 1) =\\n⌊ℓ/t⌋\\nX\\nb=1\\nb · δ(ℓ, b, k, t).\\n(18)\\nThis can be computed by dynamic programming. Analyti-\\ncally, note that for t = 1, k = 1, π(ℓ, 1, 1, 1) is the harmonic\\nnumber Hℓ= 1+ 1\\n2 + 1\\n3 +...+ 1\\nℓ, which converges to ln(ℓ)+γ\\n(the Euler-Mascheroni constant γ) as ℓ→∞.\\nFor t = 1, k > 1, ℓ> k, π(ℓ, k, 1, 1) = k + k(Hℓ−Hk)\\nor O(k log(ℓ)), as the ﬁrst k elements are in the successive\\nmin-k, and the expectation for the rest is\\nk\\nk+1 +\\nk\\nk+2 +...+ k\\nℓ.\\nFor t > 1, k > 1, ℓ> k, note that there are some number\\nD, k ≤D ≤ℓof successive min-k determinations D made\\nfor each possible {a1, ..., aℓ}. The number of won ballots for\\neach case is by deﬁnition ⌊D/t⌋, as the thread queue must\\nﬁll up t times. Thus, π(ℓ, k, t, 1) = O(k log(ℓ)/t).\\nMultiple lanes.\\nThe w > 1 case is complicated by the\\nfact that there are joint probabilities to consider (if more\\nthan one of the w workers triggers a sort for a given group,\\nonly one sort takes place). However, the likelihood can be\\nbounded. Let π′(ℓ, k, t, w) be the expected won ballots as-\\nsuming no mutual interference between the w workers for\\nwinning ballots (i.e., we win b ballots if there are b ≤w\\nworkers that independently win a ballot at a single step),\\nbut with the shared min-k set after each sort from the joint\\nsequence. Assume that k ≥w. Then:\\nπ′(ℓ, k, 1, w) ≤w\\n \\x18 k\\nw\\n\\x19\\n+\\n⌈ℓ/w⌉−⌈k/w⌉\\nX\\ni=1\\nk\\nw(⌈k/w⌉+ i)\\n!\\n≤wπ(⌈ℓ/w⌉, k, 1, 1) = O(wk log(ℓ/w))\\n(19)\\nwhere the likelihood of the w workers seeing a successive\\nmin-k element has an upper bound of that of the ﬁrst worker\\nat each step. As before, the number of won ballots is scaled\\nby t, so π′(ℓ, k, t, w) = O(wk log(ℓ/w)/t). Mutual interfer-\\nence can only reduce the number of ballots, so we obtain the\\nsame upper bound for π(ℓ, k, t, w).\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 0}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research; ‡University College London; ⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric\\nmemory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\\n(RAG) — models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\\npare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\\narchitectures. For language generation tasks, we ﬁnd that RAG models generate\\nmore speciﬁc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1\\nIntroduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\\ntheir predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that\\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\\narXiv:2005.11401v4  [cs.CL]  12 Apr 2021'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 1}, page_content='The Divine\\nComedy (x)\\nq\\nQuery\\nEncoder\\nq(x)\\nMIPS\\npθ\\nGenerator\\xa0pθ\\n(Parametric)\\nMargin-\\nalize\\nThis 14th century work\\nis divided into 3\\nsections: \"Inferno\",\\n\"Purgatorio\" &\\n\"Paradiso\"         (y)\\nEnd-to-End Backprop through q and\\xa0pθ\\nBarack Obama was\\nborn in Hawaii.(x)\\nFact Veriﬁcation: Fact Query\\nsupports (y)\\nQuestion Generation\\nFact Veriﬁcation:\\nLabel Generation\\nDocument\\nIndex\\nDefine \"middle ear\"(x)\\nQuestion Answering:\\nQuestion Query\\nThe middle ear includes\\nthe tympanic cavity and\\nthe three ossicles.  (y)\\nQuestion Answering:\\nAnswer Generation\\nRetriever pη\\n(Non-Parametric)\\nz4\\nz3\\nz2\\nz1\\nd(z)\\nJeopardy Question\\nGeneration:\\nAnswer Query\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document\\nIndex) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use\\nMaximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we\\ntreat z as a latent variable and marginalize over seq2seq predictions given different documents.\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\\nand non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\\na general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).\\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with\\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\\ncan be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\\nmemory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack-\\naugmented networks [25] and memory layers [30]. In contrast, we explore a setting where both\\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\\npresent without additional training.\\nOur results highlight the beneﬁts of combining parametric and non-parametric memory with genera-\\ntion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform\\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\\nextractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\\ngeneration, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and\\ndiverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of\\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\\nthe non-parametric memory can be replaced to update the models’ knowledge as the world changes.1\\n2\\nMethods\\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them\\nas additional context when generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated)\\ndistributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 2}, page_content='by θ that generates a current token based on a context of the previous i −1 tokens y1:i−1, the original\\ninput x and a retrieved passage z.\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\\nWe propose two models that marginalize over the latent documents in different ways to produce a\\ndistribution over generated text. In one approach, RAG-Sequence, the model uses the same document\\nto predict each target token. The second approach, RAG-Token, can predict each target token based\\non a different document. In the following, we formally introduce both models and then describe the\\npη and pθ components, as well as the training and decoding procedure.\\n2.1\\nModels\\nRAG-Sequence Model\\nThe RAG-Sequence model uses the same retrieved document to generate\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable that\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the\\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence(y|x) ≈\\nX\\nz∈top-k(p(·|x))\\npη(z|x)pθ(y|x, z) =\\nX\\nz∈top-k(p(·|x))\\npη(z|x)\\nN\\nY\\ni\\npθ(yi|x, z, y1:i−1)\\nRAG-Token Model\\nIn the RAG-Token model we can draw a different latent document for each\\ntarget token and marginalize accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output token for each document,\\nbefore marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\\npRAG-Token(y|x) ≈\\nN\\nY\\ni\\nX\\nz∈top-k(p(·|x))\\npη(z|x)pθ(yi|x, z, y1:i−1)\\nFinally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class\\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2\\nRetriever: DPR\\nThe retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\npη(z|x) ∝exp\\n\\x00d(z)⊤q(x)\\n\\x01\\nd(z) = BERTd(z), q(x) = BERTq(x)\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],\\nand q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating\\ntop-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner\\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use\\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\\nretriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and\\nNatural Questions [29]. We refer to the document index as the non-parametric memory.\\n2.3\\nGenerator: BART\\nThe generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use\\nBART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input\\nx with the retrieved content z when generating from BART, we simply concatenate them. BART was\\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\\nmodels [32]. We refer to the BART generator parameters θ as the parametric memory henceforth.\\n2.4\\nTraining\\nWe jointly train the retriever and generator components without any direct supervision on what\\ndocument should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj, yj), we\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 3}, page_content='minimize the negative marginal log-likelihood of each target, P\\nj −log p(yj|xj) using stochastic\\ngradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as\\nit requires the document index to be periodically updated as REALM does during pre-training [20].\\nWe do not ﬁnd this step necessary for strong performance, and keep the document encoder (and\\nindex) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.\\n2.5\\nDecoding\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x).\\nRAG-Token\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with transition probability: p′\\nθ(yi|x, y1:i−1) = P\\nz∈top-k(p(·|x)) pη(zi|x)pθ(yi|x, zi, y1:i−1) To\\ndecode, we can plug p′\\nθ(yi|x, y1:i−1) into a standard beam decoder.\\nRAG-Sequence\\nFor RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-\\ntoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\neach document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses\\nY , some of which may not have appeared in the beams of all documents. To estimate the probability\\nof an hypothesis y we run an additional forward pass for each document z for which y does not\\nappear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across\\nbeams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer\\noutput sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding,\\nwe can make a further approximation that pθ(y|x, zi) ≈0 where y was not generated during beam\\nsearch from x, zi. This avoids the need to run additional forward passes once the candidate set Y has\\nbeen generated. We refer to this decoding procedure as “Fast Decoding.”\\n3\\nExperiments\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\\nembedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical\\nNavigable Small World approximation for fast retrieval [37]. During training, we retrieve the top\\nk documents for each query. We consider k ∈{5, 10} for training and set k for test time using dev\\ndata. We now discuss experimental details for each task.\\n3.1\\nOpen-domain Question Answering\\nOpen-domain question answering (QA) is an important real-world application and common testbed\\nfor knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y)\\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\\ndocuments, relying primarily on non-parametric knowledge. We also compare to “Closed-Book\\nQA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\\nQuestions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As\\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\\nmodel. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM)\\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n3.2\\nAbstractive Question Answering\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\\ntext generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,\\nwe use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 4}, page_content='MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\\nanswered in a way that matches the reference answer without access to the gold passages, such as\\n“What is the weather in Volcano, CA?” so performance will be lower without using gold passages.\\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\\nRAG can rely on parametric knowledge to generate reasonable responses.\\n3.3\\nJeopardy Question Generation\\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-\\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\\nFor example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst\\ncountry to host this international sports competition twice.” As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As\\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\\nSQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\\none for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external\\nsources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow\\nbest practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two\\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\\noptions—quuestion A is better, question B is better, both are good, or neither is good.\\n3.4\\nFact Veriﬁcation\\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by\\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\\nwhether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem\\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\\nexploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER\\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with\\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\\nretrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and\\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\\ntwo variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n4\\nResults\\n4.1\\nOpen-domain Question Answering\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\\nthe generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of\\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\\nwithout expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s\\nretriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-\\nencoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\\na correct answer being generated, which is not possible with standard extractive approaches, leading\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 5}, page_content='Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See Appendix D for further details.\\nModel\\nNQ\\nTQA\\nWQ\\nCT\\nClosed\\nBook\\nT5-11B [52]\\n34.5\\n- /50.1\\n37.4\\n-\\nT5-11B+SSM[52]\\n36.6\\n- /60.5\\n44.7\\n-\\nOpen\\nBook\\nREALM [20]\\n40.4\\n- / -\\n40.7\\n46.8\\nDPR [26]\\n41.5\\n57.9/ -\\n41.1\\n50.6\\nRAG-Token\\n44.1\\n55.2/66.1\\n45.5\\n50.0\\nRAG-Seq.\\n44.5\\n56.8/68.0\\n45.2\\n52.2\\nTable 2: Generation and classiﬁcation Test Scores.\\nMS-MARCO SotA is [4], FEVER-3 is [68] and\\nFEVER-2 is [57] *Uses gold context/evidence.\\nBest model without gold access underlined.\\nModel\\nJeopardy\\nMSMARCO FVR3 FVR2\\nB-1 QB-1\\nR-L\\nB-1\\nLabel Acc.\\nSotA\\n-\\n-\\n49.8* 49.9*\\n76.8\\n92.2*\\nBART\\n15.1\\n19.7\\n38.2\\n41.6\\n64.0\\n81.1\\nRAG-Tok. 17.3\\n22.2\\n40.1\\n41.5\\n72.5\\n89.5\\nRAG-Seq. 14.7\\n21.4\\n40.8\\n44.2\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\\ncases for NQ, where an extractive model would score 0%.\\n4.2\\nAbstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciﬁc information required to\\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see §4.5).\\n4.3\\nJeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\\nBART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\\nthe task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more\\nspeciﬁc by a large margin. Table 3 shows typical generations from each model.\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\\nbest because it can generate responses that combine content from several documents. Figure 2 shows\\nan example. When generating “Sun”, the posterior is high for document 2 which mentions “The\\nSun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is\\ngenerated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\\nThis observation suggests that the generator can complete the titles without depending on speciﬁc\\ndocuments. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\\nﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\\nSun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun\\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\\nBART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A\\nwith \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows\\nhow parametric and non-parametric memories work together—the non-parametric component helps\\nto guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\\n4.4\\nFact Veriﬁcation\\nTable 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of\\nstate-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and\\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}, page_content='Document 1: his works are considered classics of American\\nliterature ... His wartime experiences formed the basis for his novel\\n”A Farewell to Arms” (1929) ...\\nDocument 2: ... artists of the 1920s ”Lost Generation” expatriate\\ncommunity. His debut novel, ”The Sun Also Rises”, was published\\nin 1926.\\nBOS ”\\nThe\\nSun\\nAlso\\nR\\nises\\n”\\nis\\na\\nnovel\\nby\\nthis\\nauthorof\\n”\\nA\\nFare\\nwell\\nto\\nArms”\\nDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nDoc 5\\nFigure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hem-\\ningway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\\nwhen generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\\nTable 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\\nresponses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\\nTask\\nInput\\nModel\\nGeneration\\nMS-\\nMARCO\\ndeﬁne middle\\near\\nBART\\n?The middle ear is the part of the ear between the middle ear and the nose.\\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\\nwhat currency\\nneeded in\\nscotland\\nBART\\nThe currency needed in Scotland is Pound sterling.\\nRAG-T Pound is the currency needed in Scotland.\\nRAG-S The currency needed in Scotland is the pound sterling.\\nJeopardy\\nQuestion\\nGener\\n-ation\\nWashington\\nBART\\n?This state has the largest number of counties in the U.S.\\nRAG-T It’s the only U.S. state named for a U.S. president\\nRAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park\\nThe Divine\\nComedy\\nBART\\n*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\\nFor 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved\\nby RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n4.5\\nAdditional Results\\nGeneration Diversity\\nSection 4.3 shows that RAG models are more factual and speciﬁc than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\\n[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\\ntotal ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\\nmore diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\\nany diversity-promoting decoding.\\nRetrieval Ablations\\nA key feature of RAG is learning to retrieve relevant information for the task.\\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\\nWe compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace\\nRAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating\\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\nIndex hot-swapping\\nAn advantage of non-parametric memory models like RAG is that knowledge\\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 7}, page_content='Table 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality\\nSpeciﬁcity\\nBART better\\n7.1%\\n16.8%\\nRAG better\\n42.7%\\n37.4%\\nBoth good\\n11.7%\\n11.8%\\nBoth poor\\n17.7%\\n6.9%\\nNo majority\\n20.8%\\n20.1%\\nTable 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO\\nJeopardy QGen\\nGold\\n89.6%\\n90.0%\\nBART\\n70.7%\\n32.4%\\nRAG-Token\\n77.8%\\n46.8%\\nRAG-Seq.\\n83.5%\\n53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent.\\nModel\\nNQ\\nTQA\\nWQ\\nCT\\nJeopardy-QGen\\nMSMarco\\nFVR-3\\nFVR-2\\nExact Match\\nB-1\\nQB-1\\nR-L\\nB-1\\nLabel Accuracy\\nRAG-Token-BM25\\n29.7\\n41.5\\n32.1\\n33.1\\n17.5\\n22.3\\n55.5\\n48.4\\n75.1\\n91.6\\nRAG-Sequence-BM25\\n31.8\\n44.1\\n36.6\\n33.8\\n11.1\\n19.5\\n56.5\\n46.9\\nRAG-Token-Frozen\\n37.8\\n50.1\\n37.1\\n51.1\\n16.7\\n21.7\\n55.9\\n49.4\\n72.9\\n89.4\\nRAG-Sequence-Frozen\\n41.2\\n52.1\\n41.8\\n52.6\\n11.8\\n19.6\\n56.7\\n47.3\\nRAG-Token\\n43.5\\n54.8\\n46.5\\n51.9\\n17.9\\n22.6\\n56.2\\n49.4\\n74.5\\n90.6\\nRAG-Sequence\\n44.0\\n55.8\\n44.9\\n53.4\\n15.3\\n21.5\\n57.2\\n47.5\\nbetween these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)\\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\\nThis shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.\\nEffect of Retrieving more documents\\nModels are trained with either 5 or 10 retrieved latent\\ndocuments, and we do not observe signiﬁcant differences in performance between them. We have the\\nﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n10\\n20\\n30\\n40\\n50\\nK Retrieved Docs\\n39\\n40\\n41\\n42\\n43\\n44\\nNQ Exact Match\\nRAG-Tok\\nRAG-Seq\\n10\\n20\\n30\\n40\\n50\\nK Retrieved Docs\\n40\\n50\\n60\\n70\\n80\\nNQ Answer Recall @ K\\nRAG-Tok\\nRAG-Seq\\nFixed DPR\\nBM25\\n10\\n20\\n30\\n40\\n50\\nK Retrieved Docs\\n48\\n50\\n52\\n54\\n56\\nBleu-1 / Rouge-L score\\nRAG-Tok R-L\\nRAG-Tok B-1\\nRAG-Seq R-L\\nRAG-Seq B-1\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n5\\nRelated Work\\nSingle-Task Retrieval\\nPrior work has shown that retrieval improves performance across a variety of\\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],\\nfact checking [56], fact completion [48], long-form question answering [12], Wikipedia article\\ngeneration [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our\\nwork uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 8}, page_content='General-Purpose Architectures for NLP\\nPrior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\nhas been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-\\nmarks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained\\nlanguage model could achieve strong performance across both discriminative and generative tasks.\\nFor further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder\\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\\nand generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed\\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\\nLearned Retrieval\\nThere is signiﬁcant work on learning to retrieve documents in information\\nretrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some\\nwork optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,\\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\\ncan be ﬁne-tuned for strong performance on a variety of tasks.\\nMemory-based Architectures\\nOur document index can be seen as a large external memory for\\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\\nwork. Other work improves the ability of dialog models to generate factual text by attending over\\nfact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\\nmemory by editing the document index. This approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\\nrather than end-to-end learnt retrieval [9].\\nRetrieve-and-Edit approaches\\nOur method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\\nto provide a ﬁnal output. These approaches have proved successful in a number of domains including\\nMachine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6\\nDiscussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\\nfound that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual\\nand speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating\\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\\nanother objective. Our work opens up new research directions on how parametric and non-parametric\\nmemories interact and how to most effectively combine them, showing promise in being applied to a\\nwide variety of NLP tasks.\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 9}, page_content='Broader Impact\\nThis work offers several positive societal beneﬁts over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\nReferences\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\\nStoica, Saurabh Tiwary, and Tong Wang.\\nMS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n//arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In\\nInternational Conference of the Cross-Language Evaluation Forum for European Languages,\\npages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20.\\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013.\\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\\ning&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,\\n2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\\nhttps://www.aclweb.org/anthology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 10}, page_content='[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\\nhension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.\\narXiv: 1710.10723.\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\\nURL https://www.aclweb.org/anthology/N19-1423.\\n[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\\nLearning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\n[10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun\\nCho.\\nSearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\\narXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:\\n1704.05179.\\n[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\\nP18-1082.\\n[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\\nanthology/P19-1346.\\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\\nH1gx1CNKPH.\\n[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.\\nEntities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202,\\n2020. URL https://arxiv.org/abs/2004.07202.\\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\\nConference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/\\nAAAI/AAAI18/paper/view/16710.\\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\\nexceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL\\nhttp://arxiv.org/abs/1705.08807.\\n[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation.\\nIn AAAI Conference on Artiﬁcial Intelligence, 2018.\\nURL https:\\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, 32nd\\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018.\\n32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018\\nThrough 07-02-2018.\\n[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\\nediting prototypes. Transactions of the Association for Computational Linguistics, 6:437–450,\\n2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 11}, page_content='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:\\n//arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto,\\nKelvin Guu,\\nYonatan Oren,\\nand Percy S Liang.\\nA\\nretrieve-and-edit\\nframework\\nfor\\npredicting\\nstructured\\noutputs.\\nIn\\nS.\\nBengio,\\nH. Wallach,\\nH. Larochelle,\\nK. Grauman,\\nN. Cesa-Bianchi,\\nand R. Garnett,\\ned-\\nitors,\\nAdvances\\nin\\nNeural\\nInformation\\nProcessing\\nSystems\\n31,\\npages\\n10052–\\n10062.\\nCurran\\nAssociates,\\nInc.,\\n2018.\\nURL\\nhttp://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/\\nanthology/2020.acl-main.228.\\n[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\\ndoi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\\n[25] Armand Joulin and Tomas Mikolov.\\nInferring algorithmic patterns with stack-\\naugmented recurrent nets.\\nIn Proceedings of the 28th International Conference on\\nNeural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cam-\\nbridge,\\nMA, USA, 2015. MIT Press.\\nURL https://papers.nips.cc/paper/\\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\\narXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\\ntion through memorization: Nearest neighbor language models. In International Conference on\\nLearning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\\nhttp://arxiv.org/abs/1412.6980.\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,\\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov.\\nNatural Questions:\\na Benchmark for Ques-\\ntion Answering Research.\\nTransactions of the Association of Computational Lin-\\nguistics, 2019.\\nURL https://tomkwiat.users.x20web.corp.google.com/papers/\\nnatural-questions/main-1455-kwiatkowski.pdf.\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and\\nHerve Jegou.\\nLarge memory layers with product keys.\\nIn H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\\nformation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http:\\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.\\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 12}, page_content='for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\\nanthology/P19-1612.\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv preprint\\narXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, pages 110–119, San Diego, California, June 2016. Association for Computational\\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\\nN16-1014.\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\\nwith optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL\\nhttps://arxiv.org/abs/1909.03087.\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy,\\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\\nhttps://www.aclweb.org/anthology/P19-1291.\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\\nConference on Learning Representations, 2018. URL https://openreview.net/forum?\\nid=Hyg0vbWC-.\\n[37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search\\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv\\npreprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis\\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel.\\nHow decoding strategies affect the\\nveriﬁability of generated text.\\narXiv preprint arXiv:1911.03587, 2019.\\nURL https:\\n//arxiv.org/abs/1911.03587.\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\\nprecision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\\ning background knowledge for building conversation systems. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus-\\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\\nProcessing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\\nanthology/D18-1429.\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\\nTarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,\\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 13}, page_content='approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop\\nProceedings. CEUR-WS.org, 2016.\\nURL http://ceur-ws.org/Vol-1773/CoCoNIPS_\\n2016_paper9.pdf.\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\\narXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\\norg/anthology/N19-4009.\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\\nCho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\\n2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics.\\ndoi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong\\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H.\\nMiller, and Sebastian Riedel. How context affects language models’ factual predictions. In\\nAutomated Knowledge Base Construction, 2020. URL https://openreview.net/forum?\\nid=025X0zPfn.\\n[49] Alec\\nRadford,\\nKarthik\\nNarasimhan,\\nTim\\nSalimans,\\nand\\nIlya\\nSutskever.\\nIm-\\nproving\\nLanguage\\nUnderstanding\\nby\\nGenerative\\nPre-Training,\\n2018.\\nURL\\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\\nlanguage-unsupervised/language_understanding_paper.pdf.\\n[50] Alec\\nRadford,\\nJeff\\nWu,\\nRewon\\nChild,\\nDavid\\nLuan,\\nDario\\nAmodei,\\nand\\nIlya\\nSutskever.\\nLanguage models are unsupervised multitask learners,\\n2019.\\nURL\\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\\nmodels_are_unsupervised_multitask_learners.pdf.\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/\\n2002.08910.\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\\nbeyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\\n1500000019. URL https://doi.org/10.1561/1500000019.\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec\\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\\nArXiv, abs/1908.09203, 2019.\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\\nin Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 14}, page_content='[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\\nhttps://www.aclweb.org/anthology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\\nbiases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv, abs/2004.14366,\\n2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.\\nAAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/\\nanthology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\\nF. d\\\\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information\\nProcessing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://\\narxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain\\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\\nthe Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative\\nApplications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\\nAdvances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\\n2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\\nranking in open-domain question answering. In ICLR, 2018. URL https://openreview.\\nnet/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\\nhttp://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence\\ngeneration models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd\\nInternational Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium,\\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\\nhttps://www.aclweb.org/anthology/W18-5713.\\n15'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 15}, page_content='[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers:\\nState-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\\nods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, Novem-\\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\\nhttps://www.aclweb.org/anthology/D19-1253.\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019.\\nURL https://arxiv.org/abs/1909.03745.\\n16'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 16}, page_content='Appendices for Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nA\\nImplementation Details\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\\nwe did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,\\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\nB\\nHuman Evaluation\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\\nand a worked example appear when clicking \"view tool guide\".\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\\nAnnotators were encouraged to research the topic using the internet, and were given detailed instruc-\\ntions and worked examples in a full instructions tab. We included some gold sentences in order to\\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\\ntheir annotations were removed from the results.\\nC\\nTraining setup Details\\nWe train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision\\nﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\\ntraining and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search\\nwith FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring ∼100\\nGB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\\nTransformers [66]3, which achieves equivalent performance to the previous version but is a cleaner\\nand easier to use implementation. This version is also open-sourced. We also compress the document\\nindex using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to\\nrun experiments with RAG can be found at https://github.com/huggingface/transformers/\\nblob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\\nat https://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 17}, page_content='D\\nFurther Details on Open-Domain QA\\nFor open-domain QA, multiple answer annotations are often available for a given question. These\\nanswer annotations are exploited by extractive models during training as typically all the answer\\nannotations are used to ﬁnd matches within documents when preparing training data. For RAG, we\\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training\\nthe model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA,\\nthere are often many valid answers to a given question, some of which are not suitable training targets,\\nsuch as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur\\nin top 1000 documents for the query.\\nCuratedTrec preprocessing\\nThe answers for CuratedTrec are given in the form of regular expres-\\nsions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].\\nTo overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for\\neach query, and use the answer that most frequently matches the regex pattern as the supervision\\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for\\neach regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\nTriviaQA Evaluation setups\\nThe open-domain QA community customarily uses public develop-\\nment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\\ncompehension purposes. We report our results using the datasets splits used in DPR [26], which are\\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public\\nTriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set\\ninstead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\\nappendix of [14]). We report results on both test sets to enable fair comparison to both approaches.\\nWe ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more\\nconventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being\\nsimpler to answer from Wikipedia.\\nE\\nFurther Details on FEVER\\nFor FEVER classiﬁcation, we follow the practice from [32], and ﬁrst re-generate the claim, and\\nthen classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across\\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\\nﬁrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\\nwe explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia\\nas evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to\\nus, directly tackling this task is not straightforward. We hope to address this in future work.\\nF\\nNull Document Probabilities\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order\\nto model cases where no useful information could be retrieved for a given input. Here, if k documents\\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\\ndocument, before marginalizing over k + 1 predictions. We explored modelling this null document\\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\\n(iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in\\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\\ndocuments for questions that are less likely to beneﬁt from retrieval, suggesting that null document\\nmechanisms may not be necessary for RAG.\\nG\\nParameters\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n18'), Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 18}, page_content='Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\nTask\\nTrain\\nDevelopment\\nTest\\nNatural Questions\\n79169\\n8758\\n3611\\nTriviaQA\\n78786\\n8838\\n11314\\nWebQuestions\\n3418\\n362\\n2033\\nCuratedTrec\\n635\\n134\\n635\\nJeopardy Question Generation\\n97392\\n13714\\n26849\\nMS-MARCO\\n153726\\n12468\\n101093*\\nFEVER-3-way\\n145450\\n10000\\n10000\\nFEVER-2-way\\n96966\\n6666\\n6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating\\npoint precision to manage memory and disk footprints.\\nH\\nRetrieval Collapse\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\\nretrieval component would “collapse” and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\nI\\nNumber of instances per dataset\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\\n19'), Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 0}, page_content=\"Understanding the approximate nearest neighbor \\n(ANN) algorithm \\nBy Elastic Platform Team \\n17 April 2024 \\n \\nIf you grew up in a time before the internet made its debut, you’ll remember it wasn’t \\nalways easy to find new things to like. We discovered new bands when we happened to \\nhear them on the radio, we’d see a new TV show by accident because we forgot to \\nchange the channel, and we’d find a new favorite video game based almost entirely on \\nthe picture on the cover.  \\nNowadays, things are very different. Spotify will point me to artists that match my \\ntastes, Netflix will highlight movies and TV shows it knows we’ll enjoy, and Xbox knows \\nwhat we’ll probably want to play next. These recommendation systems make it so much \\neasier for us to find the things we’re actually looking for, and they’re powered by nearest \\nneighbor (NN) algorithms. NN looks at the extensive sea of information it has available \\nand identifies the closest thing to something you like, or something you’re searching for. \\nBut NN algorithms have an inherent flaw. If the amount of data they’re analyzing gets \\ntoo big, crawling through every option takes forever. This is a problem, especially as \\nthese data sources get bigger and bigger every year. This is where approximate nearest \\nneighbor (ANN) grabs the baton from NN and changes the game. \\nIn this document, we’ll cover the following key topics about ANN: \\n• \\nANN definition \\n• \\nHow ANN works \\n• \\nWhen to use ANN search \\n• \\nANN importance in vector search \\n• \\nVarious types of ANN algorithms \\n \\nApproximate nearest neighbor explained \\nApproximate nearest neighbor (ANN) is an algorithm that finds a data point in a data \\nset that's very close to the given query point, but not necessarily the absolute closest \\none. An NN algorithm searches exhaustively through all the data to find the perfect \\nmatch, whereas an ANN algorithm will settle for a match that’s close enough. \\nThis might sound like a worse solution, but it’s actually the key to nailing fast similarity \\nsearch. ANN uses intelligent shortcuts and data structures to efficiently navigate the \\nsearch space. So instead of taking up huge amounts of time and resources, it can\"), Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 1}, page_content='identify data points with much less effort that are close enough to be useful in most \\npractical scenarios. \\nEssentially, it’s a trade-off. If you absolutely need to find the one best match, you can do \\nthat at the expense of speed and performance with NN. But if you can tolerate a tiny \\ndrop in accuracy, ANN is almost always a better solution. \\n \\nHow approximate nearest neighbor algorithms work \\nThe first part of how ANN works is dimensionality reduction, where the goal is to turn a \\nhigher-dimensional data set into a lower-dimensional one. The aim is to make the \\npredictive model task less complicated and more efficient than having to analyze all the \\ndata. \\nThese algorithms rest on the mathematical concept of metric spaces — where data \\npoints reside and distances between them are defined. These distances must adhere to \\nspecific rules (non-negativity, identity, symmetry, triangle inequality), and common \\nfunctions like Euclidean distance or cosine similarity are used to calculate them.  \\nTo better understand this, imagine you’re on holiday searching for the villa you’ve \\nrented. Instead of checking every single building one-by-one (higher-dimensional), \\nyou’d use a map, which reduces the problem into two dimensions (lower-dimensional). \\n(This is a deliberately simplistic example. Dimensionality reduction is not the sole \\nmethod employed by ANN algorithms to improve efficiency.) \\nANN algorithms also leverage clever data structures called indexes to improve \\nefficiency. By pre-processing the data into these indexes, ANN can navigate the search \\nspace much quicker. Think of these as street signs, helping you find where you are on \\nthe map to reach your holiday villa quicker. \\n \\nWhen to use approximate nearest neighbor search \\nIn the fast-paced world of data science, efficiency reigns supreme. While finding the \\ntrue closest neighbor (exact nearest neighbor search) holds value, it often comes at a \\ncomputational cost, as we’ve already talked about. This is where ANN search shines, \\noffering a compelling trade-off: lightning speed with high, but not absolute, accuracy. \\nBut when exactly should you choose ANN over other search methods? \\nExact nearest neighbor might be slow, but it’s the best option when accuracy is your \\npriority or you’re using small data sets. k-nearest neighbors (kNN) sits between NN \\nand ANN by giving you faster results while maintaining high accuracy. But it can be hard \\nto get right when deciding the value of k, and it also struggles with high-dimensional \\ndata.'), Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 2}, page_content='ANN’s speed and efficiency combined with its high (but not absolute) accuracy makes it \\nperfect in a number of situations: \\n• \\nLarge data sets: When dealing with millions or even billions of data points, the \\nexhaustive nature of exact NN becomes sluggish. ANN excels in navigating vast \\ndata landscapes, delivering results swiftly. \\n• \\nHigh-dimensional data: As dimensions climb, exact NN computations explode. \\nANNs dimensionality reduction techniques effectively shrink the search space \\nand boost efficiency in complex data like images or text. \\n• \\nReal-time applications: Need results instantly? Recommendation systems, \\nfraud detection, and anomaly detection rely on real-time insights. ANN’s speed \\nmakes it ideal for these scenarios. \\n• \\nAcceptable approximation: If your application can tolerate slight inaccuracies \\nin results, ANN’s speed becomes invaluable. For example, in image search, \\nfinding visually similar images — instead of the absolute closest one — might be \\nsufficient. \\n \\nImportance of ANN in vector search \\nVector search deals with data encoded as dense vectors, capturing complex \\nrelationships and embedded meanings. This makes it ideal for searching content like \\nimages, text, and user preferences, where traditional keyword-based search often falls \\nshort. But the curse of dimensionality applies here, too. Because as the number of \\ndimensions representing these vectors increases, traditional search methods struggle, \\nbecoming slow and inefficient. \\nANN solves this problem by switching the focus from finding an exact match to “close \\nenough” matches. This enables fast retrieval, where your vector search can find similar \\nvectors in massive data sets lightning fast. It also gives you baked-in scalability, so you \\ncan grow your data set as much as you want without sacrificing speed. \\nThese real-time responses combined with the improved relevance and efficiency often \\nmean that ANN can play a critical role in unlocking the true potential of your vector \\nsearch. \\n \\nTypes of approximate nearest neighbor algorithms \\nWhile the concept of ANN offers a compelling speed advantage in search, this term \\nactually covers a diverse toolbox of algorithms. They all have their own strengths and \\ntrade-offs, and understanding these nuances is critical when choosing the right tool for \\nyour specific data and search needs.'), Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 3}, page_content='KD-trees \\nKD-trees organize data points in a hierarchical tree structure, partitioning the space \\nbased on specific dimensions. This enables fast and efficient search in low-\\ndimensional spaces and Euclidean distance-based queries. \\nBut while KD-trees excel at finding nearest neighbors in low dimensions, they suffer \\nfrom the “curse of dimensionality.” This is where, as the number of dimensions \\nincreases, the space between points explodes. In these high dimensions, KD-trees\\' \\nstrategy of splitting based on single axes becomes ineffective. This makes the search \\nexamine most of the data, losing the efficiency advantage and approaching the \\nslowness of a simple linear scan through all points. \\n \\nLocality-sensitive hashing (LSH) \\nLSH is a powerful ANN technique that works by \"hashing\" data points into lower-\\ndimensional spaces in a way that cleverly preserves their similarity relationships. This \\nclustering makes them easier to find, and it allows LSH to excel in searching massive, \\nhigh-dimensional data sets like images or text with both speed and scalability. And it \\ndoes all this while still returning \"close enough\" matches with good accuracy. But keep \\nin mind that LSH might also occasionally produce false positives (finding non-similar \\npoints as similar), and its effectiveness can vary based on the distance metric and data \\ntype. There are various LSH families designed to work with different metrics (e.g., \\nEuclidean distance, Jaccard similarity), which means LSH remains versatile. \\n \\nAnnoy \\nAnnoy (Approximate Nearest Neighbors Oh Yeah) isn\\'t a single algorithm, but an open-\\nsource C++ library that uses its own algorithms for building and querying trees, without \\ndirectly implementing LSH or KD-trees. It\\'s designed for memory-efficient and fast \\nsearch in high-dimensional spaces, making it suitable for real-time queries. Essentially, \\nit’s a user-friendly interface offering flexibility for different data types and search \\nscenarios. Annoy\\'s strength lies in leveraging multiple ANN approaches under one roof, \\nallowing you to choose the best fit for your needs. While it simplifies the process, \\nremember that picking the right internal algorithm within Annoy is crucial for optimal \\nperformance, and its effectiveness still depends on factors like your data and accuracy \\nrequirements.  \\n \\nLinear scan algorithm \\nAlthough not typically classified as an ANN technique, it’s worth mentioning linear scan \\nbecause it’s a brute-force approach that gives you similar results to other ANN \\nalgorithms. It iterates through every data point sequentially, calculating the distances'), Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 4}, page_content=\"between records and keeping track of the best matches. Because of the simplistic \\nnature of the algorithm, it’s easy to implement and great for small data sets. The \\ndownside of the more basic approach is that it’s inefficient for large data sets, slow \\nwhen used with high-dimensional data, and impractical for real-time applications. \\n \\nChoosing the right ANN \\nBefore you dive into picking an ANN, there are a few things you should consider before \\ndeciding: \\n• \\nData set size and dimensionality: Consider using locality-sensitive hashing for \\nlarge and high-dimensional data and KD-trees for smaller and lower-dimensional \\ndata. \\n• \\nDesired accuracy level: If absolute precision is vital, linear scan is likely the \\nbest option — otherwise, look at LSH or Annoy for good accuracy with speed. \\n• \\nComputational resources: Annoy offers flexibility, but consider memory and \\nprocessing limitations before choosing an algorithm within it. \\nRemember – there's no one-size-fits-all solution. Experiment with different ANN \\nalgorithms and evaluate their performance on your specific data to find the perfect \\nmatch for your vector search needs. Beyond these options, the world of ANN algorithms \\nis constantly evolving, so it’s also worth keeping an ear to the ground so you don’t miss \\nsomething new that could improve your search. \\n \\nANN is the secret sauce for better search \\nThe vast, complex world of data demands efficient tools to navigate its labyrinths. This \\nis where ANN can be the secret sauce that takes your similarity search from good to \\ngreat. It offers speed and scalability, albeit at the cost of a slight accuracy compromise. \\nAnd there is ongoing research with developments being made weekly, which will all \\ncontribute to the dynamic nature of ANN space. For instance, advancements in \\nquantum computing and machine learning could lead to new types of ANN algorithms \\nthat are even faster and more efficient. \\nWe've explored different ANN algorithms, each with its unique strengths and \\nweaknesses. But ultimately, the optimal choice depends on your specific needs. \\nConsider factors like data size, dimensionality, accuracy requirements, and resources. \\nExperiment, explore, and choose the right algorithm to get the most out of ANNs. From \\nimage search to fraud detection, these algorithms can make a huge difference, \\nrevealing hidden connections and empowering data-driven insights fast.\"), Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 5}, page_content='So, the next time you search for the next song, movie, or video game, remember the \\nsilent heroes behind the scenes — the ANN algorithms — joining the dots and making \\nconnections.')]\n"
     ]
    }
   ],
   "source": [
    "# Read the pdf files\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "# Load all the pdf files in the directory\n",
    "pdf_dir_path = \"../data/pdf_files\"\n",
    "pdf_loader = DirectoryLoader(\n",
    "    pdf_dir_path,\n",
    "    glob=\"**/*.pdf\", # Patter to match the files\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "pdf_documents = pdf_loader.load()\n",
    "print(f\"Number of pdf documents loaded: {len(pdf_documents)}\")\n",
    "print(pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf94da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 documents split into 428 chunks.\n",
      "Sample chunk:\n",
      "Content: Efﬁcient Estimation of Word Representations in\n",
      "Vector Space\n",
      "Tomas Mikolov\n",
      "Google Inc., Mountain View, CA\n",
      "tmikolov@google.com\n",
      "Kai Chen\n",
      "Google Inc., Mountain View, CA\n",
      "kaichen@google.com\n",
      "Greg Corrado\n",
      "Google Inc., Mountain View, CA\n",
      "gcorrado@google.com\n",
      "Jeffrey Dean\n",
      "Google Inc., Mountain View, CA\n",
      "jeff@google.com\n",
      "Abstract\n",
      "We propose two novel model architectures for computing continuous vector repre-\n",
      "sentations of words from very large data sets. The quality of these representations\n",
      "Metadata: {'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 0}, page_content='Efﬁcient Estimation of Word Representations in\\nVector Space\\nTomas Mikolov\\nGoogle Inc., Mountain View, CA\\ntmikolov@google.com\\nKai Chen\\nGoogle Inc., Mountain View, CA\\nkaichen@google.com\\nGreg Corrado\\nGoogle Inc., Mountain View, CA\\ngcorrado@google.com\\nJeffrey Dean\\nGoogle Inc., Mountain View, CA\\njeff@google.com\\nAbstract\\nWe propose two novel model architectures for computing continuous vector repre-\\nsentations of words from very large data sets. The quality of these representations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 0}, page_content='is measured in a word similarity task, and the results are compared to the previ-\\nously best performing techniques based on different types of neural networks. We\\nobserve large improvements in accuracy at much lower computational cost, i.e. it\\ntakes less than a day to learn high quality word vectors from a 1.6 billion words\\ndata set. Furthermore, we show that these vectors provide state-of-the-art perfor-\\nmance on our test set for measuring syntactic and semantic word similarities.\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 0}, page_content='1\\nIntroduction\\nMany current NLP systems and techniques treat words as atomic units - there is no notion of similar-\\nity between words, as these are represented as indices in a vocabulary. This choice has several good\\nreasons - simplicity, robustness and the observation that simple models trained on huge amounts of\\ndata outperform complex systems trained on less data. An example is the popular N-gram model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 0}, page_content='used for statistical language modeling - today, it is possible to train N-grams on virtually all available\\ndata (trillions of words [3]).\\nHowever, the simple techniques are at their limits in many tasks. For example, the amount of\\nrelevant in-domain data for automatic speech recognition is limited - the performance is usually\\ndominated by the size of high quality transcribed speech data (often just millions of words). In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 0}, page_content='machine translation, the existing corpora for many languages contain only a few billions of words\\nor less. Thus, there are situations where simple scaling up of the basic techniques will not result in\\nany signiﬁcant progress, and we have to focus on more advanced techniques.\\nWith progress of machine learning techniques in recent years, it has become possible to train more\\ncomplex models on much larger data set, and they typically outperform the simple models. Probably'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 0}, page_content='the most successful concept is to use distributed representations of words [10]. For example, neural\\nnetwork based language models signiﬁcantly outperform N-gram models [1, 27, 17].\\n1.1\\nGoals of the Paper\\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word\\nvectors from huge data sets with billions of words, and with millions of words in the vocabulary. As'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 0}, page_content='far as we know, none of the previously proposed architectures has been successfully trained on more\\n1\\narXiv:1301.3781v3  [cs.CL]  7 Sep 2013'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='than a few hundred of millions of words, with a modest dimensionality of the word vectors between\\n50 - 100.\\nWe use recently proposed techniques for measuring the quality of the resulting vector representa-\\ntions, with the expectation that not only will similar words tend to be close to each other, but that\\nwords can have multiple degrees of similarity [20]. This has been observed earlier in the context'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='of inﬂectional languages - for example, nouns can have multiple word endings, and if we search for\\nsimilar words in a subspace of the original vector space, it is possible to ﬁnd words that have similar\\nendings [13, 14].\\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple\\nsyntactic regularities. Using a word offset technique where simple algebraic operations are per-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='formed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vec-\\ntor(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20].\\nIn this paper, we try to maximize accuracy of these vector operations by developing new model\\narchitectures that preserve the linear regularities among words. We design a new comprehensive test\\nset for measuring both syntactic and semantic regularities1, and show that many such regularities'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends\\non the dimensionality of the word vectors and on the amount of the training data.\\n1.2\\nPrevious Work\\nRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular model\\narchitecture for estimating neural network language model (NNLM) was proposed in [1], where a\\nfeedforward neural network with a linear projection layer and a non-linear hidden layer was used to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='learn jointly the word vector representation and a statistical language model. This work has been\\nfollowed by many others.\\nAnother interesting architecture of NNLM was presented in [13, 14], where the word vectors are\\nﬁrst learned using neural network with a single hidden layer. The word vectors are then used to train\\nthe NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='work, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are\\nlearned using a simple model.\\nIt was later shown that the word vectors can be used to signiﬁcantly improve and simplify many\\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='vectors were made available for future research and comparison2. However, as far as we know, these\\narchitectures were signiﬁcantly more computationally expensive for training than the one proposed\\nin [13], with the exception of certain version of log-bilinear model where diagonal weight matrices\\nare used [23].\\n2\\nModel Architectures\\nMany different types of models were proposed for estimating continuous representations of words,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\\nIn this paper, we focus on distributed representations of words learned by neural networks, as it was\\npreviously shown that they perform signiﬁcantly better than LSA for preserving linear regularities\\namong words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.\\nSimilar to [18], to compare different model architectures we deﬁne ﬁrst the computational complex-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 1}, page_content='ity of a model as the number of parameters that need to be accessed to fully train the model. Next,\\nwe will try to maximize the accuracy, while minimizing the computational complexity.\\n1The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt\\n2http://ronan.collobert.com/senna/\\nhttp://metaoptimize.com/projects/wordreprs/\\nhttp://www.fit.vutbr.cz/˜imikolov/rnnlm/\\nhttp://ai.stanford.edu/˜ehhuang/\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='For all the following models, the training complexity is proportional to\\nO = E × T × Q,\\n(1)\\nwhere E is number of the training epochs, T is the number of the words in the training set and Q is\\ndeﬁned further for each model architecture. Common choice is E = 3 −50 and T up to one billion.\\nAll models are trained using stochastic gradient descent and backpropagation [26].\\n2.1\\nFeedforward Neural Net Language Model (NNLM)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='2.1\\nFeedforward Neural Net Language Model (NNLM)\\nThe probabilistic feedforward neural network language model has been proposed in [1]. It consists\\nof input, projection, hidden and output layers. At the input layer, N previous words are encoded\\nusing 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a\\nprojection layer P that has dimensionality N × D, using a shared projection matrix. As only N'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='inputs are active at any given time, composition of the projection layer is a relatively cheap operation.\\nThe NNLM architecture becomes complex for computation between the projection and the hidden\\nlayer, as values in the projection layer are dense. For a common choice of N = 10, the size of the\\nprojection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000\\nunits. Moreover, the hidden layer is used to compute probability distribution over all the words in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='vocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity\\nper each training example is\\nQ = N × D + N × D × H + H × V,\\n(2)\\nwhere the dominating term is H × V . However, several practical solutions were proposed for\\navoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized\\nmodels completely by using models that are not normalized during training [4, 9]. With binary tree'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='representations of the vocabulary, the number of output units that need to be evaluated can go down\\nto around log2(V ). Thus, most of the complexity is caused by the term N × D × H.\\nIn our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary\\ntree. This follows previous observations that the frequency of words works well for obtaining classes\\nin neural net language models [16]. Huffman trees assign short binary codes to frequent words, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='this further reduces the number of output units that need to be evaluated: while balanced binary tree\\nwould require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires\\nonly about log2(Unigram perplexity(V )). For example when the vocabulary size is one million\\nwords, this results in about two times speedup in evaluation. While this is not crucial speedup for\\nneural network LMs as the computational bottleneck is in the N ×D×H term, we will later propose'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='architectures that do not have hidden layers and thus depend heavily on the efﬁciency of the softmax\\nnormalization.\\n2.2\\nRecurrent Neural Net Language Model (RNNLM)\\nRecurrent neural network based language model has been proposed to overcome certain limitations\\nof the feedforward NNLM, such as the need to specify the context length (the order of the model N),\\nand because theoretically RNNs can efﬁciently represent more complex patterns than the shallow'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='neural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and\\noutput layer. What is special for this type of model is the recurrent matrix that connects hidden\\nlayer to itself, using time-delayed connections. This allows the recurrent model to form some kind\\nof short term memory, as information from the past can be represented by the hidden layer state that\\ngets updated based on the current input and the state of the hidden layer in the previous time step.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 2}, page_content='The complexity per training example of the RNN model is\\nQ = H × H + H × V,\\n(3)\\nwhere the word representations D have the same dimensionality as the hidden layer H. Again, the\\nterm H × V can be efﬁciently reduced to H × log2(V ) by using hierarchical softmax. Most of the\\ncomplexity then comes from H × H.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='2.3\\nParallel Training of Neural Networks\\nTo train models on huge data sets, we have implemented several models on top of a large-scale\\ndistributed framework called DistBelief [6], including the feedforward NNLM and the new models\\nproposed in this paper. The framework allows us to run multiple replicas of the same model in\\nparallel, and each replica synchronizes its gradient updates through a centralized server that keeps'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='all the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with\\nan adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use\\none hundred or more model replicas, each using many CPU cores at different machines in a data\\ncenter.\\n3\\nNew Log-linear Models\\nIn this section, we propose two new model architectures for learning distributed representations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='of words that try to minimize computational complexity. The main observation from the previous\\nsection was that most of the complexity is caused by the non-linear hidden layer in the model. While\\nthis is what makes neural networks so attractive, we decided to explore simpler models that might\\nnot be able to represent the data as precisely as neural networks, but can possibly be trained on much\\nmore data efﬁciently.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='more data efﬁciently.\\nThe new architectures directly follow those proposed in our earlier work [13, 14], where it was\\nfound that neural network language model can be successfully trained in two steps: ﬁrst, continuous\\nword vectors are learned using simple model, and then the N-gram NNLM is trained on top of these\\ndistributed representations of words. While there has been later substantial amount of work that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='focuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.\\nNote that related models have been proposed also much earlier [26, 8].\\n3.1\\nContinuous Bag-of-Words Model\\nThe ﬁrst proposed architecture is similar to the feedforward NNLM, where the non-linear hidden\\nlayer is removed and the projection layer is shared for all words (not just the projection matrix);'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='thus, all words get projected into the same position (their vectors are averaged). We call this archi-\\ntecture a bag-of-words model as the order of words in the history does not inﬂuence the projection.\\nFurthermore, we also use words from the future; we have obtained the best performance on the task\\nintroduced in the next section by building a log-linear classiﬁer with four future and four history'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='words at the input, where the training criterion is to correctly classify the current (middle) word.\\nTraining complexity is then\\nQ = N × D + D × log2(V ).\\n(4)\\nWe denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous\\ndistributed representation of the context. The model architecture is shown at Figure 1. Note that the\\nweight matrix between the input and the projection layer is shared for all word positions in the same\\nway as in the NNLM.\\n3.2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='way as in the NNLM.\\n3.2\\nContinuous Skip-gram Model\\nThe second architecture is similar to CBOW, but instead of predicting the current word based on the\\ncontext, it tries to maximize classiﬁcation of a word based on another word in the same sentence.\\nMore precisely, we use each current word as an input to a log-linear classiﬁer with continuous\\nprojection layer, and predict words within a certain range before and after the current word. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='found that increasing the range improves quality of the resulting word vectors, but it also increases\\nthe computational complexity. Since the more distant words are usually less related to the current\\nword than those close to it, we give less weight to the distant words by sampling less from those\\nwords in our training examples.\\nThe training complexity of this architecture is proportional to\\nQ = C × (D + D × log2(V )),\\n(5)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 3}, page_content='Q = C × (D + D × log2(V )),\\n(5)\\nwhere C is the maximum distance of the words. Thus, if we choose C = 5, for each training word\\nwe will select randomly a number R in range < 1; C >, and then use R words from history and\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 4}, page_content='w(t-2)\\nw(t+1)\\nw(t-1)\\nw(t+2)\\nw(t)\\nSUM\\n       INPUT         PROJECTION         OUTPUT\\nw(t)\\n          INPUT         PROJECTION      OUTPUT\\nw(t-2)\\nw(t-1)\\nw(t+1)\\nw(t+2)\\n                   CBOW                                                   Skip-gram\\nFigure 1: New model architectures. The CBOW architecture predicts the current word based on the\\ncontext, and the Skip-gram predicts surrounding words given the current word.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 4}, page_content='R words from the future of the current word as correct labels. This will require us to do R × 2\\nword classiﬁcations, with the current word as input, and each of the R + R words as output. In the\\nfollowing experiments, we use C = 10.\\n4\\nResults\\nTo compare the quality of different versions of word vectors, previous papers typically use a table\\nshowing example words and their most similar words, and understand them intuitively. Although'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 4}, page_content='it is easy to show that word France is similar to Italy and perhaps some other countries, it is much\\nmore challenging when subjecting those vectors in a more complex similarity task, as follows. We\\nfollow previous observation that there can be many different types of similarities between words, for\\nexample, word big is similar to bigger in the same sense that small is similar to smaller. Example'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 4}, page_content='of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further\\ndenote two pairs of words with the same relationship as a question, as we can ask: ”What is the\\nword that is similar to small in the same sense as biggest is similar to big?”\\nSomewhat surprisingly, these questions can be answered by performing simple algebraic operations\\nwith the vector representation of words. To ﬁnd a word that is similar to small in the same sense as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 4}, page_content='biggest is similar to big, we can simply compute vector X = vector(”biggest”)−vector(”big”)+\\nvector(”small”). Then, we search in the vector space for the word closest to X measured by cosine\\ndistance, and use it as the answer to the question (we discard the input question words during this\\nsearch). When the word vectors are well trained, it is possible to ﬁnd the correct answer (word\\nsmallest) using this method.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 4}, page_content='smallest) using this method.\\nFinally, we found that when we train high dimensional word vectors on a large amount of data, the\\nresulting vectors can be used to answer very subtle semantic relationships between words, such as\\na city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors\\nwith such semantic relationships could be used to improve many existing NLP applications, such'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 4}, page_content='as machine translation, information retrieval and question answering systems, and may enable other\\nfuture applications yet to be invented.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 5}, page_content='Table 1: Examples of ﬁve types of semantic and nine types of syntactic questions in the Semantic-\\nSyntactic Word Relationship test set.\\nType of relationship\\nWord Pair 1\\nWord Pair 2\\nCommon capital city\\nAthens\\nGreece\\nOslo\\nNorway\\nAll capital cities\\nAstana\\nKazakhstan\\nHarare\\nZimbabwe\\nCurrency\\nAngola\\nkwanza\\nIran\\nrial\\nCity-in-state\\nChicago\\nIllinois\\nStockton\\nCalifornia\\nMan-Woman\\nbrother\\nsister\\ngrandson\\ngranddaughter\\nAdjective to adverb\\napparent\\napparently\\nrapid\\nrapidly\\nOpposite\\npossibly\\nimpossibly'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 5}, page_content='rapid\\nrapidly\\nOpposite\\npossibly\\nimpossibly\\nethical\\nunethical\\nComparative\\ngreat\\ngreater\\ntough\\ntougher\\nSuperlative\\neasy\\neasiest\\nlucky\\nluckiest\\nPresent Participle\\nthink\\nthinking\\nread\\nreading\\nNationality adjective\\nSwitzerland\\nSwiss\\nCambodia\\nCambodian\\nPast tense\\nwalking\\nwalked\\nswimming\\nswam\\nPlural nouns\\nmouse\\nmice\\ndollar\\ndollars\\nPlural verbs\\nwork\\nworks\\nspeak\\nspeaks\\n4.1\\nTask Description\\nTo measure quality of the word vectors, we deﬁne a comprehensive test set that contains ﬁve types'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 5}, page_content='of semantic questions, and nine types of syntactic questions. Two examples from each category are\\nshown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions\\nin each category were created in two steps: ﬁrst, a list of similar word pairs was created manually.\\nThen, a large list of questions is formed by connecting two word pairs. For example, we made a\\nlist of 68 large American cities and the states they belong to, and formed about 2.5K questions by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 5}, page_content='picking two word pairs at random. We have included in our test set only single token words, thus\\nmulti-word entities are not present (such as New York).\\nWe evaluate the overall accuracy for all question types, and for each question type separately (se-\\nmantic, syntactic). Question is assumed to be correctly answered only if the closest word to the\\nvector computed using the above method is exactly the same as the correct word in the question;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 5}, page_content='synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely\\nto be impossible, as the current models do not have any input information about word morphology.\\nHowever, we believe that usefulness of the word vectors for certain applications should be positively\\ncorrelated with this accuracy metric. Further progress can be achieved by incorporating information\\nabout structure of words, especially for the syntactic questions.\\n4.2\\nMaximization of Accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 5}, page_content='4.2\\nMaximization of Accuracy\\nWe have used a Google News corpus for training the word vectors. This corpus contains about\\n6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we\\nare facing time constrained optimization problem, as it can be expected that both using more data\\nand higher dimensional word vectors will improve the accuracy. To estimate the best choice of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 5}, page_content='model architecture for obtaining as good as possible results quickly, we have ﬁrst evaluated models\\ntrained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.\\nThe results using the CBOW architecture with different choice of word vector dimensionality and\\nincreasing amount of the training data are shown in Table 2.\\nIt can be seen that after some point, adding more dimensions or adding more training data provides'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 5}, page_content='diminishing improvements. So, we have to increase both vector dimensionality and the amount\\nof the training data together. While this observation might seem trivial, it must be noted that it is\\ncurrently popular to train word vectors on relatively large amounts of data, but with insufﬁcient size\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 6}, page_content='Table 2:\\nAccuracy on subset of the Semantic-Syntactic Word Relationship test set, using word\\nvectors from the CBOW architecture with limited vocabulary. Only questions containing words from\\nthe most frequent 30k words are used.\\nDimensionality / Training words\\n24M\\n49M\\n98M\\n196M\\n391M\\n783M\\n50\\n13.4\\n15.7\\n18.6\\n19.1\\n22.5\\n23.2\\n100\\n19.4\\n23.1\\n27.8\\n28.7\\n33.4\\n32.2\\n300\\n23.2\\n29.2\\n35.3\\n38.6\\n43.7\\n45.9\\n600\\n24.0\\n30.1\\n36.5\\n40.8\\n46.6\\n50.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 6}, page_content='38.6\\n43.7\\n45.9\\n600\\n24.0\\n30.1\\n36.5\\n40.8\\n46.6\\n50.4\\nTable 3: Comparison of architectures using models trained on the same data, with 640-dimensional\\nword vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set,\\nand on the syntactic relationship test set of [20]\\nModel\\nSemantic-Syntactic Word Relationship test set\\nMSR Word Relatedness\\nArchitecture\\nSemantic Accuracy [%]\\nSyntactic Accuracy [%]\\nTest Set [20]\\nRNNLM\\n9\\n36\\n35\\nNNLM\\n23\\n53\\n47\\nCBOW\\n24\\n64\\n61\\nSkip-gram\\n55\\n59\\n56'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 6}, page_content='35\\nNNLM\\n23\\n53\\n47\\nCBOW\\n24\\n64\\n61\\nSkip-gram\\n55\\n59\\n56\\n(such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the\\nsame increase of computational complexity as increasing vector size twice.\\nFor the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradi-\\nent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so\\nthat it approaches zero at the end of the last training epoch.\\n4.3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 6}, page_content='4.3\\nComparison of Model Architectures\\nFirst we compare different model architectures for deriving the word vectors using the same training\\ndata and using the same dimensionality of 640 of the word vectors. In the further experiments, we\\nuse full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to\\nthe 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic\\nsimilarity between words3.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 6}, page_content='similarity between words3.\\nThe training data consists of several LDC corpora and is described in detail in [18] (320M words,\\n82K vocabulary). We used these data to provide a comparison to a previously trained recurrent\\nneural network language model that took about 8 weeks to train on a single CPU. We trained a feed-\\nforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 6}, page_content='using a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the\\nprojection layer has size 640 × 8).\\nIn Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly\\non the syntactic questions. The NNLM vectors perform signiﬁcantly better than the RNN - this is\\nnot surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 6}, page_content='layer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the\\nsame on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic\\ntask than the CBOW model (but still better than the NNLM), and much better on the semantic part\\nof the test than all the other models.\\nNext, we evaluated our models trained using one CPU only and compared the results against publicly'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 6}, page_content='available word vectors. The comparison is given in Table 4. The CBOW model was trained on subset\\n3We thank Geoff Zweig for providing us the test set.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 7}, page_content='Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relation-\\nship test set, and word vectors from our models. Full vocabularies are used.\\nModel\\nVector\\nTraining\\nAccuracy [%]\\nDimensionality\\nwords\\nSemantic\\nSyntactic\\nTotal\\nCollobert-Weston NNLM\\n50\\n660M\\n9.3\\n12.3\\n11.0\\nTurian NNLM\\n50\\n37M\\n1.4\\n2.6\\n2.1\\nTurian NNLM\\n200\\n37M\\n1.4\\n2.2\\n1.8\\nMnih NNLM\\n50\\n37M\\n1.8\\n9.1\\n5.8\\nMnih NNLM\\n100\\n37M\\n3.3\\n13.2\\n8.8\\nMikolov RNNLM\\n80\\n320M\\n4.9\\n18.4\\n12.7\\nMikolov RNNLM\\n640\\n320M\\n8.6\\n36.5\\n24.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 7}, page_content='18.4\\n12.7\\nMikolov RNNLM\\n640\\n320M\\n8.6\\n36.5\\n24.6\\nHuang NNLM\\n50\\n990M\\n13.3\\n11.6\\n12.3\\nOur NNLM\\n20\\n6B\\n12.9\\n26.4\\n20.3\\nOur NNLM\\n50\\n6B\\n27.9\\n55.8\\n43.2\\nOur NNLM\\n100\\n6B\\n34.2\\n64.5\\n50.8\\nCBOW\\n300\\n783M\\n15.5\\n53.1\\n36.1\\nSkip-gram\\n300\\n783M\\n50.0\\n55.9\\n53.3\\nTable 5: Comparison of models trained for three epochs on the same data and models trained for\\none epoch. Accuracy is reported on the full Semantic-Syntactic data set.\\nModel\\nVector\\nTraining\\nAccuracy [%]\\nTraining time\\nDimensionality\\nwords\\n[days]\\nSemantic\\nSyntactic'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 7}, page_content='Dimensionality\\nwords\\n[days]\\nSemantic\\nSyntactic\\nTotal\\n3 epoch CBOW\\n300\\n783M\\n15.5\\n53.1\\n36.1\\n1\\n3 epoch Skip-gram\\n300\\n783M\\n50.0\\n55.9\\n53.3\\n3\\n1 epoch CBOW\\n300\\n783M\\n13.8\\n49.9\\n33.6\\n0.3\\n1 epoch CBOW\\n300\\n1.6B\\n16.1\\n52.6\\n36.1\\n0.6\\n1 epoch CBOW\\n600\\n783M\\n15.4\\n53.3\\n36.2\\n0.7\\n1 epoch Skip-gram\\n300\\n783M\\n45.6\\n52.2\\n49.2\\n1\\n1 epoch Skip-gram\\n300\\n1.6B\\n52.2\\n55.1\\n53.8\\n2\\n1 epoch Skip-gram\\n600\\n783M\\n56.7\\n54.5\\n55.5\\n2.5\\nof the Google News data in about a day, while training time for the Skip-gram model was about three\\ndays.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 7}, page_content='days.\\nFor experiments reported further, we used just one training epoch (again, we decrease the learning\\nrate linearly so that it approaches zero at the end of training). Training a model on twice as much\\ndata using one epoch gives comparable or better results than iterating over the same data for three\\nepochs, as is shown in Table 5, and provides additional small speedup.\\n4.4\\nLarge Scale Parallel Training of Models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 7}, page_content='4.4\\nLarge Scale Parallel Training of Models\\nAs mentioned earlier, we have implemented various models in a distributed framework called Dis-\\ntBelief. Below we report the results of several models trained on the Google News 6B data set,\\nwith mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Ada-\\ngrad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 8}, page_content='Table 6:\\nComparison of models trained using the DistBelief distributed framework. Note that\\ntraining of NNLM with 1000-dimensional vectors would take too long to complete.\\nModel\\nVector\\nTraining\\nAccuracy [%]\\nTraining time\\nDimensionality\\nwords\\n[days x CPU cores]\\nSemantic\\nSyntactic\\nTotal\\nNNLM\\n100\\n6B\\n34.2\\n64.5\\n50.8\\n14 x 180\\nCBOW\\n1000\\n6B\\n57.3\\n68.9\\n63.7\\n2 x 140\\nSkip-gram\\n1000\\n6B\\n66.1\\n65.1\\n65.6\\n2.5 x 125\\nTable 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 8}, page_content='Architecture\\nAccuracy [%]\\n4-gram [32]\\n39\\nAverage LSA similarity [32]\\n49\\nLog-bilinear model [24]\\n54.8\\nRNNLMs [19]\\n55.4\\nSkip-gram\\n48.0\\nSkip-gram + RNNLMs\\n58.9\\nestimate since the data center machines are shared with other production tasks, and the usage can\\nﬂuctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of\\nthe CBOW model and the Skip-gram model are much closer to each other than their single-machine'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 8}, page_content='implementations. The result are reported in Table 6.\\n4.5\\nMicrosoft Research Sentence Completion Challenge\\nThe Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing\\nlanguage modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one\\nword is missing in each sentence and the goal is to select word that is the most coherent with the\\nrest of the sentence, given a list of ﬁve reasonable choices. Performance of several techniques has'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 8}, page_content='been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear\\nmodel [24] and a combination of recurrent neural networks that currently holds the state of the art\\nperformance of 55.4% accuracy on this benchmark [19].\\nWe have explored the performance of Skip-gram architecture on this task. First, we train the 640-\\ndimensional model on 50M words provided in [32]. Then, we compute score of each sentence in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 8}, page_content='the test set by using the unknown word at the input, and predict all surrounding words in a sentence.\\nThe ﬁnal sentence score is then the sum of these individual predictions. Using the sentence scores,\\nwe choose the most likely sentence.\\nA short summary of some previous results together with the new results is presented in Table 7.\\nWhile the Skip-gram model itself does not perform on this task better than LSA similarity, the scores'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 8}, page_content='from this model are complementary to scores obtained with RNNLMs, and a weighted combination\\nleads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and\\n58.7% on the test part of the set).\\n5\\nExamples of the Learned Relationships\\nTable 8 shows words that follow various relationships. We follow the approach described above: the\\nrelationship is deﬁned by subtracting two word vectors, and the result is added to another word. Thus'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 8}, page_content='for example, Paris - France + Italy = Rome. As it can be seen, accuracy is quite good, although\\nthere is clearly a lot of room for further improvements (note that using our accuracy metric that\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 9}, page_content='Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-\\ngram model trained on 783M words with 300 dimensionality).\\nRelationship\\nExample 1\\nExample 2\\nExample 3\\nFrance - Paris\\nItaly: Rome\\nJapan: Tokyo\\nFlorida: Tallahassee\\nbig - bigger\\nsmall: larger\\ncold: colder\\nquick: quicker\\nMiami - Florida\\nBaltimore: Maryland\\nDallas: Texas\\nKona: Hawaii\\nEinstein - scientist\\nMessi: midﬁelder\\nMozart: violinist\\nPicasso: painter\\nSarkozy - France\\nBerlusconi: Italy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 9}, page_content='Sarkozy - France\\nBerlusconi: Italy\\nMerkel: Germany\\nKoizumi: Japan\\ncopper - Cu\\nzinc: Zn\\ngold: Au\\nuranium: plutonium\\nBerlusconi - Silvio\\nSarkozy: Nicolas\\nPutin: Medvedev\\nObama: Barack\\nMicrosoft - Windows\\nGoogle: Android\\nIBM: Linux\\nApple: iPhone\\nMicrosoft - Ballmer\\nGoogle: Yahoo\\nIBM: McNealy\\nApple: Jobs\\nJapan - sushi\\nGermany: bratwurst\\nFrance: tapas\\nUSA: pizza\\nassumes exact match, the results in Table 8 would score only about 60%). We believe that word'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 9}, page_content='vectors trained on even larger data sets with larger dimensionality will perform signiﬁcantly better,\\nand will enable the development of new innovative applications. Another way to improve accuracy is\\nto provide more than one example of the relationship. By using ten examples instead of one to form\\nthe relationship vector (we average the individual vectors together), we have observed improvement\\nof accuracy of our best models by about 10% absolutely on the semantic-syntactic test.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 9}, page_content='It is also possible to apply the vector operations to solve different tasks. For example, we have\\nobserved good accuracy for selecting out-of-the-list words, by computing average vector for a list of\\nwords, and ﬁnding the most distant word vector. This is a popular type of problems in certain human\\nintelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.\\n6\\nConclusion'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 9}, page_content='6\\nConclusion\\nIn this paper we studied the quality of vector representations of words derived by various models on\\na collection of syntactic and semantic language tasks. We observed that it is possible to train high\\nquality word vectors using very simple model architectures, compared to the popular neural network\\nmodels (both feedforward and recurrent). Because of the much lower computational complexity, it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 9}, page_content='is possible to compute very accurate high dimensional word vectors from a much larger data set.\\nUsing the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram\\nmodels even on corpora with one trillion words, for basically unlimited size of the vocabulary. That\\nis several orders of magnitude larger than the best previously published results for similar models.\\nAn interesting task where the word vectors have recently been shown to signiﬁcantly outperform the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 9}, page_content='previous state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were\\nused together with other techniques to achieve over 50% increase in Spearman’s rank correlation\\nover the previous best result [31]. The neural network based word vectors were previously applied\\nto many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can\\nbe expected that these applications can beneﬁt from the model architectures described in this paper.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 9}, page_content='Our ongoing work shows that the word vectors can be successfully applied to automatic extension\\nof facts in Knowledge Bases, and also for veriﬁcation of correctness of existing facts. Results\\nfrom machine translation experiments also look very promising. In the future, it would be also\\ninteresting to compare our techniques to Latent Relational Analysis [30] and others. We believe that\\nour comprehensive test set will help the research community to improve the existing techniques for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 9}, page_content='estimating the word vectors. We also expect that high quality word vectors will become an important\\nbuilding block for future NLP applications.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 10}, page_content='7\\nFollow-Up Work\\nAfter the initial version of this paper was written, we published single-machine multi-threaded C++\\ncode for computing the word vectors, using both the continuous bag-of-words and skip-gram archi-\\ntectures4. The training speed is signiﬁcantly higher than reported earlier in this paper, i.e. it is in the\\norder of billions of words per hour for typical hyperparameter choices. We also published more than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 10}, page_content='1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of\\nour follow-up work will be published in an upcoming NIPS 2013 paper [21].\\nReferences\\n[1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-\\nchine Learning Research, 3:1137-1155, 2003.\\n[2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-\\nchines, MIT Press, 2007.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 10}, page_content='chines, MIT Press, 2007.\\n[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine\\ntranslation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language\\nProcessing and Computational Language Learning, 2007.\\n[4] R. Collobert and J. Weston. A Uniﬁed Architecture for Natural Language Processing: Deep\\nNeural Networks with Multitask Learning. In International Conference on Machine Learning,\\nICML, 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 10}, page_content='ICML, 2008.\\n[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan-\\nguage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-\\n2537, 2011.\\n[6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A.\\nSenior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.\\n[7] J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 10}, page_content='stochastic optimization. Journal of Machine Learning Research, 2011.\\n[8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.\\n[9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representations\\nvia Global Context and Multiple Word Prototypes. In: Proc. Association for Computational\\nLinguistics, 2012.\\n[10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 10}, page_content='tributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations,\\nMIT Press, 1986.\\n[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring\\ndegrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic\\nEvaluation (SemEval 2012), 2012.\\n[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for\\nsentiment analysis. In Proceedings of ACL, 2011.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 10}, page_content='sentiment analysis. In Proceedings of ACL, 2011.\\n[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-\\nversity of Technology, 2007.\\n[14] T. Mikolov, J. Kopeck´y, L. Burget, O. Glembek and J. ˇCernock´y. Neural network based lan-\\nguage models for higly inﬂective languages, In: Proc. ICASSP 2009.\\n[15] T. Mikolov, M. Karaﬁ´at, L. Burget, J. ˇCernock´y, S. Khudanpur. Recurrent neural network\\nbased language model, In: Proceedings of Interspeech, 2010.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 10}, page_content='[16] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock´y, S. Khudanpur. Extensions of recurrent neural\\nnetwork language model, In: Proceedings of ICASSP 2011.\\n[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ˇCernock´y. Empirical Evaluation and Com-\\nbination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.\\n4The code is available at https://code.google.com/p/word2vec/\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 11}, page_content='[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. ˇCernock´y. Strategies for Training Large Scale\\nNeural Network Language Models, In: Proc. Automatic Speech Recognition and Understand-\\ning, 2011.\\n[19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-\\nsity of Technology, 2012.\\n[20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-\\ntations. NAACL HLT 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 11}, page_content='tations. NAACL HLT 2013.\\n[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of\\nWords and Phrases and their Compositionality. Accepted to NIPS 2013.\\n[22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,\\n2007.\\n[23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural\\nInformation Processing Systems 21, MIT Press, 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 11}, page_content='[24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language\\nmodels. ICML, 2012.\\n[25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,\\n2005.\\n[26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-\\npropagating errors. Nature, 323:533.536, 1986.\\n[27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,\\n2007.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 11}, page_content='2007.\\n[28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and\\nUnfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.\\n[29] J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for\\nSemi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.\\n[30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2013-09-10T00:03:46+00:00', 'source': '../data/pdf_files/embeddings.pdf', 'file_path': '../data/pdf_files/embeddings.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-09-10T00:03:46+00:00', 'trapped': '', 'modDate': 'D:20130910000346Z', 'creationDate': 'D:20130910000346Z', 'page': 11}, page_content='tional Joint Conference on Artiﬁcial Intelligence, 2005.\\n[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for\\nMeasuring Relational Similarity. NAACL HLT 2013.\\n[32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft\\nResearch Technical Report MSR-TR-2011-129, 2011.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='Billion-scale similarity search with GPUs\\nJeff Johnson\\nFacebook AI Research\\nNew York\\nMatthijs Douze\\nFacebook AI Research\\nParis\\nHerv´e J´egou\\nFacebook AI Research\\nParis\\nABSTRACT\\nSimilarity search ﬁnds application in specialized database\\nsystems handling complex data such as images or videos,\\nwhich are typically represented by high-dimensional features\\nand require speciﬁc indexing structures. This paper tackles\\nthe problem of better utilizing GPUs for this task. While'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='GPUs excel at data-parallel tasks, prior approaches are bot-\\ntlenecked by algorithms that expose less parallelism, such as\\nk-min selection, or make poor use of the memory hierarchy.\\nWe propose a design for k-selection that operates at up\\nto 55% of theoretical peak performance, enabling a nearest\\nneighbor implementation that is 8.5× faster than prior GPU\\nstate of the art. We apply it in diﬀerent similarity search\\nscenarios, by proposing optimized design for brute-force, ap-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='proximate and compressed-domain search based on product\\nquantization. In all these setups, we outperform the state of\\nthe art by large margins. Our implementation enables the\\nconstruction of a high accuracy k-NN graph on 95 million\\nimages from the Yfcc100M dataset in 35 minutes, and of\\na graph connecting 1 billion vectors in less than 12 hours\\non 4 Maxwell Titan X GPUs. We have open-sourced our\\napproach1 for the sake of comparison and reproducibility.\\n1.\\nINTRODUCTION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='1.\\nINTRODUCTION\\nImages and videos constitute a new massive source of data\\nfor indexing and search. Extensive metadata for this con-\\ntent is often not available. Search and interpretation of this\\nand other human-generated content, like text, is diﬃcult and\\nimportant. A variety of machine learning and deep learn-\\ning algorithms are being used to interpret and classify these\\ncomplex, real-world entities. Popular examples include the\\ntext representation known as word2vec [32], representations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='of images by convolutional neural networks [39, 19], and im-\\nage descriptors for instance search [20]. Such representations\\nor embeddings are usually real-valued, high-dimensional vec-\\ntors of 50 to 1000+ dimensions. Many of these vector repre-\\nsentations can only eﬀectively be produced on GPU systems,\\n1https://github.com/facebookresearch/faiss\\nas the underlying processes either have high arithmetic com-\\nplexity and/or high data bandwidth demands [28], or cannot'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='be eﬀectively partitioned without failing due to communi-\\ncation overhead or representation quality [38].\\nOnce pro-\\nduced, their manipulation is itself arithmetically intensive.\\nHowever, how to utilize GPU assets is not straightforward.\\nMore generally, how to exploit new heterogeneous architec-\\ntures is a key subject for the database community [9].\\nIn this context, searching by numerical similarity rather\\nthan via structured relations is more suitable. This could be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='to ﬁnd the most similar content to a picture, or to ﬁnd the\\nvectors that have the highest response to a linear classiﬁer\\non all vectors of a collection.\\nOne of the most expensive operations to be performed on\\nlarge collections is to compute a k-NN graph. It is a directed\\ngraph where each vector of the database is a node and each\\nedge connects a node to its k nearest neighbors.\\nThis is\\nour ﬂagship application. Note, state of the art methods like'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='NN-Descent [15] have a large memory overhead on top of\\nthe dataset itself and cannot readily scale to the billion-sized\\ndatabases we consider.\\nSuch applications must deal with the curse of dimension-\\nality [46], rendering both exhaustive search or exact index-\\ning for non-exhaustive search impractical on billion-scale\\ndatabases.\\nThis is why there is a large body of work on\\napproximate search and/or graph construction. To handle\\nhuge datasets that do not ﬁt in RAM, several approaches'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='employ an internal compressed representation of the vec-\\ntors using an encoding.\\nThis is especially convenient for\\nmemory-limited devices like GPUs. It turns out that accept-\\ning a minimal accuracy loss results in orders of magnitude\\nof compression [21]. The most popular vector compression\\nmethods can be classiﬁed into either binary codes [18, 22],\\nor quantization methods [25, 37]. Both have the desirable\\nproperty that searching neighbors does not require recon-\\nstructing the vectors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='structing the vectors.\\nOur paper focuses on methods based on product quanti-\\nzation (PQ) codes, as these were shown to be more eﬀective\\nthan binary codes [34]. In addition, binary codes incur im-\\nportant overheads for non-exhaustive search methods [35].\\nSeveral improvements were proposed after the original prod-\\nuct quantization proposal known as IVFADC [25]; most are\\ndiﬃcult to implement eﬃciently on GPU. For instance, the\\ninverted multi-index [4], useful for high-speed/low-quality'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 0}, page_content='operating points, depends on a complicated “multi-sequence”\\nalgorithm. The optimized product quantization or OPQ [17]\\nis a linear transformation on the input vectors that improves\\nthe accuracy of the product quantization; it can be applied\\n1\\narXiv:1702.08734v1  [cs.CV]  28 Feb 2017'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='as a pre-processing. The SIMD-optimized IVFADC imple-\\nmentation from [2] operates only with sub-optimal parame-\\nters (few coarse quantization centroids). Many other meth-\\nods, like LOPQ and the Polysemous codes [27, 16] are too\\ncomplex to be implemented eﬃciently on GPUs.\\nThere are many implementations of similarity search on\\nGPUs, but mostly with binary codes [36], small datasets [44],\\nor exhaustive search [14, 40, 41]. To the best of our knowl-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='edge, only the work by Wieschollek et al. [47] appears suit-\\nable for billion-scale datasets with quantization codes. This\\nis the prior state of the art on GPUs, which we compare\\nagainst in Section 6.4.\\nThis paper makes the following contributions:\\n• a GPU k-selection algorithm, operating in fast register\\nmemory and ﬂexible enough to be fusable with other\\nkernels, for which we provide a complexity analysis;\\n• a near-optimal algorithmic layout for exact and ap-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='proximate k-nearest neighbor search on GPU;\\n• a range of experiments that show that these improve-\\nments outperform previous art by a large margin on\\nmid- to large-scale nearest-neighbor search tasks, in\\nsingle or multi-GPU conﬁgurations.\\nThe paper is organized as follows. Section 2 introduces\\nthe context and notation.\\nSection 3 reviews GPU archi-\\ntecture and discusses problems appearing when using it for\\nsimilarity search. Section 4 introduces one of our main con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='tributions, i.e., our k-selection method for GPUs, while Sec-\\ntion 5 provides details regarding the algorithm computation\\nlayout. Finally, Section 6 provides extensive experiments for\\nour approach, compares it to the state of the art, and shows\\nconcrete use cases for image collections.\\n2.\\nPROBLEM STATEMENT\\nWe are concerned with similarity search in vector collec-\\ntions. Given the query vector x ∈Rd and the collection2\\n[yi]i=0:ℓ(yi ∈Rd), we search:\\nL = k-argmini=0:ℓ∥x −yi∥2,\\n(1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='L = k-argmini=0:ℓ∥x −yi∥2,\\n(1)\\ni.e., we search the k nearest neighbors of x in terms of L2\\ndistance. The L2 distance is used most often, as it is op-\\ntimized by design when learning several embeddings (e.g.,\\n[20]), due to its attractive linear algebra properties.\\nThe lowest distances are collected by k-selection. For an\\narray [ai]i=0:ℓ, k-selection ﬁnds the k lowest valued elements\\n[asi]i=0:k, asi ≤asi+1, along with the indices [si]i=0:k, 0 ≤'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='si < ℓ, of those elements from the input array. The ai will be\\n32-bit ﬂoating point values; the si are 32- or 64-bit integers.\\nOther comparators are sometimes desired; e.g., for cosine\\nsimilarity we search for highest values. The order between\\nequivalent keys asi = asj is not speciﬁed.\\nBatching.\\nTypically, searches are performed in batches\\nof nq query vectors [xj]j=0:nq (xj ∈Rd) in parallel, which\\nallows for more ﬂexibility when executing on multiple CPU'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='threads or on GPU. Batching for k-selection entails selecting\\nnq × k elements and indices from nq separate arrays, where\\neach array is of a potentially diﬀerent length ℓi ≥k.\\n2To avoid clutter in 0-based indexing, we use the array no-\\ntation 0 : ℓto denote the range {0, ..., ℓ−1} inclusive.\\nExact search. The exact solution computes the full pair-\\nwise distance matrix D = [∥xj −yi∥2\\n2]j=0:nq,i=0:ℓ∈Rnq×ℓ.\\nIn practice, we use the decomposition\\n∥xj −yi∥2\\n2 = ∥xj∥2 + ∥yi∥2 −2⟨xj, yi⟩.\\n(2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='∥xj −yi∥2\\n2 = ∥xj∥2 + ∥yi∥2 −2⟨xj, yi⟩.\\n(2)\\nThe two ﬁrst terms can be precomputed in one pass over\\nthe matrices X and Y whose rows are the [xj] and [yi]. The\\nbottleneck is to evaluate ⟨xj, yi⟩, equivalent to the matrix\\nmultiplication XY ⊤. The k-nearest neighbors for each of\\nthe nq queries are k-selected along each row of D.\\nCompressed-domain search. From now on, we focus on\\napproximate nearest-neighbor search. We consider, in par-\\nticular, the IVFADC indexing structure [25]. The IVFADC'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='index relies on two levels of quantization, and the database\\nvectors are encoded. The database vector y is approximated\\nas:\\ny ≈q(y) = q1(y) + q2(y −q1(y))\\n(3)\\nwhere q1 : Rd →C1 ⊂Rd and q2 : Rd →C2 ⊂Rd are quan-\\ntizers; i.e., functions that output an element from a ﬁnite\\nset. Since the sets are ﬁnite, q(y) is encoded as the index of\\nq1(y) and that of q2(y −q1(y)). The ﬁrst-level quantizer is a\\ncoarse quantizer and the second level ﬁne quantizer encodes\\nthe residual vector after the ﬁrst level.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='the residual vector after the ﬁrst level.\\nThe Asymmetric Distance Computation (ADC) search\\nmethod returns an approximate result:\\nLADC = k-argmini=0:ℓ∥x −q(yi)∥2.\\n(4)\\nFor IVFADC the search is not exhaustive.\\nVectors for\\nwhich the distance is computed are pre-selected depending\\non the ﬁrst-level quantizer q1:\\nLIVF = τ-argminc∈C1∥x −c∥2.\\n(5)\\nThe multi-probe parameter τ is the number of coarse-level\\ncentroids we consider.\\nThe quantizer operates a nearest-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='The quantizer operates a nearest-\\nneighbor search with exact distances, in the set of reproduc-\\ntion values. Then, the IVFADC search computes\\nLIVFADC =\\nk-argmin\\ni=0:ℓs.t. q1(yi)∈LIVF\\n∥x −q(yi)∥2.\\n(6)\\nHence, IVFADC relies on the same distance estimations as\\nthe two-step quantization of ADC, but computes them only\\non a subset of vectors.\\nThe corresponding data structure, the inverted ﬁle, groups\\nthe vectors yi into |C1| inverted lists I1, ..., I|C1| with homo-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='geneous q1(yi). Therefore, the most memory-intensive op-\\neration is computing LIVFADC, and boils down to linearly\\nscanning τ inverted lists.\\nThe quantizers. The quantizers q1 and q2 have diﬀerent\\nproperties. q1 needs to have a relatively low number of repro-\\nduction values so that the number of inverted lists does not\\nexplode. We typically use |C1| ≈\\n√\\nℓ, trained via k-means.\\nFor q2, we can aﬀord to spend more memory for a more ex-\\ntensive representation. The ID of the vector (a 4- or 8-byte'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 1}, page_content='integer) is also stored in the inverted lists, so it makes no\\nsense to have shorter codes than that; i.e., log2 |C2| > 4 × 8.\\nProduct quantizer. We use a product quantizer [25] for q2,\\nwhich provides a large number of reproduction values with-\\nout increasing the processing cost. It interprets the vector y\\nas b sub-vectors y = [y0...yb−1], where b is an even divisor of\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='the dimension d. Each sub-vector is quantized with its own\\nquantizer, yielding the tuple (q0(y0), ..., qb−1(yb−1)). The\\nsub-quantizers typically have 256 reproduction values, to ﬁt\\nin one byte. The quantization value of the product quantizer\\nis then q2(y) = q0(y0) + 256 × q1(y1) + ... + 256b−1 × qb−1,\\nwhich from a storage point of view is just the concatena-\\ntion of the bytes produced by each sub-quantizer. Thus, the\\nproduct quantizer generates b-byte codes with |C2| = 256b'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='reproduction values. The k-means dictionaries of the quan-\\ntizers are small and quantization is computationally cheap.\\n3.\\nGPU: OVERVIEW AND K-SELECTION\\nThis section reviews salient details of Nvidia’s general-\\npurpose GPU architecture and programming model [30]. We\\nthen focus on one of the less GPU-compliant parts involved\\nin similarity search, namely the k-selection, and discuss the\\nliterature and challenges.\\n3.1\\nArchitecture\\nGPU lanes and warps. The Nvidia GPU is a general-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='GPU lanes and warps. The Nvidia GPU is a general-\\npurpose computer that executes instruction streams using\\na 32-wide vector of CUDA threads (the warp); individual\\nthreads in the warp are referred to as lanes, with a lane\\nID from 0 – 31. Despite the “thread” terminology, the best\\nanalogy to modern vectorized multicore CPUs is that each\\nwarp is a separate CPU hardware thread, as the warp shares\\nan instruction counter. Warp lanes taking diﬀerent execu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='tion paths results in warp divergence, reducing performance.\\nEach lane has up to 255 32-bit registers in a shared register\\nﬁle. The CPU analogy is that there are up to 255 vector\\nregisters of width 32, with warp lanes as SIMD vector lanes.\\nCollections of warps. A user-conﬁgurable collection of 1\\nto 32 warps comprises a block or a co-operative thread ar-\\nray (CTA). Each block has a high speed shared memory, up\\nto 48 KiB in size. Individual CUDA threads have a block-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='relative ID, called a thread id, which can be used to parti-\\ntion and assign work. Each block is run on a single core of\\nthe GPU called a streaming multiprocessor (SM). Each SM\\nhas functional units, including ALUs, memory load/store\\nunits, and various special instruction units. A GPU hides\\nexecution latencies by having many operations in ﬂight on\\nwarps across all SMs. Each individual warp lane instruction\\nthroughput is low and latency is high, but the aggregate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='arithmetic throughput of all SMs together is 5 – 10× higher\\nthan typical CPUs.\\nGrids and kernels. Blocks are organized in a grid of blocks\\nin a kernel. Each block is assigned a grid relative ID. The\\nkernel is the unit of work (instruction stream with argu-\\nments) scheduled by the host CPU for the GPU to execute.\\nAfter a block runs through to completion, new blocks can\\nbe scheduled. Blocks from diﬀerent kernels can run concur-\\nrently. Ordering between kernels is controllable via ordering'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='primitives such as streams and events.\\nResources and occupancy. The number of blocks execut-\\ning concurrently depends upon shared memory and register\\nresources used by each block. Per-CUDA thread register us-\\nage is determined at compilation time, while shared memory\\nusage can be chosen at runtime. This usage aﬀects occu-\\npancy on the GPU. If a block demands all 48 KiB of shared\\nmemory for its private usage, or 128 registers per thread as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='opposed to 32, then only 1 – 2 other blocks can run concur-\\nrently on the same SM, resulting in low occupancy. Under\\nhigh occupancy more blocks will be present across all SMs,\\nallowing more work to be in ﬂight at once.\\nMemory types. Diﬀerent blocks and kernels communicate\\nthrough global memory, typically 4 – 32 GB in size, with 5 –\\n10× higher bandwidth than CPU main memory.\\nShared\\nmemory is analogous to CPU L1 cache in terms of speed.\\nGPU register ﬁle memory is the highest bandwidth memory.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='In order to maintain the high number of instructions in ﬂight\\non a GPU, a vast register ﬁle is also required: 14 MB in the\\nlatest Pascal P100, in contrast with a few tens of KB on\\nCPU. A ratio of 250 : 6.25 : 1 for register to shared to global\\nmemory aggregate cross-sectional bandwidth is typical on\\nGPU, yielding 10 – 100s of TB/s for the register ﬁle [10].\\n3.2\\nGPU register ﬁle usage\\nStructured register data. Shared and register memory\\nusage involves eﬃciency tradeoﬀs; they lower occupancy but'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='can increase overall performance by retaining a larger work-\\ning set in a faster memory. Making heavy use of register-\\nresident data at the expense of occupancy or instead of\\nshared memory is often proﬁtable [43].\\nAs the GPU register ﬁle is very large, storing structured\\ndata (not just temporary operands) is useful. A single lane\\ncan use its (scalar) registers to solve a local task, but with\\nlimited parallelism and storage.\\nInstead, lanes in a GPU'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='Instead, lanes in a GPU\\nwarp can instead exchange register data using the warp shuf-\\nﬂe instruction, enabling warp-wide parallelism and storage.\\nLane-stride register array. A common pattern to achieve\\nthis is a lane-stride register array. That is, given elements\\n[ai]i=0:ℓ, each successive value is held in a register by neigh-\\nboring lanes. The array is stored in ℓ/32 registers per lane,\\nwith ℓa multiple of 32. Lane j stores {aj, a32+j, ..., aℓ−32+j},'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='while register r holds {a32r, a32r+1, ..., a32r+31}.\\nFor manipulating the [ai], the register in which ai is stored\\n(i.e., ⌊i/32⌋) and ℓmust be known at assembly time, while\\nthe lane (i.e., i mod 32) can be runtime knowledge. A wide\\nvariety of access patterns (shift, any-to-any) are provided;\\nwe use the butterﬂy permutation [29] extensively.\\n3.3\\nk-selection on CPU versus GPU\\nk-selection algorithms, often for arbitrarily large ℓand\\nk, can be translated to a GPU, including radix selection'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='and bucket selection [1], probabilistic selection [33], quick-\\nselect [14], and truncated sorts [40]. Their performance is\\ndominated by multiple passes over the input in global mem-\\nory. Sometimes for similarity search, the input distances are\\ncomputed on-the-ﬂy or stored only in small blocks, not in\\ntheir entirety. The full, explicit array might be too large to\\nﬁt into any memory, and its size could be unknown at the\\nstart of the processing, rendering algorithms that require'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 2}, page_content='multiple passes impractical. They suﬀer from other issues\\nas well.\\nQuickselect requires partitioning on a storage of\\nsize O(ℓ), a data-dependent memory movement. This can\\nresult in excessive memory transactions, or requiring parallel\\npreﬁx sums to determine write oﬀsets, with synchronization\\noverhead. Radix selection has no partitioning but multiple\\npasses are still required.\\nHeap parallelism. In similarity search applications, one\\nis usually interested only in a small number of results, k <\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='1000 or so. In this regime, selection via max-heap is a typi-\\ncal choice on the CPU, but heaps do not expose much data\\nparallelism (due to serial tree update) and cannot saturate\\nSIMD execution units. The ad-heap [31] takes better advan-\\ntage of parallelism available in heterogeneous systems, but\\nstill attempts to partition serial and parallel work between\\nappropriate execution units.\\nDespite the serial nature of\\nheap update, for small k the CPU can maintain all of its'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='state in the L1 cache with little eﬀort, and L1 cache latency\\nand bandwidth remains a limiting factor. Other similarity\\nsearch components, like PQ code manipulation, tend to have\\ngreater impact on CPU performance [2].\\nGPU heaps.\\nHeaps can be similarly implemented on a\\nGPU [7]. However, a straightforward GPU heap implemen-\\ntation suﬀers from high warp divergence and irregular, data-\\ndependent memory movement, since the path taken for each\\ninserted element depends upon other values in the heap.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='GPU parallel priority queues [24] improve over the serial\\nheap update by allowing multiple concurrent updates, but\\nthey require a potential number of small sorts for each insert\\nand data-dependent memory movement. Moreover, it uses\\nmultiple synchronization barriers through kernel launches in\\ndiﬀerent streams, plus the additional latency of successive\\nkernel launches and coordination with the CPU host.\\nOther more novel GPU algorithms are available for small'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='k, namely the selection algorithm in the fgknn library [41].\\nThis is a complex algorithm that may suﬀer from too many\\nsynchronization points, greater kernel launch overhead, us-\\nage of slower memories, excessive use of hierarchy, partition-\\ning and buﬀering. However, we take inspiration from this\\nparticular algorithm through the use of parallel merges as\\nseen in their merge queue structure.\\n4.\\nFAST K-SELECTION ON THE GPU\\nFor any CPU or GPU algorithm, either memory or arith-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='metic throughput should be the limiting factor as per the\\nrooﬂine performance model [48]. For input from global mem-\\nory, k-selection cannot run faster than the time required to\\nscan the input once at peak memory bandwidth. We aim to\\nget as close to this limit as possible. Thus, we wish to per-\\nform a single pass over the input data (from global memory\\nor produced on-the-ﬂy, perhaps fused with a kernel that is\\ngenerating the data).\\nWe want to keep intermediate state in the fastest memory:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='the register ﬁle. The major disadvantage of register memory\\nis that the indexing into the register ﬁle must be known at\\nassembly time, which is a strong constraint on the algorithm.\\n4.1\\nIn-register sorting\\nWe use an in-register sorting primitive as a building block.\\nSorting networks are commonly used on SIMD architec-\\ntures [13], as they exploit vector parallelism. They are eas-\\nily implemented on the GPU, and we build sorting networks\\nwith lane-stride register arrays.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='with lane-stride register arrays.\\nWe use a variant of Batcher’s bitonic sorting network [8],\\nwhich is a set of parallel merges on an array of size 2k. Each\\nmerge takes s arrays of length t (s and t a power of 2) to s/2\\narrays of length 2t, using log2(t) parallel steps. A bitonic\\nsort applies this merge recursively: to sort an array of length\\nℓ, merge ℓarrays of length 1 to ℓ/2 arrays of length 2, to ℓ/4\\narrays of length 4, successively to 1 sorted array of length ℓ,\\nleading to 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='leading to 1\\n2(log2(ℓ)2 + log2(ℓ)) parallel merge steps.\\nAlgorithm 1 Odd-size merging network\\nfunction merge-odd([Li]i=0:ℓL, [Ri]i=0:ℓR)\\nparallel for i ←0 : min(ℓL, ℓR) do\\n▷inverted 1st stage; inputs are already sorted\\ncompare-swap(LℓL−i−1, Ri)\\nend for\\nparallel do\\n▷If ℓL = ℓR and a power-of-2, these are equivalent\\nmerge-odd-continue([Li]i=0:ℓL, left)\\nmerge-odd-continue([Ri]i=0:ℓR, right)\\nend do\\nend function\\nfunction merge-odd-continue([xi]i=0:ℓ, p)\\nif ℓ> 1 then\\nh ←2⌈log2 ℓ⌉−1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='if ℓ> 1 then\\nh ←2⌈log2 ℓ⌉−1\\n▷largest power-of-2 < ℓ\\nparallel for i ←0 : ℓ−h do\\n▷Implemented with warp shuﬄe butterﬂy\\ncompare-swap(xi, xi+h)\\nend for\\nparallel do\\nif p = left then\\n▷left side recursion\\nmerge-odd-continue([xi]i=0:ℓ−h, left)\\nmerge-odd-continue([xi]i=ℓ−h:ℓ, right)\\nelse\\n▷right side recursion\\nmerge-odd-continue([xi]i=0:h, left)\\nmerge-odd-continue([xi]i=h:ℓ, right)\\nend if\\nend do\\nend if\\nend function\\nOdd-size merging and sorting networks. If some input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='data is already sorted, we can modify the network to avoid\\nmerging steps. We may also not have a full power-of-2 set of\\ndata, in which case we can eﬃciently shortcut to deal with\\nthe smaller size.\\nAlgorithm 1 is an odd-sized merging network that merges\\nalready sorted left and right arrays, each of arbitrary length.\\nWhile the bitonic network merges bitonic sequences, we start\\nwith monotonic sequences: sequences sorted monotonically.\\nA bitonic merge is made monotonic by reversing the ﬁrst'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 3}, page_content='comparator stage.\\nThe odd size algorithm is derived by considering arrays to\\nbe padded to the next highest power-of-2 size with dummy\\n1\\n3\\n4\\n8\\n9\\n0\\n3\\n7\\n1\\n3\\n4\\n3\\n0\\n9\\n8\\n7\\n0\\n3\\n4\\n3\\n1\\n7\\n8\\n9\\n0\\n3\\n1\\n3\\n4\\n7\\n8\\n9\\n0\\n1\\n3\\n3\\n4\\n7\\n8\\n9\\nstep 1\\nstep 2\\nstep 3\\nstep 4\\nFigure 1:\\nOdd-size network merging arrays of sizes\\n5 and 3.\\nBullets indicate parallel compare/swap.\\nDashed lines are elided elements or comparisons.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='input \\ninsertion\\nthread queue\\nmerging  network\\nwarp queue\\nlane 0\\nlane 1\\nlane 31\\ncoalesced \\nread\\n. . . . .\\n. . . . .\\n. . . . .\\n. . . . . . . . . . .\\n. . . . . . . . . . . . . . . . . . . .  \\n. . . . . . . . . . . . . . . . . . . . \\n. . . . . . . . . \\n. . . . . . . . . . . .\\nFigure 2:\\nOverview of WarpSelect. The input val-\\nues stream in on the left, and the warp queue on the\\nright holds the output result.\\nelements that are never swapped (the merge is monotonic)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='and are already properly positioned; any comparisons with\\ndummy elements are elided. A left array is considered to\\nbe padded with dummy elements at the start; a right ar-\\nray has them at the end.\\nA merge of two sorted arrays\\nof length ℓL and ℓR to a sorted array of ℓL + ℓR requires\\n⌈log2(max(ℓL, ℓR))⌉+1 parallel steps. Figure 1 shows Algo-\\nrithm 1’s merging network for arrays of size 5 and 3, with 4\\nparallel steps.\\nThe compare-swap is implemented using warp shuﬄes on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='a lane-stride register array. Swaps with a stride a multiple\\nof 32 occur directly within a lane as the lane holds both\\nelements locally. Swaps of stride ≤16 or a non-multiple of\\n32 occur with warp shuﬄes. In practice, used array lengths\\nare multiples of 32 as they are held in lane-stride arrays.\\nAlgorithm 2 Odd-size sorting network\\nfunction sort-odd([xi]i=0:ℓ)\\nif ℓ> 1 then\\nparallel do\\nsort-odd([xi]i=0:⌊ℓ/2⌋)\\nsort-odd([xi]i=⌊ℓ/2⌋:ℓ)\\nend do\\nmerge-odd([xi]i=0:⌊ℓ/2⌋, [xi]i=⌊ℓ/2⌋:ℓ)\\nend if'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='merge-odd([xi]i=0:⌊ℓ/2⌋, [xi]i=⌊ℓ/2⌋:ℓ)\\nend if\\nend function\\nAlgorithm 2 extends the merge to a full sort. Assuming no\\nstructure present in the input data, 1\\n2(⌈log2(ℓ)⌉2+⌈log2(ℓ)⌉)\\nparallel steps are required for sorting data of length ℓ.\\n4.2\\nWarpSelect\\nOur k-selection implementation, WarpSelect, maintains\\nstate entirely in registers, requires only a single pass over\\ndata and avoids cross-warp synchronization. It uses merge-\\nodd and sort-odd as primitives. Since the register ﬁle pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='vides much more storage than shared memory, it supports\\nk ≤1024. Each warp is dedicated to k-selection to a single\\none of the n arrays [ai]. If n is large enough, a single warp\\nper each [ai] will result in full GPU occupancy. Large ℓper\\nwarp is handled by recursive decomposition, if ℓis known in\\nadvance.\\nOverview. Our approach (Algorithm 3 and Figure 2) oper-\\nates on values, with associated indices carried along (omit-\\nted from the description for simplicity). It selects the k least'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='values that come from global memory, or from intermediate\\nvalue registers if fused into another kernel providing the val-\\nues. Let [ai]i=0:ℓbe the sequence provided for selection.\\nThe elements (on the left of Figure 2) are processed in\\ngroups of 32, the warp size. Lane j is responsible for pro-\\ncessing {aj, a32+j, ...}; thus, if the elements come from global\\nmemory, the reads are contiguous and coalesced into a min-\\nimal number of memory transactions.\\nData structures.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='Data structures.\\nEach lane j maintains a small queue\\nof t elements in registers, called the thread queues [T j\\ni ]i=0:t,\\nordered from largest to smallest (T j\\ni ≥T j\\ni+1). The choice of\\nt is made relative to k, see Section 4.3. The thread queue is\\na ﬁrst-level ﬁlter for new values coming in. If a new a32i+j\\nis greater than the largest key currently in the queue, T j\\n0 , it\\nis guaranteed that it won’t be in the k smallest ﬁnal results.\\nThe warp shares a lane-stride register array of k smallest'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='seen elements, [Wi]i=0:k, called the warp queue. It is ordered\\nfrom smallest to largest (Wi ≤Wi+1); if the requested k is\\nnot a multiple of 32, we round it up. This is a second level\\ndata structure that will be used to maintain all of the k\\nsmallest warp-wide seen values. The thread and warp queues\\nare initialized to maximum sentinel values, e.g., +∞.\\nUpdate. The three invariants maintained are:\\n• all per-lane T j\\n0 are not in the min-k\\n• all per-lane T j\\n0 are greater than all warp queue keys'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='0 are greater than all warp queue keys\\nWi\\n• all ai seen so far in the min-k are contained in either\\nsome lane’s thread queue ([T j\\ni ]i=0:t,j=0:32), or in the\\nwarp queue.\\nLane j receives a new a32i+j and attempts to insert it into\\nits thread queue. If a32i+j > T j\\n0 , then the new pair is by\\ndeﬁnition not in the k minimum, and can be rejected.\\nOtherwise, it is inserted into its proper sorted position\\nin the thread queue, thus ejecting the old T j\\n0 .\\nAll lanes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='0 .\\nAll lanes\\ncomplete doing this with their new received pair and their\\nthread queue, but it is now possible that the second invariant\\nhave been violated. Using the warp ballot instruction, we\\ndetermine if any lane has violated the second invariant. If\\nnot, we are free to continue processing new elements.\\nRestoring the invariants. If any lane has its invariant\\nviolated, then the warp uses odd-merge to merge and sort\\nthe thread and warp queues together. The new warp queue'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='Algorithm 3 WarpSelect pseudocode for lane j\\nfunction WarpSelect(a)\\nif a < T j\\n0 then\\ninsert a into our [T j\\ni ]i=0:t\\nend if\\nif warp-ballot(T j\\n0 < Wk−1) then\\n▷Reinterpret thread queues as lane-stride array\\n[αi]i=0:32t ←cast([T j\\ni ]i=0:t,j=0:32)\\n▷concatenate and sort thread queues\\nsort-odd([αi]i=0:32t)\\nmerge-odd([Wi]i=0:k, [αi]i=0:32t)\\n▷Reinterpret lane-stride array as thread queues\\n[T j\\ni ]i=0:t,j=0:32 ←cast([αi]i=0:32t)\\nreverse-array([Ti]i=0:t)\\n▷Back in thread queue order, invariant restored'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 4}, page_content='▷Back in thread queue order, invariant restored\\nend if\\nend function\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='will be the min-k elements across the merged, sorted queues,\\nand the new thread queues will be the remainder, from min-\\n(k +1) to min-(k +32t+1). This restores the invariants and\\nwe are free to continue processing subsequent elements.\\nSince the thread and warp queues are already sorted, we\\nmerge the sorted warp queue of length k with 32 sorted\\narrays of length t. Supporting odd-sized merges is important\\nbecause Batcher’s formulation would require that 32t = k'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='and is a power-of-2; thus if k = 1024, t must be 32. We\\nfound that the optimal t is way smaller (see below).\\nUsing odd-merge to merge the 32 already sorted thread\\nqueues would require a struct-of-arrays to array-of-structs\\ntransposition in registers across the warp, since the t succes-\\nsive sorted values are held in diﬀerent registers in the same\\nlane rather than a lane-stride array. This is possible [12],\\nbut would use a comparable number of warp shuﬄes, so we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='just reinterpret the thread queue registers as an (unsorted)\\nlane-stride array and sort from scratch. Signiﬁcant speedup\\nis realizable by using odd-merge for the merge of the ag-\\ngregate sorted thread queues with the warp queue.\\nHandling the remainder. If there are remainder elements\\nbecause ℓis not a multiple of 32, those are inserted into the\\nthread queues for the lanes that have them, after which we\\nproceed to the output stage.\\nOutput. A ﬁnal sort and merge is made of the thread and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='warp queues, after which the warp queue holds all min-k\\nvalues.\\n4.3\\nComplexity and parameter selection\\nFor each incoming group of 32 elements, WarpSelect\\ncan perform 1, 2 or 3 constant-time operations, all happen-\\ning in warp-wide parallel time:\\n1. read 32 elements, compare to all thread queue heads\\nT j\\n0 , cost C1, happens N1 times;\\n2. if ∃j ∈{0, ..., 31}, a32n+j < T j\\n0 , perform insertion sort\\non those speciﬁc thread queues, cost C2 = O(t), hap-\\npens N2 times;\\n3. if ∃j, T j'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='pens N2 times;\\n3. if ∃j, T j\\n0 < Wk−1, sort and merge queues, cost C3 =\\nO(t log(32t)2 + k log(max(k, 32t))), happens N3 times.\\nThus, the total cost is N1C1 + N2C2 + N3C3. N1 = ℓ/32,\\nand on random data drawn independently, N2 = O(k log(ℓ))\\nand N3 = O(k log(ℓ)/t), see the Appendix for a full deriva-\\ntion. Hence, the trade-oﬀis to balance a cost in N2C2 and\\none in N3C3. The practical choice for t given k and ℓwas\\nmade by experiment on a variety of k-NN data. For k ≤32,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='we use t = 2, k ≤128 uses t = 3, k ≤256 uses t = 4, and\\nk ≤1024 uses t = 8, all irrespective of ℓ.\\n5.\\nCOMPUTATION LAYOUT\\nThis section explains how IVFADC, one of the indexing\\nmethods originally built upon product quantization [25], is\\nimplemented eﬃciently. Details on distance computations\\nand articulation with k-selection are the key to understand-\\ning why this method can outperform more recent GPU-\\ncompliant approximate nearest neighbor strategies [47].\\n5.1\\nExact search'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='5.1\\nExact search\\nWe brieﬂy come back to the exhaustive search method,\\noften referred to as exact brute-force. It is interesting on its\\nown for exact nearest neighbor search in small datasets. It\\nis also a component of many indexes in the literature. In\\nour case, we use it for the IVFADC coarse quantizer q1.\\nAs stated in Section 2, the distance computation boils\\ndown to a matrix multiplication. We use optimized GEMM\\nroutines in the cuBLAS library to calculate the −2⟨xj, yi⟩'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='term for L2 distance, resulting in a partial distance matrix\\nD′. To complete the distance calculation, we use a fused\\nk-selection kernel that adds the ∥yi∥2 term to each entry of\\nthe distance matrix and immediately submits the value to\\nk-selection in registers. The ∥xj∥2 term need not be taken\\ninto account before k-selection. Kernel fusion thus allows\\nfor only 2 passes (GEMM write, k-select read) over D′, com-\\npared to other implementations that may require 3 or more.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='Row-wise k-selection is likely not fusable with a well-tuned\\nGEMM kernel, or would result in lower overall eﬃciency.\\nAs D′ does not ﬁt in GPU memory for realistic problem\\nsizes, the problem is tiled over the batch of queries, with\\ntq ≤nq queries being run in a single tile. Each of the ⌈nq/tq⌉\\ntiles are independent problems, but we run two in parallel\\non diﬀerent streams to better occupy the GPU, so the eﬀec-\\ntive memory requirement of D is O(2ℓtq). The computation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='can similarly be tiled over ℓ. For very large input coming\\nfrom the CPU, we support buﬀering with pinned memory\\nto overlap CPU to GPU copy with GPU compute.\\n5.2\\nIVFADC indexing\\nPQ lookup tables. At its core, the IVFADC requires com-\\nputing the distance from a vector to a set of product quanti-\\nzation reproduction values. By developing Equation (6) for\\na database vector y, we obtain:\\n∥x −q(y)∥2\\n2 = ∥x −q1(y) −q2(y −q1(y))∥2\\n2.\\n(7)\\nIf we decompose the residual vectors left after q1 as:\\ny −q1(y)\\n='),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='y −q1(y)\\n=\\n[ ey1 · · · eyb] and\\n(8)\\nx −q1(y)\\n=\\n[f\\nx1 · · · exb]\\n(9)\\nthen the distance is rewritten as:\\n∥x −q(y)∥2\\n2 = ∥f\\nx1 −q1( ey1)∥2\\n2 + ... + ∥exb −qb( eyb)∥2\\n2.\\n(10)\\nEach quantizer q1, ..., qb has 256 reproduction values, so\\nwhen x and q1(y) are known all distances can be precom-\\nputed and stored in tables T1, ..., Tb each of size 256 [25].\\nComputing the sum (10) consists of b look-ups and addi-\\ntions. Comparing the cost to compute n distances:\\n• Explicit computation: n × d mutiply-adds;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='• Explicit computation: n × d mutiply-adds;\\n• With lookup tables: 256 × d multiply-adds and n × b\\nlookup-adds.\\nThis is the key to the eﬃciency of the product quantizer.\\nIn our GPU implementation, b is any multiple of 4 up to\\n64. The codes are stored as sequential groups of b bytes per\\nvector within lists.\\nIVFADC lookup tables.\\nWhen scanning over the ele-\\nments of the inverted list IL (where by deﬁnition q1(y) is\\nconstant), the look-up table method can be applied, as the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 5}, page_content='query x and q1(y) are known.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='Moreover, the computation of the tables T1 . . . Tb is fur-\\nther optimized [5]. The expression of ∥x−q(y)∥2\\n2 in Equation\\n(7) can be decomposed as:\\n∥q2(...)∥2\\n2 + 2⟨q1(y), q2(...)⟩\\n|\\n{z\\n}\\nterm 1\\n+ ∥x −q1(y)∥2\\n2\\n|\\n{z\\n}\\nterm 2\\n−2 ⟨x, q2(...)⟩\\n|\\n{z\\n}\\nterm 3\\n.\\n(11)\\nThe objective is to minimize inner loop computations.\\nThe computations we can do in advance and store in lookup\\ntables are as follows:\\n• Term 1 is independent of the query. It can be precom-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='puted from the quantizers, and stored in a table T of\\nsize |C1| × 256 × b;\\n• Term 2 is the distance to q1’s reproduction value. It is\\nthus a by-product of the ﬁrst-level quantizer q1;\\n• Term 3 can be computed independently of the inverted\\nlist. Its computation costs d × 256 multiply-adds.\\nThis decomposition is used to produce the lookup tables\\nT1 . . . Tb used during the scan of the inverted list.\\nFor a\\nsingle query, computing the τ × b tables from scratch costs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='τ × d × 256 multiply-adds, while this decomposition costs\\n256×d multiply-adds and τ×b×256 additions. On the GPU,\\nthe memory usage of T can be prohibitive, so we enable the\\ndecomposition only when memory is a not a concern.\\n5.3\\nGPU implementation\\nAlgorithm 4 summarizes the process as one would im-\\nplement it on a CPU. The inverted lists are stored as two\\nseparate arrays, for PQ codes and associated IDs. IDs are\\nresolved only if k-selection determines k-nearest member-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='ship. This lookup yields a few sparse memory reads in a\\nlarge array, thus the IDs can optionally be stored on CPU\\nfor tiny performance cost.\\nList scanning. A kernel is responsible for scanning the τ\\nclosest inverted lists for each query, and calculating the per-\\nvector pair distances using the lookup tables Ti. The Ti are\\nstored in shared memory: up to nq×τ ×maxi |Ii|×b lookups\\nare required for a query set (trillions of accesses in practice),\\nand are random access.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='and are random access.\\nThis limits b to at most 48 (32-\\nbit ﬂoating point) or 96 (16-bit ﬂoating point) with current\\narchitectures. In case we do not use the decomposition of\\nEquation (11), the Ti are calculated by a separate kernel\\nbefore scanning.\\nMulti-pass kernels. Each nq × τ pairs of query against\\ninverted list can be processed independently.\\nAt one ex-\\ntreme, a block is dedicated to each of these, resulting in up\\nto nq × τ × maxi |Ii| partial results being written back to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='global memory, which is then k-selected to nq × k ﬁnal re-\\nsults. This yields high parallelism but can exceed available\\nGPU global memory; as with exact search, we choose a tile\\nsize tq ≤nq to reduce memory consumption, bounding its\\ncomplexity by O(2tqτ maxi |Ii|) with multi-streaming.\\nA single warp could be dedicated to k-selection of each\\ntq set of lists, which could result in low parallelism.\\nWe\\nintroduce a two-pass k-selection, reducing tq × τ × maxi |Ii|'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='to tq × f × k partial results for some subdivision factor f.\\nThis is reduced again via k-selection to the ﬁnal tq×k results.\\nFused kernel. As with exact search, we experimented with\\na kernel that dedicates a single block to scanning all τ lists\\nfor a single query, with k-selection fused with distance com-\\nputation. This is possible as WarpSelect does not ﬁght for\\nthe shared memory resource which is severely limited. This\\nreduces global memory write-back, since almost all interme-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='diate results can be eliminated. However, unlike k-selection\\noverhead for exact computation, a signiﬁcant portion of the\\nruntime is the gather from the Ti in shared memory and lin-\\near scanning of the Ii from global memory; the write-back is\\nnot a dominant contributor. Timing for the fused kernel is\\nimproved by at most 15%, and for some problem sizes would\\nbe subject to lower parallelism and worse performance with-\\nout subsequent decomposition. Therefore, and for reasons'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='of implementation simplicity, we do not use this layout.\\nAlgorithm 4 IVFPQ batch search routine\\nfunction ivfpq-search([x1, ..., xnq], I1, ..., I|C1|)\\nfor i ←0 : nq do ▷batch quantization of Section 5.1\\nLi\\nIVF ←τ-argminc∈C1∥x −c∥2\\nend for\\nfor i ←0 : nq do\\nL ←[]\\n▷distance table\\nCompute term 3 (see Section 5.2)\\nfor L in Li\\nIVF do\\n▷τ loops\\nCompute distance tables T1, ..., Tb\\nfor j in IL do\\n▷distance estimation, Equation (10)\\nd ←∥xi −q(yj)∥2\\n2\\nAppend (d, L, j) to L\\nend for\\nend for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='2\\nAppend (d, L, j) to L\\nend for\\nend for\\nRi ←k-select smallest distances d from L\\nend for\\nreturn R\\nend function\\n5.4\\nMulti-GPU parallelism\\nModern servers can support several GPUs. We employ\\nthis capability for both compute power and memory.\\nReplication. If an index instance ﬁts in the memory of a\\nsingle GPU, it can be replicated across R diﬀerent GPUs. To\\nquery nq vectors, each replica handles a fraction nq/R of the\\nqueries, joining the results back together on a single GPU'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='or in CPU memory. Replication has near linear speedup,\\nexcept for a potential loss in eﬃciency for small nq.\\nSharding. If an index instance does not ﬁt in the memory\\nof a single GPU, an index can be sharded across S diﬀer-\\nent GPUs. For adding ℓvectors, each shard receives ℓ/S of\\nthe vectors, and for query, each shard handles the full query\\nset nq, joining the partial results (an additional round of k-\\nselection is still required) on a single GPU or in CPU mem-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 6}, page_content='ory. For a given index size ℓ, sharding will yield a speedup\\n(sharding has a query of nq against ℓ/S versus replication\\nwith a query of nq/R against ℓ), but is usually less than\\npure replication due to ﬁxed overhead and cost of subse-\\nquent k-selection.\\nReplication and sharding can be used together (S shards,\\neach with R replicas for S × R GPUs in total). Sharding or\\nreplication are both fairly trivial, and the same principle can\\nbe used to distribute an index across multiple machines.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='\\x01\\x01\\x02\\x03\\n\\x01\\x03\\n\\x01\\x03\\x01\\n\\x01\\x03\\x01\\x01\\n\\x01\\x03\\x01\\x04\\x05\\n\\x01\\x05\\x01\\x06\\x07\\n\\x01\\x03\\x07\\x08\\t\\x05\\n\\x01\\x07'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='\\x08\\x07\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x01\\x08\\x06\\t\\n\\x0b\\x0c\\x0c\\x0b\\r\\x01\\x0e\\x0f\\x10\\x11\\x12\\x13\\n\\x12\\x0c\\x14\\x10\\x15\\x0b\\x12\\x0f\\x16\\x01\\x17\\x18\\x12\\x19\\x10\\x18\\x15\\x01\\x1a\\x19\\x0c\\x12\\n\\x1b\\x11\\x1c\\x10\\x10\\x01\\x1a\\x0f\\x0e\\x0f\\x15\\x12\\n\\x1d\\x0b\\x0c\\x1e\\x1f\\x0f\\x0e\\x0f\\x15\\x12\\n \\x0f \\x19\\x0c\\r\\x01\\x17\\x0b\\x10\\x16!\\x18\\x16\\x12\\x13\\x01\\x0e\\x18 \\x18\\x12\\nFigure 3:\\nRuntimes for diﬀerent k-selection meth-\\nods, as a function of array length ℓ. Simultaneous\\narrays processed are nq = 10000. k = 100 for full lines,\\nk = 1000 for dashed lines.\\n6.\\nEXPERIMENTS & APPLICATIONS\\nThis section compares our GPU k-selection and nearest-\\nneighbor approach to existing libraries. Unless stated other-\\nwise, experiments are carried out on a 2×2.8GHz Intel Xeon'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='E5-2680v2 with 4 Maxwell Titan X GPUs on CUDA 8.0.\\n6.1\\nk-selection performance\\nWe compare against two other GPU small k-selection im-\\nplementations: the row-based Merge Queue with Buﬀered\\nSearch and Hierarchical Partition extracted from the fgknn\\nlibrary of Tang et al. [41] and Truncated Bitonic Sort (TBiS)\\nfrom Sismanis et al. [40]. Both were extracted from their re-\\nspective exact search libraries.\\nWe evaluate k-selection for k = 100 and 1000 of each row'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='from a row-major matrix nq × ℓof random 32-bit ﬂoating\\npoint values on a single Titan X. The batch size nq is ﬁxed\\nat 10000, and the array lengths ℓvary from 1000 to 128000.\\nInputs and outputs to the problem remain resident in GPU\\nmemory, with the output being of size nq × k, with corre-\\nsponding indices. Thus, the input problem sizes range from\\n40 MB (ℓ= 1000) to 5.12 GB (ℓ= 128k). TBiS requires large\\nauxiliary storage, and is limited to ℓ≤48000 in our tests.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='Figure 3 shows our relative performance against TBiS and\\nfgknn. It also includes the peak possible performance given\\nby the memory bandwidth limit of the Titan X. The rela-\\ntive performance of WarpSelect over fgknn increases for\\nlarger k; even TBiS starts to outperform fgknn for larger ℓ\\nat k = 1000. We look especially at the largest ℓ= 128000.\\nWarpSelect is 1.62× faster at k = 100, 2.01× at k = 1000.\\nPerformance against peak possible drops oﬀfor all imple-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='mentations at larger k. WarpSelect operates at 55% of\\npeak at k = 100 but only 16% of peak at k = 1000. This\\nis due to additional overhead assocated with bigger thread\\nqueues and merge/sort networks for large k.\\nDiﬀerences from fgknn. WarpSelect is inﬂuenced by\\nfgknn, but has several improvements: all state is maintained\\nin registers (no shared memory), no inter-warp synchroniza-\\ntion or buﬀering is used, no “hierarchical partition”, the k-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='selection can be fused into other kernels, and it uses odd-size\\nnetworks for eﬃcient merging and sorting.\\n# centroids\\nmethod\\n# GPUs\\n256\\n4096\\nBIDMach [11]\\n1\\n320 s\\n735 s\\nOurs\\n1\\n140 s\\n316 s\\nOurs\\n4\\n84 s\\n100 s\\nTable 1:\\nMNIST8m k-means performance\\n6.2\\nk-means clustering\\nThe exact search method with k = 1 can be used by a k-\\nmeans clustering method in the assignment stage, to assign\\nnq training vectors to |C1| centroids. Despite the fact that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='it does not use the IVFADC and k = 1 selection is trivial (a\\nparallel reduction is used for the k = 1 case, not WarpSe-\\nlect), k-means is a good benchmark for the clustering used\\nto train the quantizer q1.\\nWe apply the algorithm on MNIST8m images. The 8.1M\\nimages are graylevel digits in 28x28 pixels, linearized to vec-\\ntors of 784-d. We compare this k-means implementation to\\nthe GPU k-means of BIDMach [11], which was shown to be\\nmore eﬃcient than several distributed k-means implemen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='tations that require dozens of machines3. Both algorithms\\nwere run for 20 iterations. Table 1 shows that our imple-\\nmentation is more than 2× faster, although both are built\\nupon cuBLAS. Our implementation receives some beneﬁt\\nfrom the k-selection fusion into L2 distance computation.\\nFor multi-GPU execution via replicas, the speedup is close\\nto linear for large enough problems (3.16× for 4 GPUs with\\n4096 centroids). Note that this benchmark is somewhat un-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='realistic, as one would typically sub-sample the dataset ran-\\ndomly when so few centroids are requested.\\nLarge scale. We can also compare to [3], an approximate\\nCPU method that clusters 108 128-d vectors to 85k cen-\\ntroids. Their clustering method runs in 46 minutes, but re-\\nquires 56 minutes (at least) of pre-processing to encode the\\nvectors. Our method performs exact k-means on 4 GPUs in\\n52 minutes without any pre-processing.\\n6.3\\nExact nearest neighbor search'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='6.3\\nExact nearest neighbor search\\nWe consider a classical dataset used to evaluate nearest\\nneighbor search: Sift1M [25]. Its characteristic sizes are\\nℓ= 106, d = 128, nq = 104. Computing the partial distance\\nmatrix D′ costs nq × ℓ× d = 1.28 Tﬂop, which runs in less\\nthan one second on current GPUs. Figure 4 shows the cost\\nof the distance computations against the cost of our tiling\\nof the GEMM for the −2 ⟨xj, yi⟩term of Equation 2 and\\nthe peak possible k-selection performance on the distance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='matrix of size nq×ℓ, which additionally accounts for reading\\nthe tiled result matrix D′ at peak memory bandwidth.\\nIn addition to our method from Section 5, we include\\ntimes from the two GPU libraries evaluated for k-selection\\nperformance in Section 6.1. We make several observations:\\n• for k-selection, the naive algorithm that sorts the full\\nresult array for each query using thrust::sort_by_key\\nis more than 10× slower than the comparison methods;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 7}, page_content='• L2 distance and k-selection cost is dominant for all but\\nour method, which has 85 % of the peak possible\\nperformance, assuming GEMM usage and our tiling\\n3BIDMach numbers from https://github.com/BIDData/\\nBIDMach/wiki/Benchmarks#KMeans\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='\\x01\\x01\\n\\x01\\x01\\x02\\x03\\n\\x01\\x04\\n\\x01\\x04\\x02\\x03\\n\\x01\\x05\\n\\x01\\x05\\x02\\x03\\n\\x01\\x06\\n\\x01\\x06\\x02\\x03\\n\\x01\\x07\\n\\x01\\x04\\n\\x01\\x07\\n\\x01\\x04\\x08\\n\\x01\\x08\\x07\\n\\x01\\x05\\x03\\x08\\n\\x01\\x04\\x01\\x05\\x07\\n\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x01\\x08\\t\\n\\t\\x01'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='\\x05\\x0b\\x0c\\x01\\r\\x0e\\x0f\\x10\\x10\\x01\\x11\\x12\\x13\\x01\\x14\\x15\\x16\\x17\\x18\\x19\\n\\x1a\\x17\\x12\\t\\x01\\x1a\\x1b\\x13\\x13\\x15\\x1c\\x16\\x17\\x01\\t\\n\\x13\\x17\\x16\\x17\\x1d\\x14\\n\\x1b\\x1e\\x1f\\x01 \\x17\\x14!\\x1b\\x18\\n\\x14\\x1f\\x1e\"\\x1d\\x12\\x14\\x17\\x18\\x01\\x1c\\x15\\x14\\x1b\"\\x15\\x1d\\x01\\x13\\x1b\\x1f\\x14\\n#$\\t\"\"\\nFigure 4:\\nExact search k-NN time for the SIFT1M\\ndataset with varying k on 1 Titan X GPU.\\nof the partial distance matrix D′ on top of GEMM is\\nclose to optimal. The cuBLAS GEMM itself has low\\neﬃciency for small reduction sizes (d = 128);\\n• Our fused L2/k-selection kernel is important.\\nOur\\nsame exact algorithm without fusion (requiring an ad-\\nditional pass through D′) is at least 25% slower.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='ditional pass through D′) is at least 25% slower.\\nEﬃcient k-selection is even more important in situations\\nwhere approximate methods are used to compute distances,\\nbecause the relative cost of k-selection with respect to dis-\\ntance computation increases.\\n6.4\\nBillion-scale approximate search\\nThere are few studies on GPU-based approximate nearest-\\nneighbor search on large datasets (ℓ≫106). We report a\\nfew comparison points here on index search, using standard'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='datasets and evaluation protocol in this ﬁeld.\\nSIFT1M. For the sake of completeness, we ﬁrst compare\\nour GPU search speed on Sift1M with the implementation\\nof Wieschollek et al. [47]. They obtain a nearest neighbor re-\\ncall at 1 (fraction of queries where the true nearest neighbor\\nis in the top 1 result) of R@1 = 0.51, and R@100 = 0.86 in\\n0.02 ms per query on a Titan X. For the same time budget,\\nour implementation obtains R@1 = 0.80 and R@100 = 0.95.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='SIFT1B. We compare again with Wieschollek et al., on the\\nSift1B dataset [26] of 1 billion SIFT image features at nq =\\n104. We compare the search performance in terms of same\\nmemory usage for similar accuracy (more accurate methods\\nmay involve greater search time or memory usage). On a\\nsingle GPU, with m = 8 bytes per vector, R@10 = 0.376 in\\n17.7 µs per query vector, versus their reported R@10 = 0.35\\nin 150 µs per query vector.\\nThus, our implementation is\\nmore accurate at a speed 8.5× faster.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='more accurate at a speed 8.5× faster.\\nDEEP1B. We also experimented on the Deep1B dataset [6]\\nof ℓ=1 billion CNN representations for images at nq = 104.\\nThe paper that introduces the dataset reports CPU results\\n(1 thread): R@1 = 0.45 in 20 ms search time per vector. We\\nuse a PQ encoding of m = 20, with d = 80 via OPQ [17],\\nand |C1| = 218, which uses a comparable dataset storage as\\nthe original paper (20 GB). This requires multiple GPUs as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='it is too large for a single GPU’s global memory, so we con-\\nsider 4 GPUs with S = 2, R = 2. We obtain a R@1 = 0.4517\\nin 0.0133 ms per vector. While the hardware platforms are\\n\\x01\\x01\\n\\x01\\x02\\x01\\n\\x01\\x03\\x01\\n\\x01\\x04\\x01\\n\\x01\\x05\\x01\\n\\x01\\x06\\x01\\x01\\n\\x01\\x06\\x02\\x01\\n\\x01\\x01\\x07\\x06\\n\\x01\\x01\\x07\\x02\\n\\x01\\x01\\x07\\x08\\n\\x01\\x01\\x07\\x03\\n\\x01\\x01\\x07\\t\\n\\x01\\x01\\x07\\x04\\n\\x01\\x01\\x07\\n\\x01\\x01\\x07\\x05\\n\\x01\\x01\\x07\\x0b\\n\\x01\\x02\\x03\\x03\\x01\\x04\\x05\\x06\\x07\\x08\\x01\\t\\n\\x0b\\x0c\\r\\x01\\x0e\\x0b\\x0f\\x10\\x01\\x11\\x0f\\x0b\\x12\\x13\\n\\x06\\x01\\x0c\\r\\x0e\\x0f\\x10\\x11\\x12\\x10\\x13\\x0f\\r\\x14\\x0e\\x01\\x15\\x0f\\x01\\x06\\x01\\n\\x01\\x02\\x03\\x03\\x04\\x05\\x05\\x06\\n\\x03\\x01\\x16\\r\\x0f\\x15\\x0e\\x01\\x17\\x18\\x01\\x19\\x1a\\x04\\x03\\x1b\\x01\\x1c\\x1a\\x06\\x1b\\x01\\x1d\\x1a\\x03\\n\\x03\\x01\\x16\\r\\x0f\\x15\\x0e\\x01\\x17\\x18\\x01\\x19\\x1a\\x08\\x02\\x1b\\x01\\x1c\\x1a\\x06\\x1b\\x01\\x1d\\x1a\\x03\\n\\x03\\x01\\x16\\r\\x0f\\x15\\x0e\\x01\\x17\\x18\\x01\\x19\\x1a\\x06\\x04\\x1b\\x01\\x1c\\x1a\\x06\\x1b\\x01\\x1d\\x1a\\x03\\n\\x01\\x01\\n\\x01\\x02\\n\\x01\\x03\\n\\x01\\x04\\x05\\n\\x01\\x04\\x06\\n\\x01\\x05\\x01\\n\\x01\\x05\\x02\\n\\x01\\x01\\x07\\x04\\n\\x01\\x01\\x07\\x05\\n\\x01\\x01\\x07\\x08\\n\\x01\\x01\\x07\\x02\\n\\x01\\x01\\x07\\t\\n\\x01\\x01\\x07\\x06\\n\\x01\\x01\\x07\\n\\x01\\x01\\x07\\x03\\n\\x01\\x01\\x07\\x0b\\n\\x01\\x02\\x03\\x03\\x01\\x04\\x05\\x06\\x07\\x08\\x01\\t\\n\\x0b\\x0c\\r\\x01\\x0e\\x0b\\x0f\\x10\\x01\\x11\\x08\\x12\\n\\x05\\x13\\x14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='\\x01\\x01\\x07\\x06\\n\\x01\\x01\\x07\\n\\x01\\x01\\x07\\x03\\n\\x01\\x01\\x07\\x0b\\n\\x01\\x02\\x03\\x03\\x01\\x04\\x05\\x06\\x07\\x08\\x01\\t\\n\\x0b\\x0c\\r\\x01\\x0e\\x0b\\x0f\\x10\\x01\\x11\\x08\\x12\\n\\x05\\x13\\x14\\n\\x04\\x01\\x0c\\r\\x0e\\x0f\\x10\\x11\\x12\\x10\\x13\\x0f\\r\\x14\\x0e\\x01\\x15\\x0f\\x01\\x04\\x01\\n\\x01\\x02\\x02\\x03\\x04\\x05\\n\\x02\\x01\\x16\\r\\x0f\\x15\\x0e\\x01\\x17\\x18\\x01\\x19\\x1a\\x02\\x01\\x1b\\x01\\x1c\\x1a\\x02\\x1b\\x01\\x1d\\x1a\\x04\\n\\x02\\x01\\x16\\r\\x0f\\x15\\x0e\\x01\\x17\\x18\\x01\\x19\\x1a\\x05\\x01\\x1b\\x01\\x1c\\x1a\\x05\\x1b\\x01\\x1d\\x1a\\x05\\n\\x03\\x01\\x1e\\x02\\x01\\x18\\x01\\x19\\x1a\\x02\\x01\\x1b\\x01\\x1c\\x1a\\x02\\x1b\\x01\\x1d\\x1a\\x05\\n\\x03\\x01\\x1e\\x02\\x01\\x18\\x01\\x19\\x1a\\x05\\x01\\x1b\\x01\\x1c\\x1a\\x05\\x1b\\x01\\x1d\\x1a\\x02\\nFigure 5:\\nSpeed/accuracy trade-oﬀof brute-force\\n10-NN graph construction for the YFCC100M and\\nDEEP1B datasets.\\ndiﬀerent, it shows that making searches on GPUs is a game-\\nchanger in terms of speed achievable on a single machine.\\n6.5\\nThe k-NN graph\\nAn example usage of our similarity search method is to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='construct a k-nearest neighbor graph of a dataset via brute\\nforce (all vectors queried against the entire index).\\nExperimental setup. We evaluate the trade-oﬀbetween\\nspeed, precision and memory on two datasets: 95 million\\nimages from the Yfcc100M dataset [42] and Deep1B. For\\nYfcc100M, we compute CNN descriptors as the one-before-\\nlast layer of a ResNet [23], reduced to d = 128 with PCA.\\nThe evaluation measures the trade-oﬀbetween:\\n• Speed: How much time it takes to build the IVFADC'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='index from scratch and construct the whole k-NN graph\\n(k = 10) by searching nearest neighbors for all vectors\\nin the dataset. Thus, this is an end-to-end test that\\nincludes indexing as well as search time;\\n• Quality: We sample 10,000 images for which we com-\\npute the exact nearest neighbors. Our accuracy mea-\\nsure is the fraction of 10 found nearest neighbors that\\nare within the ground-truth 10 nearest neighbors.\\nFor Yfcc100M, we use a coarse quantizer (216 centroids),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 8}, page_content='and consider m = 16, 32 and 64 byte PQ encodings for each\\nvector. For Deep1B, we pre-process the vectors to d = 120\\nvia OPQ, use |C1| = 218 and consider m = 20, 40. For a\\ngiven encoding, we vary τ from 1 to 256, to obtain trade-\\noﬀs between eﬃciency and quality, as seen in Figure 5.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='Figure 6:\\nPath in the k-NN graph of 95 million images from YFCC100M. The ﬁrst and the last image are\\ngiven; the algorithm computes the smoothest path between them.\\nDiscussion. For Yfcc100M we used S = 1, R = 4. An\\naccuracy of more than 0.8 is obtained in 35 minutes. For\\nDeep1B, a lower-quality graph can be built in 6 hours,\\nwith higher quality in about half a day.\\nWe also experi-\\nmented with more GPUs by doubling the replica set, us-\\ning 8 Maxwell M40s (the M40 is roughly equivalent in per-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='formance to the Titan X). Performance is improved sub-\\nlinearly (∼1.6× for m = 20, ∼1.7× for m = 40).\\nFor comparison, the largest k-NN graph construction we\\nare aware of used a dataset comprising 36.5 million 384-\\nd vectors, which took a cluster of 128 CPU servers 108.7\\nhours of compute [45], using NN-Descent [15]. Note that\\nNN-Descent could also build or reﬁne the k-NN graph for\\nthe datasets we consider, but it has a large memory over-\\nhead over the graph storage, which is already 80 GB for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='Deep1B. Moreover it requires random access across all vec-\\ntors (384 GB for Deep1B).\\nThe largest GPU k-NN graph construction we found is a\\nbrute-force construction using exact search with GEMM, of\\na dataset of 20 million 15,000-d vectors, which took a cluster\\nof 32 Tesla C2050 GPUs 10 days [14]. Assuming computa-\\ntion scales with GEMM cost for the distance matrix, this\\napproach for Deep1B would take an impractical 200 days\\nof computation time on their cluster.\\n6.6\\nUsing the k-NN graph'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='6.6\\nUsing the k-NN graph\\nWhen a k-NN graph has been constructed for an image\\ndataset, we can ﬁnd paths in the graph between any two\\nimages, provided there is a single connected component (this\\nis the case). For example, we can search the shortest path\\nbetween two images of ﬂowers, by propagating neighbors\\nfrom a starting image to a destination image. Denoting by\\nS and D the source and destination images, and dij the\\ndistance between nodes, we search the path P = {p1, ..., pn}'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='with p1 = S and pn = D such that\\nmin\\nP\\nmax\\ni=1..n dpipi+1,\\n(12)\\ni.e., we want to favor smooth transitions. An example re-\\nsult is shown in Figure 6 from Yfcc100M4. It was ob-\\ntained after 20 seconds of propagation in a k-NN graph with\\nk = 15 neighbors. Since there are many ﬂower images in the\\ndataset, the transitions are smooth.\\n4The mapping from vectors to images is not available for\\nDeep1B\\n7.\\nCONCLUSION\\nThe arithmetic throughput and memory bandwidth of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='The arithmetic throughput and memory bandwidth of\\nGPUs are well into the teraﬂops and hundreds of gigabytes\\nper second.\\nHowever, implementing algorithms that ap-\\nproach these performance levels is complex and counter-\\nintuitive. In this paper, we presented the algorithmic struc-\\nture of similarity search methods that achieves near-optimal\\nperformance on GPUs.\\nThis work enables applications that needed complex ap-\\nproximate algorithms before. For example, the approaches'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='presented here make it possible to do exact k-means cluster-\\ning or to compute the k-NN graph with simple brute-force\\napproaches in less time than a CPU (or a cluster of them)\\nwould take to do this approximately.\\nGPU hardware is now very common on scientiﬁc work-\\nstations, due to their popularity for machine learning algo-\\nrithms. We believe that our work further demonstrates their\\ninterest for database applications. Along with this work, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='are publishing a carefully engineered implementation of this\\npaper’s algorithms, so that these GPUs can now also be used\\nfor eﬃcient similarity search.\\n8.\\nREFERENCES\\n[1] T. Alabi, J. D. Blanchard, B. Gordon, and R. Steinbach.\\nFast k-selection algorithms for graphics processing units.\\nACM Journal of Experimental Algorithmics,\\n17:4.2:4.1–4.2:4.29, October 2012.\\n[2] F. Andr´e, A.-M. Kermarrec, and N. L. Scouarnec. Cache\\nlocality is not enough: High-performance nearest neighbor'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='search with product quantization fast scan. In Proc.\\nInternational Conference on Very Large DataBases, pages\\n288–299, 2015.\\n[3] Y. Avrithis, Y. Kalantidis, E. Anagnostopoulos, and I. Z.\\nEmiris. Web-scale image clustering revisited. In Proc.\\nInternational Conference on Computer Vision, pages\\n1502–1510, 2015.\\n[4] A. Babenko and V. Lempitsky. The inverted multi-index.\\nIn Proc. IEEE Conference on Computer Vision and\\nPattern Recognition, pages 3069–3076, June 2012.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='Pattern Recognition, pages 3069–3076, June 2012.\\n[5] A. Babenko and V. Lempitsky. Improving bilayer product\\nquantization for billion-scale approximate nearest neighbors\\nin high dimensions. arXiv preprint arXiv:1404.1831, 2014.\\n[6] A. Babenko and V. Lempitsky. Eﬃcient indexing of\\nbillion-scale datasets of deep descriptors. In Proc. IEEE\\nConference on Computer Vision and Pattern Recognition,\\npages 2055–2063, June 2016.\\n[7] R. Barrientos, J. G´omez, C. Tenllado, M. Prieto, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 9}, page_content='M. Marin. knn query processing in metric spaces using\\nGPUs. In International European Conference on Parallel\\nand Distributed Computing, volume 6852 of Lecture Notes\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='in Computer Science, pages 380–392, Bordeaux, France,\\nSeptember 2011. Springer.\\n[8] K. E. Batcher. Sorting networks and their applications. In\\nProc. Spring Joint Computer Conference, AFIPS ’68\\n(Spring), pages 307–314, New York, NY, USA, 1968. ACM.\\n[9] P. Boncz, W. Lehner, and T. Neumann. Special issue:\\nModern hardware. The VLDB Journal, 25(5):623–624,\\n2016.\\n[10] J. Canny, D. L. W. Hall, and D. Klein. A multi-teraﬂop\\nconstituency parser using GPUs. In Proc. Empirical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='Methods on Natural Language Processing, pages 1898–1907.\\nACL, 2013.\\n[11] J. Canny and H. Zhao. Bidmach: Large-scale learning with\\nzero memory allocation. In BigLearn workshop, NIPS,\\n2013.\\n[12] B. Catanzaro, A. Keller, and M. Garland. A decomposition\\nfor in-place matrix transposition. In Proc. ACM\\nSymposium on Principles and Practice of Parallel\\nProgramming, PPoPP ’14, pages 193–206, 2014.\\n[13] J. Chhugani, A. D. Nguyen, V. W. Lee, W. Macy,\\nM. Hagog, Y.-K. Chen, A. Baransi, S. Kumar, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='M. Hagog, Y.-K. Chen, A. Baransi, S. Kumar, and\\nP. Dubey. Eﬃcient implementation of sorting on multi-core\\nsimd cpu architecture. Proc. VLDB Endow.,\\n1(2):1313–1324, August 2008.\\n[14] A. Dashti. Eﬃcient computation of k-nearest neighbor\\ngraphs for large high-dimensional data sets on gpu clusters.\\nMaster’s thesis, University of Wisconsin Milwaukee, August\\n2013.\\n[15] W. Dong, M. Charikar, and K. Li. Eﬃcient k-nearest\\nneighbor graph construction for generic similarity measures.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='In WWW: Proceeding of the International Conference on\\nWorld Wide Web, pages 577–586, March 2011.\\n[16] M. Douze, H. J´egou, and F. Perronnin. Polysemous codes.\\nIn Proc. European Conference on Computer Vision, pages\\n785–801. Springer, October 2016.\\n[17] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product\\nquantization. IEEE Trans. PAMI, 36(4):744–755, 2014.\\n[18] Y. Gong and S. Lazebnik. Iterative quantization: A\\nprocrustean approach to learning binary codes. In Proc.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 817–824, June 2011.\\n[19] Y. Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale\\norderless pooling of deep convolutional activation features.\\nIn Proc. European Conference on Computer Vision, pages\\n392–407, 2014.\\n[20] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep\\nimage retrieval: Learning global representations for image\\nsearch. In Proc. European Conference on Computer Vision,\\npages 241–257, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='pages 241–257, 2016.\\n[21] S. Han, H. Mao, and W. J. Dally. Deep compression:\\nCompressing deep neural networks with pruning, trained\\nquantization and huﬀman coding. arXiv preprint\\narXiv:1510.00149, 2015.\\n[22] K. He, F. Wen, and J. Sun. K-means hashing: An\\naﬃnity-preserving quantization method for learning binary\\ncompact codes. In Proc. IEEE Conference on Computer\\nVision and Pattern Recognition, pages 2938–2945, June\\n2013.\\n[23] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='learning for image recognition. In Proc. IEEE Conference\\non Computer Vision and Pattern Recognition, pages\\n770–778, June 2016.\\n[24] X. He, D. Agarwal, and S. K. Prasad. Design and\\nimplementation of a parallel priority queue on many-core\\narchitectures. IEEE International Conference on High\\nPerformance Computing, pages 1–10, 2012.\\n[25] H. J´egou, M. Douze, and C. Schmid. Product quantization\\nfor nearest neighbor search. IEEE Trans. PAMI,\\n33(1):117–128, January 2011.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='33(1):117–128, January 2011.\\n[26] H. J´egou, R. Tavenard, M. Douze, and L. Amsaleg.\\nSearching in one billion vectors: re-rank with source\\ncoding. In International Conference on Acoustics, Speech,\\nand Signal Processing, pages 861–864, May 2011.\\n[27] Y. Kalantidis and Y. Avrithis. Locally optimized product\\nquantization for approximate nearest neighbor search. In\\nProc. IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 2329–2336, June 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='Recognition, pages 2329–2336, June 2014.\\n[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\\nclassiﬁcation with deep convolutional neural networks. In\\nAdvances in Neural Information Processing Systems, pages\\n1097–1105, 2012.\\n[29] F. T. Leighton. Introduction to Parallel Algorithms and\\nArchitectures: Array, Trees, Hypercubes. Morgan\\nKaufmann Publishers Inc., San Francisco, CA, USA, 1992.\\n[30] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='NVIDIA Tesla: a uniﬁed graphics and computing\\narchitecture. IEEE Micro, 28(2):39–55, March 2008.\\n[31] W. Liu and B. Vinter. Ad-heap: An eﬃcient heap data\\nstructure for asymmetric multicore processors. In Proc. of\\nWorkshop on General Purpose Processing Using GPUs,\\npages 54:54–54:63. ACM, 2014.\\n[32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\\nJ. Dean. Distributed representations of words and phrases\\nand their compositionality. In Advances in Neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='and their compositionality. In Advances in Neural\\nInformation Processing Systems, pages 3111–3119, 2013.\\n[33] L. Monroe, J. Wendelberger, and S. Michalak. Randomized\\nselection on the GPU. In Proc. ACM Symposium on High\\nPerformance Graphics, pages 89–98, 2011.\\n[34] M. Norouzi and D. Fleet. Cartesian k-means. In Proc.\\nIEEE Conference on Computer Vision and Pattern\\nRecognition, pages 3017–3024, June 2013.\\n[35] M. Norouzi, A. Punjani, and D. J. Fleet. Fast search in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='Hamming space with multi-index hashing. In Proc. IEEE\\nConference on Computer Vision and Pattern Recognition,\\npages 3108–3115, 2012.\\n[36] J. Pan and D. Manocha. Fast GPU-based locality sensitive\\nhashing for k-nearest neighbor computation. In Proc. ACM\\nInternational Conference on Advances in Geographic\\nInformation Systems, pages 211–220, 2011.\\n[37] L. Paulev´e, H. J´egou, and L. Amsaleg. Locality sensitive\\nhashing: a comparison of hash function types and querying'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='mechanisms. Pattern recognition letters, 31(11):1348–1358,\\nAugust 2010.\\n[38] O. Shamir. Fundamental limits of online and distributed\\nalgorithms for statistical learning and estimation. In\\nAdvances in Neural Information Processing Systems, pages\\n163–171, 2014.\\n[39] A. Sharif Razavian, H. Azizpour, J. Sullivan, and\\nS. Carlsson. CNN features oﬀ-the-shelf: an astounding\\nbaseline for recognition. In CVPR workshops, pages\\n512–519, 2014.\\n[40] N. Sismanis, N. Pitsianis, and X. Sun. Parallel search of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='k-nearest neighbors with synchronous operations. In IEEE\\nHigh Performance Extreme Computing Conference, pages\\n1–6, 2012.\\n[41] X. Tang, Z. Huang, D. M. Eyers, S. Mills, and M. Guo.\\nEﬃcient selection algorithm for fast k-nn search on GPUs.\\nIn IEEE International Parallel & Distributed Processing\\nSymposium, pages 397–406, 2015.\\n[42] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde,\\nK. Ni, D. Poland, D. Borth, and L.-J. Li. YFCC100M: The\\nnew data in multimedia research. Communications of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='ACM, 59(2):64–73, January 2016.\\n[43] V. Volkov and J. W. Demmel. Benchmarking GPUs to tune\\ndense linear algebra. In Proc. ACM/IEEE Conference on\\nSupercomputing, pages 31:1–31:11, 2008.\\n[44] A. Wakatani and A. Murakami. GPGPU implementation of\\nnearest neighbor search with product quantization. In\\nIEEE International Symposium on Parallel and Distributed\\nProcessing with Applications, pages 248–253, 2014.\\n[45] T. Warashina, K. Aoyama, H. Sawada, and T. Hattori.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 10}, page_content='Eﬃcient k-nearest neighbor graph construction using\\nmapreduce for large-scale data sets. IEICE Transactions,\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='97-D(12):3142–3154, 2014.\\n[46] R. Weber, H.-J. Schek, and S. Blott. A quantitative\\nanalysis and performance study for similarity-search\\nmethods in high-dimensional spaces. In Proc. International\\nConference on Very Large DataBases, pages 194–205, 1998.\\n[47] P. Wieschollek, O. Wang, A. Sorkine-Hornung, and\\nH. P. A. Lensch. Eﬃcient large-scale approximate nearest\\nneighbor search on the GPU. In Proc. IEEE Conference on\\nComputer Vision and Pattern Recognition, pages\\n2027–2035, June 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='2027–2035, June 2016.\\n[48] S. Williams, A. Waterman, and D. Patterson. Rooﬂine: An\\ninsightful visual performance model for multicore\\narchitectures. Communications of the ACM, 52(4):65–76,\\nApril 2009.\\nAppendix: Complexity analysis of WarpSelect\\nWe derive the average number of times updates are triggered\\nin WarpSelect, for use in Section 4.3.\\nLet the input to k-selection be a sequence {a1, a2, ..., aℓ}\\n(1-based indexing), a randomly chosen permutation of a set\\nof distinct elements.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='of distinct elements.\\nElements are read sequentially in c\\ngroups of size w (the warp; in our case, w = 32); assume ℓ\\nis a multiple of w, so c = ℓ/w. Recall that t is the thread\\nqueue length.\\nWe call elements prior to or at position n\\nin the min-k seen so far the successive min-k (at n). The\\nlikelihood that an is in the successive min-k at n is:\\nα(n, k) :=\\n(\\n1\\nif n ≤k\\nk/n\\nif n > k\\n(13)\\nas each an, n > k has a k/n chance as all permutations are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='equally likely, and all elements in the ﬁrst k qualify.\\nCounting the insertion sorts. In a given lane, an inser-\\ntion sort is triggered if the incoming value is in the successive\\nmin-k + t values, but the lane has “seen” only wc0 + (c−c0)\\nvalues, where c0 is the previous won warp ballot. The prob-\\nability of this happening is:\\nα(wc0 + (c −c0), k + t) ≈k + t\\nwc\\nfor c > k.\\n(14)\\nThe approximation considers that the thread queue has seen\\nall the wc values, not just those assigned to its lane. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='probability of any lane triggering an insertion sort is then:\\n1 −\\n\\x12\\n1 −k + t\\nwc\\n\\x13w\\n≈k + t\\nc\\n.\\n(15)\\nHere the approximation is a ﬁrst-order Taylor expansion.\\nSumming up the probabilities over c gives an expected num-\\nber of insertions of N2 ≈(k + t) log(c) = O(k log(ℓ/w)).\\nCounting full sorts. We seek N3 = π(ℓ, k, t, w), the ex-\\npected number of full sorts required for WarpSelect.\\nSingle lane. For now, we assume w = 1, so c = ℓ. Let\\nγ(ℓ, m, k) be the probability that in an sequence {a1, ..., aℓ},'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='exactly m of the elements as encountered by a sequential\\nscanner (w = 1) are in the successive min-k. Given m, there\\nare\\n\\x00 ℓ\\nm\\n\\x01\\nplaces where these successive min-k elements can\\noccur. It is given by a recurrence relation:\\nγ(ℓ, m, k) :=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n1\\nℓ= 0 and m = 0\\n0\\nℓ= 0 and m > 0\\n0\\nℓ> 0 and m = 0\\n(γ(ℓ−1, m −1, k) · α(ℓ, k)+\\nγ(ℓ−1, m, k) · (1 −α(ℓ, k)))\\notherwise.\\n(16)\\nThe last case is the probability of: there is a ℓ−1 se-\\nquence with m −1 successive min-k elements preceding us,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='and the current element is in the successive min-k, or the\\ncurrent element is not in the successive min-k, m ones are\\nbefore us. We can then develop a recurrence relationship for\\nπ(ℓ, k, t, 1). Note that\\nδ(ℓ, b, k, t) :=\\nmin((bt+max(0,t−1)),ℓ)\\nX\\nm=bt\\nγ(ℓ, m, k)\\n(17)\\nfor b where 0 ≤bt ≤ℓis the fraction of all sequences of\\nlength ℓthat will force b sorts of data by winning the thread\\nqueue ballot, as there have to be bt to (bt + max(0, t −1))'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='elements in the successive min-k for these sorts to happen (as\\nthe min-k elements will overﬂow the thread queues). There\\nare at most ⌊ℓ/t⌋won ballots that can occur, as it takes t\\nseparate sequential current min-k seen elements to win the\\nballot.\\nπ(ℓ, k, t, 1) is thus the expectation of this over all\\npossible b:\\nπ(ℓ, k, t, 1) =\\n⌊ℓ/t⌋\\nX\\nb=1\\nb · δ(ℓ, b, k, t).\\n(18)\\nThis can be computed by dynamic programming. Analyti-\\ncally, note that for t = 1, k = 1, π(ℓ, 1, 1, 1) is the harmonic'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='number Hℓ= 1+ 1\\n2 + 1\\n3 +...+ 1\\nℓ, which converges to ln(ℓ)+γ\\n(the Euler-Mascheroni constant γ) as ℓ→∞.\\nFor t = 1, k > 1, ℓ> k, π(ℓ, k, 1, 1) = k + k(Hℓ−Hk)\\nor O(k log(ℓ)), as the ﬁrst k elements are in the successive\\nmin-k, and the expectation for the rest is\\nk\\nk+1 +\\nk\\nk+2 +...+ k\\nℓ.\\nFor t > 1, k > 1, ℓ> k, note that there are some number\\nD, k ≤D ≤ℓof successive min-k determinations D made\\nfor each possible {a1, ..., aℓ}. The number of won ballots for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='each case is by deﬁnition ⌊D/t⌋, as the thread queue must\\nﬁll up t times. Thus, π(ℓ, k, t, 1) = O(k log(ℓ)/t).\\nMultiple lanes.\\nThe w > 1 case is complicated by the\\nfact that there are joint probabilities to consider (if more\\nthan one of the w workers triggers a sort for a given group,\\nonly one sort takes place). However, the likelihood can be\\nbounded. Let π′(ℓ, k, t, w) be the expected won ballots as-\\nsuming no mutual interference between the w workers for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='winning ballots (i.e., we win b ballots if there are b ≤w\\nworkers that independently win a ballot at a single step),\\nbut with the shared min-k set after each sort from the joint\\nsequence. Assume that k ≥w. Then:\\nπ′(ℓ, k, 1, w) ≤w\\n \\x18 k\\nw\\n\\x19\\n+\\n⌈ℓ/w⌉−⌈k/w⌉\\nX\\ni=1\\nk\\nw(⌈k/w⌉+ i)\\n!\\n≤wπ(⌈ℓ/w⌉, k, 1, 1) = O(wk log(ℓ/w))\\n(19)\\nwhere the likelihood of the w workers seeing a successive\\nmin-k element has an upper bound of that of the ﬁrst worker\\nat each step. As before, the number of won ballots is scaled'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2017-03-01T01:27:02+00:00', 'source': '../data/pdf_files/vector_database.pdf', 'file_path': '../data/pdf_files/vector_database.pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2017-03-01T01:27:02+00:00', 'trapped': '', 'modDate': 'D:20170301012702Z', 'creationDate': 'D:20170301012702Z', 'page': 11}, page_content='by t, so π′(ℓ, k, t, w) = O(wk log(ℓ/w)/t). Mutual interfer-\\nence can only reduce the number of ballots, so we obtain the\\nsame upper bound for π(ℓ, k, t, w).\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 0}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research; ‡University College London; ⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 0}, page_content='in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 0}, page_content='memory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\\n(RAG) — models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 0}, page_content='pare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\\narchitectures. For language generation tasks, we ﬁnd that RAG models generate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 0}, page_content='more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1\\nIntroduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have down-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 0}, page_content='sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\\ntheir predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 0}, page_content='combine masked language models [8] with a differentiable retriever, have shown promising results,\\narXiv:2005.11401v4  [cs.CL]  12 Apr 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 1}, page_content='The Divine\\nComedy (x)\\nq\\nQuery\\nEncoder\\nq(x)\\nMIPS\\npθ\\nGenerator\\xa0pθ\\n(Parametric)\\nMargin-\\nalize\\nThis 14th century work\\nis divided into 3\\nsections: \"Inferno\",\\n\"Purgatorio\" &\\n\"Paradiso\"         (y)\\nEnd-to-End Backprop through q and\\xa0pθ\\nBarack Obama was\\nborn in Hawaii.(x)\\nFact Veriﬁcation: Fact Query\\nsupports (y)\\nQuestion Generation\\nFact Veriﬁcation:\\nLabel Generation\\nDocument\\nIndex\\nDefine \"middle ear\"(x)\\nQuestion Answering:\\nQuestion Query\\nThe middle ear includes\\nthe tympanic cavity and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 1}, page_content='The middle ear includes\\nthe tympanic cavity and\\nthe three ossicles.  (y)\\nQuestion Answering:\\nAnswer Generation\\nRetriever pη\\n(Non-Parametric)\\nz4\\nz3\\nz2\\nz1\\nd(z)\\nJeopardy Question\\nGeneration:\\nAnswer Query\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document\\nIndex) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use\\nMaximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 1}, page_content='treat z as a latent variable and marginalize over seq2seq predictions given different documents.\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\\nand non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\\na general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 1}, page_content='We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 1}, page_content='the input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\\ncan be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 1}, page_content='memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack-\\naugmented networks [25] and memory layers [30]. In contrast, we explore a setting where both\\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\\npresent without additional training.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 1}, page_content='present without additional training.\\nOur results highlight the beneﬁts of combining parametric and non-parametric memory with genera-\\ntion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform\\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 1}, page_content='recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\\nextractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\\ngeneration, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and\\ndiverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 1}, page_content='state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\\nthe non-parametric memory can be replaced to update the models’ knowledge as the world changes.1\\n2\\nMethods\\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them\\nas additional context when generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 1}, page_content='distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 2}, page_content='by θ that generates a current token based on a context of the previous i −1 tokens y1:i−1, the original\\ninput x and a retrieved passage z.\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\\nWe propose two models that marginalize over the latent documents in different ways to produce a\\ndistribution over generated text. In one approach, RAG-Sequence, the model uses the same document'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 2}, page_content='to predict each target token. The second approach, RAG-Token, can predict each target token based\\non a different document. In the following, we formally introduce both models and then describe the\\npη and pθ components, as well as the training and decoding procedure.\\n2.1\\nModels\\nRAG-Sequence Model\\nThe RAG-Sequence model uses the same retrieved document to generate\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 2}, page_content='is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the\\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence(y|x) ≈\\nX\\nz∈top-k(p(·|x))\\npη(z|x)pθ(y|x, z) =\\nX\\nz∈top-k(p(·|x))\\npη(z|x)\\nN\\nY\\ni\\npθ(yi|x, z, y1:i−1)\\nRAG-Token Model\\nIn the RAG-Token model we can draw a different latent document for each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 2}, page_content='target token and marginalize accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output token for each document,\\nbefore marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\\npRAG-Token(y|x) ≈\\nN\\nY\\ni\\nX\\nz∈top-k(p(·|x))\\npη(z|x)pθ(yi|x, z, y1:i−1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 2}, page_content='Y\\ni\\nX\\nz∈top-k(p(·|x))\\npη(z|x)pθ(yi|x, z, y1:i−1)\\nFinally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class\\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2\\nRetriever: DPR\\nThe retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\npη(z|x) ∝exp\\n\\x00d(z)⊤q(x)\\n\\x01\\nd(z) = BERTd(z), q(x) = BERTq(x)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 2}, page_content='\\x00d(z)⊤q(x)\\n\\x01\\nd(z) = BERTd(z), q(x) = BERTq(x)\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],\\nand q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating\\ntop-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner\\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 2}, page_content='a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\\nretriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and\\nNatural Questions [29]. We refer to the document index as the non-parametric memory.\\n2.3\\nGenerator: BART\\nThe generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use\\nBART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 2}, page_content='x with the retrieved content z when generating from BART, we simply concatenate them. BART was\\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\\nmodels [32]. We refer to the BART generator parameters θ as the parametric memory henceforth.\\n2.4\\nTraining\\nWe jointly train the retriever and generator components without any direct supervision on what'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 2}, page_content='document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj, yj), we\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 3}, page_content='minimize the negative marginal log-likelihood of each target, P\\nj −log p(yj|xj) using stochastic\\ngradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as\\nit requires the document index to be periodically updated as REALM does during pre-training [20].\\nWe do not ﬁnd this step necessary for strong performance, and keep the document encoder (and\\nindex) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.\\n2.5\\nDecoding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 3}, page_content='2.5\\nDecoding\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x).\\nRAG-Token\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with transition probability: p′\\nθ(yi|x, y1:i−1) = P\\nz∈top-k(p(·|x)) pη(zi|x)pθ(yi|x, zi, y1:i−1) To\\ndecode, we can plug p′\\nθ(yi|x, y1:i−1) into a standard beam decoder.\\nRAG-Sequence\\nFor RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 3}, page_content='token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\neach document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses\\nY , some of which may not have appeared in the beams of all documents. To estimate the probability\\nof an hypothesis y we run an additional forward pass for each document z for which y does not\\nappear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 3}, page_content='beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer\\noutput sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding,\\nwe can make a further approximation that pθ(y|x, zi) ≈0 where y was not generated during beam\\nsearch from x, zi. This avoids the need to run additional forward passes once the candidate set Y has\\nbeen generated. We refer to this decoding procedure as “Fast Decoding.”\\n3\\nExperiments'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 3}, page_content='3\\nExperiments\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\\nembedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 3}, page_content='Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top\\nk documents for each query. We consider k ∈{5, 10} for training and set k for test time using dev\\ndata. We now discuss experimental details for each task.\\n3.1\\nOpen-domain Question Answering\\nOpen-domain question answering (QA) is an important real-world application and common testbed\\nfor knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 3}, page_content='and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\\ndocuments, relying primarily on non-parametric knowledge. We also compare to “Closed-Book\\nQA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 3}, page_content='Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As\\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\\nmodel. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM)\\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n3.2\\nAbstractive Question Answering\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 3}, page_content='text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,\\nwe use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 4}, page_content='MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\\nanswered in a way that matches the reference answer without access to the gold passages, such as\\n“What is the weather in Volcano, CA?” so performance will be lower without using gold passages.\\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\\nRAG can rely on parametric knowledge to generate reasonable responses.\\n3.3\\nJeopardy Question Generation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 4}, page_content='3.3\\nJeopardy Question Generation\\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-\\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 4}, page_content='For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst\\ncountry to host this international sports competition twice.” As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 4}, page_content='this is a new task, we train a BART model for comparison. Following [67], we evaluate using the\\nSQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\\none for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 4}, page_content='sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow\\nbest practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two\\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\\noptions—quuestion A is better, question B is better, both are good, or neither is good.\\n3.4\\nFact Veriﬁcation\\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 4}, page_content='Wikipedia, or whether there is not enough information to decide. The task requires retrieving\\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\\nwhether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem\\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\\nexploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 4}, page_content='class labels (supports, refutes, or not enough info) to single output tokens and directly train with\\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\\nretrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and\\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 4}, page_content='two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n4\\nResults\\n4.1\\nOpen-domain Question Answering\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 4}, page_content='the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of\\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\\nwithout expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s\\nretriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 4}, page_content='encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\\na correct answer being generated, which is not possible with standard extractive approaches, leading\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 5}, page_content='Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See Appendix D for further details.\\nModel\\nNQ\\nTQA\\nWQ\\nCT\\nClosed\\nBook\\nT5-11B [52]\\n34.5\\n- /50.1\\n37.4\\n-\\nT5-11B+SSM[52]\\n36.6\\n- /60.5\\n44.7\\n-\\nOpen\\nBook\\nREALM [20]\\n40.4\\n- / -\\n40.7\\n46.8\\nDPR [26]\\n41.5\\n57.9/ -\\n41.1\\n50.6\\nRAG-Token\\n44.1\\n55.2/66.1\\n45.5\\n50.0\\nRAG-Seq.\\n44.5\\n56.8/68.0\\n45.2\\n52.2\\nTable 2: Generation and classiﬁcation Test Scores.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 5}, page_content='MS-MARCO SotA is [4], FEVER-3 is [68] and\\nFEVER-2 is [57] *Uses gold context/evidence.\\nBest model without gold access underlined.\\nModel\\nJeopardy\\nMSMARCO FVR3 FVR2\\nB-1 QB-1\\nR-L\\nB-1\\nLabel Acc.\\nSotA\\n-\\n-\\n49.8* 49.9*\\n76.8\\n92.2*\\nBART\\n15.1\\n19.7\\n38.2\\n41.6\\n64.0\\n81.1\\nRAG-Tok. 17.3\\n22.2\\n40.1\\n41.5\\n72.5\\n89.5\\nRAG-Seq. 14.7\\n21.4\\n40.8\\n44.2\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 5}, page_content='even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\\ncases for NQ, where an extractive model would score 0%.\\n4.2\\nAbstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciﬁc information required to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 5}, page_content='generate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see §4.5).\\n4.3\\nJeopardy Question Generation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 5}, page_content='4.3\\nJeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 5}, page_content='BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\\nthe task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more\\nspeciﬁc by a large margin. Table 3 shows typical generations from each model.\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\\nbest because it can generate responses that combine content from several documents. Figure 2 shows'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 5}, page_content='an example. When generating “Sun”, the posterior is high for document 2 which mentions “The\\nSun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is\\ngenerated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\\nThis observation suggests that the generator can complete the titles without depending on speciﬁc\\ndocuments. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 5}, page_content='ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\\nSun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun\\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\\nBART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A\\nwith \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 5}, page_content='how parametric and non-parametric memories work together—the non-parametric component helps\\nto guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\\n4.4\\nFact Veriﬁcation\\nTable 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of\\nstate-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and\\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}, page_content='Document 1: his works are considered classics of American\\nliterature ... His wartime experiences formed the basis for his novel\\n”A Farewell to Arms” (1929) ...\\nDocument 2: ... artists of the 1920s ”Lost Generation” expatriate\\ncommunity. His debut novel, ”The Sun Also Rises”, was published\\nin 1926.\\nBOS ”\\nThe\\nSun\\nAlso\\nR\\nises\\n”\\nis\\na\\nnovel\\nby\\nthis\\nauthorof\\n”\\nA\\nFare\\nwell\\nto\\nArms”\\nDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nDoc 5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}, page_content='Fare\\nwell\\nto\\nArms”\\nDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nDoc 5\\nFigure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hem-\\ningway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\\nwhen generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\\nTable 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}, page_content='responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\\nTask\\nInput\\nModel\\nGeneration\\nMS-\\nMARCO\\ndeﬁne middle\\near\\nBART\\n?The middle ear is the part of the ear between the middle ear and the nose.\\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\\nwhat currency\\nneeded in\\nscotland\\nBART\\nThe currency needed in Scotland is Pound sterling.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}, page_content='RAG-T Pound is the currency needed in Scotland.\\nRAG-S The currency needed in Scotland is the pound sterling.\\nJeopardy\\nQuestion\\nGener\\n-ation\\nWashington\\nBART\\n?This state has the largest number of counties in the U.S.\\nRAG-T It’s the only U.S. state named for a U.S. president\\nRAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park\\nThe Divine\\nComedy\\nBART\\n*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}, page_content='RAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\\nFor 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}, page_content='We also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved\\nby RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n4.5\\nAdditional Results\\nGeneration Diversity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}, page_content='4.5\\nAdditional Results\\nGeneration Diversity\\nSection 4.3 shows that RAG models are more factual and speciﬁc than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\\n[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\\ntotal ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\\nmore diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}, page_content='any diversity-promoting decoding.\\nRetrieval Ablations\\nA key feature of RAG is learning to retrieve relevant information for the task.\\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\\nWe compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}, page_content='RAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating\\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\nIndex hot-swapping\\nAn advantage of non-parametric memory models like RAG is that knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 6}, page_content='can be easily updated at test time. Parametric-only models like T5 or BART need further training to\\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 7}, page_content='Table 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality\\nSpeciﬁcity\\nBART better\\n7.1%\\n16.8%\\nRAG better\\n42.7%\\n37.4%\\nBoth good\\n11.7%\\n11.8%\\nBoth poor\\n17.7%\\n6.9%\\nNo majority\\n20.8%\\n20.1%\\nTable 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO\\nJeopardy QGen\\nGold\\n89.6%\\n90.0%\\nBART\\n70.7%\\n32.4%\\nRAG-Token\\n77.8%\\n46.8%\\nRAG-Seq.\\n83.5%\\n53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent.\\nModel\\nNQ\\nTQA\\nWQ\\nCT'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 7}, page_content='Model\\nNQ\\nTQA\\nWQ\\nCT\\nJeopardy-QGen\\nMSMarco\\nFVR-3\\nFVR-2\\nExact Match\\nB-1\\nQB-1\\nR-L\\nB-1\\nLabel Accuracy\\nRAG-Token-BM25\\n29.7\\n41.5\\n32.1\\n33.1\\n17.5\\n22.3\\n55.5\\n48.4\\n75.1\\n91.6\\nRAG-Sequence-BM25\\n31.8\\n44.1\\n36.6\\n33.8\\n11.1\\n19.5\\n56.5\\n46.9\\nRAG-Token-Frozen\\n37.8\\n50.1\\n37.1\\n51.1\\n16.7\\n21.7\\n55.9\\n49.4\\n72.9\\n89.4\\nRAG-Sequence-Frozen\\n41.2\\n52.1\\n41.8\\n52.6\\n11.8\\n19.6\\n56.7\\n47.3\\nRAG-Token\\n43.5\\n54.8\\n46.5\\n51.9\\n17.9\\n22.6\\n56.2\\n49.4\\n74.5\\n90.6\\nRAG-Sequence\\n44.0\\n55.8\\n44.9\\n53.4\\n15.3\\n21.5\\n57.2\\n47.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 7}, page_content='44.0\\n55.8\\n44.9\\n53.4\\n15.3\\n21.5\\n57.2\\n47.5\\nbetween these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)\\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 7}, page_content='This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.\\nEffect of Retrieving more documents\\nModels are trained with either 5 or 10 retrieved latent\\ndocuments, and we do not observe signiﬁcant differences in performance between them. We have the\\nﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 7}, page_content='Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n10\\n20\\n30\\n40\\n50\\nK Retrieved Docs\\n39\\n40\\n41\\n42\\n43\\n44\\nNQ Exact Match\\nRAG-Tok\\nRAG-Seq\\n10\\n20\\n30\\n40\\n50\\nK Retrieved Docs\\n40\\n50\\n60\\n70\\n80\\nNQ Answer Recall @ K\\nRAG-Tok\\nRAG-Seq\\nFixed DPR\\nBM25\\n10\\n20\\n30\\n40\\n50\\nK Retrieved Docs\\n48\\n50\\n52\\n54'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 7}, page_content='BM25\\n10\\n20\\n30\\n40\\n50\\nK Retrieved Docs\\n48\\n50\\n52\\n54\\n56\\nBleu-1 / Rouge-L score\\nRAG-Tok R-L\\nRAG-Tok B-1\\nRAG-Seq R-L\\nRAG-Seq B-1\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n5\\nRelated Work\\nSingle-Task Retrieval\\nPrior work has shown that retrieval improves performance across a variety of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 7}, page_content='NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],\\nfact checking [56], fact completion [48], long-form question answering [12], Wikipedia article\\ngeneration [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our\\nwork uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 8}, page_content='General-Purpose Architectures for NLP\\nPrior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\nhas been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-\\nmarks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained\\nlanguage model could achieve strong performance across both discriminative and generative tasks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 8}, page_content='For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder\\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\\nand generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed\\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\\nLearned Retrieval\\nThere is signiﬁcant work on learning to retrieve documents in information'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 8}, page_content='retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some\\nwork optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,\\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\\nwork. These successes leverage different retrieval-based architectures and optimization techniques to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 8}, page_content='achieve strong performance on a single task, while we show that a single retrieval-based architecture\\ncan be ﬁne-tuned for strong performance on a variety of tasks.\\nMemory-based Architectures\\nOur document index can be seen as a large external memory for\\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 8}, page_content='work. Other work improves the ability of dialog models to generate factual text by attending over\\nfact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\\nmemory by editing the document index. This approach has also been used in knowledge-intensive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 8}, page_content='dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\\nrather than end-to-end learnt retrieval [9].\\nRetrieve-and-Edit approaches\\nOur method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\\nto provide a ﬁnal output. These approaches have proved successful in a number of domains including'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 8}, page_content='Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6\\nDiscussion'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 8}, page_content='6\\nDiscussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\\nfound that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual\\nand speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating\\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 8}, page_content='without requiring any retraining. In future work, it may be fruitful to investigate if the two components\\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\\nanother objective. Our work opens up new research directions on how parametric and non-parametric\\nmemories interact and how to most effectively combine them, showing promise in being applied to a\\nwide variety of NLP tasks.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 9}, page_content='Broader Impact\\nThis work offers several positive societal beneﬁts over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 9}, page_content='with a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 9}, page_content='to a lesser extent, including that it might be used to generate abuse, faked or misleading content in\\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 9}, page_content='Acknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\nReferences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 9}, page_content='program.\\nReferences\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\\nStoica, Saurabh Tiwary, and Tong Wang.\\nMS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n//arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 9}, page_content='International Conference of the Cross-Language Evaluation Forum for European Languages,\\npages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20.\\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 9}, page_content='Association for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\\ning&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,\\n2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 9}, page_content='Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\\nhttps://www.aclweb.org/anthology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 9}, page_content='pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 10}, page_content='[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\\nhension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.\\narXiv: 1710.10723.\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association for Computational Linguistics: Human'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 10}, page_content='Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\\nURL https://www.aclweb.org/anthology/N19-1423.\\n[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\\nLearning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 10}, page_content='[10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun\\nCho.\\nSearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\\narXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:\\n1704.05179.\\n[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 10}, page_content='Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\\nP18-1082.\\n[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 10}, page_content='Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\\nanthology/P19-1346.\\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\\nH1gx1CNKPH.\\n[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.\\nEntities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 10}, page_content='2020. URL https://arxiv.org/abs/2004.07202.\\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\\nConference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/\\nAAAI/AAAI18/paper/view/16710.\\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 10}, page_content='exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL\\nhttp://arxiv.org/abs/1705.08807.\\n[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation.\\nIn AAAI Conference on Artiﬁcial Intelligence, 2018.\\nURL https:\\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 10}, page_content='machine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, 32nd\\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018.\\n32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018\\nThrough 07-02-2018.\\n[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\\nediting prototypes. Transactions of the Association for Computational Linguistics, 6:437–450,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 10}, page_content='2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 11}, page_content='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:\\n//arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto,\\nKelvin Guu,\\nYonatan Oren,\\nand Percy S Liang.\\nA\\nretrieve-and-edit\\nframework\\nfor\\npredicting\\nstructured\\noutputs.\\nIn\\nS.\\nBengio,\\nH. Wallach,\\nH. Larochelle,\\nK. Grauman,\\nN. Cesa-Bianchi,\\nand R. Garnett,\\ned-\\nitors,\\nAdvances\\nin\\nNeural\\nInformation\\nProcessing\\nSystems\\n31,\\npages'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 11}, page_content='Neural\\nInformation\\nProcessing\\nSystems\\n31,\\npages\\n10052–\\n10062.\\nCurran\\nAssociates,\\nInc.,\\n2018.\\nURL\\nhttp://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 11}, page_content='tional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/\\nanthology/2020.acl-main.228.\\n[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 11}, page_content='55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\\ndoi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\\n[25] Armand Joulin and Tomas Mikolov.\\nInferring algorithmic patterns with stack-\\naugmented recurrent nets.\\nIn Proceedings of the 28th International Conference on\\nNeural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cam-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 11}, page_content='bridge,\\nMA, USA, 2015. MIT Press.\\nURL https://papers.nips.cc/paper/\\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\\narXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 11}, page_content='tion through memorization: Nearest neighbor language models. In International Conference on\\nLearning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\\nhttp://arxiv.org/abs/1412.6980.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 11}, page_content='http://arxiv.org/abs/1412.6980.\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,\\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov.\\nNatural Questions:\\na Benchmark for Ques-\\ntion Answering Research.\\nTransactions of the Association of Computational Lin-\\nguistics, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 11}, page_content='guistics, 2019.\\nURL https://tomkwiat.users.x20web.corp.google.com/papers/\\nnatural-questions/main-1455-kwiatkowski.pdf.\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and\\nHerve Jegou.\\nLarge memory layers with product keys.\\nIn H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\\nformation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 11}, page_content='//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.\\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 12}, page_content='for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\\nanthology/P19-1612.\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 12}, page_content='arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, pages 110–119, San Diego, California, June 2016. Association for Computational'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 12}, page_content='Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\\nN16-1014.\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\\nwith optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL\\nhttps://arxiv.org/abs/1909.03087.\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 12}, page_content='Meeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy,\\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\\nhttps://www.aclweb.org/anthology/P19-1291.\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\\nConference on Learning Representations, 2018. URL https://openreview.net/forum?\\nid=Hyg0vbWC-.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 12}, page_content='id=Hyg0vbWC-.\\n[37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search\\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv\\npreprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 12}, page_content='[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis\\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel.\\nHow decoding strategies affect the\\nveriﬁability of generated text.\\narXiv preprint arXiv:1911.03587, 2019.\\nURL https:\\n//arxiv.org/abs/1911.03587.\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 12}, page_content='precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\\ning background knowledge for building conversation systems. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus-\\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 12}, page_content='10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\\nProcessing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\\nanthology/D18-1429.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 12}, page_content='anthology/D18-1429.\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\\nTarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,\\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 13}, page_content='approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop\\nProceedings. CEUR-WS.org, 2016.\\nURL http://ceur-ws.org/Vol-1773/CoCoNIPS_\\n2016_paper9.pdf.\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\\narXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 13}, page_content='[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\\norg/anthology/N19-4009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 13}, page_content='org/anthology/N19-4009.\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\\nCho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\\n2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 13}, page_content='doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 13}, page_content='Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H.\\nMiller, and Sebastian Riedel. How context affects language models’ factual predictions. In\\nAutomated Knowledge Base Construction, 2020. URL https://openreview.net/forum?\\nid=025X0zPfn.\\n[49] Alec\\nRadford,\\nKarthik\\nNarasimhan,\\nTim\\nSalimans,\\nand\\nIlya\\nSutskever.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 13}, page_content='Narasimhan,\\nTim\\nSalimans,\\nand\\nIlya\\nSutskever.\\nIm-\\nproving\\nLanguage\\nUnderstanding\\nby\\nGenerative\\nPre-Training,\\n2018.\\nURL\\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\\nlanguage-unsupervised/language_understanding_paper.pdf.\\n[50] Alec\\nRadford,\\nJeff\\nWu,\\nRewon\\nChild,\\nDavid\\nLuan,\\nDario\\nAmodei,\\nand\\nIlya\\nSutskever.\\nLanguage models are unsupervised multitask learners,\\n2019.\\nURL\\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 13}, page_content='models_are_unsupervised_multitask_learners.pdf.\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 13}, page_content='2002.08910.\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\\nbeyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\\n1500000019. URL https://doi.org/10.1561/1500000019.\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec\\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\\nArXiv, abs/1908.09203, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 13}, page_content='ArXiv, abs/1908.09203, 2019.\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\\nin Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 14}, page_content='[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\\nhttps://www.aclweb.org/anthology/N18-1074.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 14}, page_content='https://www.aclweb.org/anthology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\\nbiases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv, abs/2004.14366,\\n2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 14}, page_content='S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 14}, page_content='AAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 14}, page_content='Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/\\nanthology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\\nF. d\\\\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 14}, page_content='Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://\\narxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain\\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\\nthe Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 14}, page_content='Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\\nAdvances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\\n2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 14}, page_content='ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.\\nnet/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\\nhttp://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 14}, page_content='generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd\\nInternational Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium,\\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\\nhttps://www.aclweb.org/anthology/W18-5713.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 15}, page_content='[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers:\\nState-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 15}, page_content='[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\\nods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, Novem-\\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\\nhttps://www.aclweb.org/anthology/D19-1253.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 15}, page_content='https://www.aclweb.org/anthology/D19-1253.\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019.\\nURL https://arxiv.org/abs/1909.03745.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 16}, page_content='Appendices for Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nA\\nImplementation Details\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\\nwe did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 16}, page_content='we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\nB\\nHuman Evaluation\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\\nand a worked example appear when clicking \"view tool guide\".'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 16}, page_content='Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\\nAnnotators were encouraged to research the topic using the internet, and were given detailed instruc-\\ntions and worked examples in a full instructions tab. We included some gold sentences in order to\\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 16}, page_content='their annotations were removed from the results.\\nC\\nTraining setup Details\\nWe train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision\\nﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\\ntraining and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search\\nwith FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring ∼100'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 16}, page_content='GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\\nTransformers [66]3, which achieves equivalent performance to the previous version but is a cleaner\\nand easier to use implementation. This version is also open-sourced. We also compress the document\\nindex using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to\\nrun experiments with RAG can be found at https://github.com/huggingface/transformers/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 16}, page_content='blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\\nat https://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 17}, page_content='D\\nFurther Details on Open-Domain QA\\nFor open-domain QA, multiple answer annotations are often available for a given question. These\\nanswer annotations are exploited by extractive models during training as typically all the answer\\nannotations are used to ﬁnd matches within documents when preparing training data. For RAG, we\\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 17}, page_content='the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA,\\nthere are often many valid answers to a given question, some of which are not suitable training targets,\\nsuch as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur\\nin top 1000 documents for the query.\\nCuratedTrec preprocessing\\nThe answers for CuratedTrec are given in the form of regular expres-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 17}, page_content='sions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].\\nTo overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for\\neach query, and use the answer that most frequently matches the regex pattern as the supervision\\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 17}, page_content='each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\nTriviaQA Evaluation setups\\nThe open-domain QA community customarily uses public develop-\\nment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\\ncompehension purposes. We report our results using the datasets splits used in DPR [26], which are\\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 17}, page_content='TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set\\ninstead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\\nappendix of [14]). We report results on both test sets to enable fair comparison to both approaches.\\nWe ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more\\nconventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 17}, page_content='simpler to answer from Wikipedia.\\nE\\nFurther Details on FEVER\\nFor FEVER classiﬁcation, we follow the practice from [32], and ﬁrst re-generate the claim, and\\nthen classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across\\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\\nﬁrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 17}, page_content='we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia\\nas evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to\\nus, directly tackling this task is not straightforward. We hope to address this in future work.\\nF\\nNull Document Probabilities\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 17}, page_content='to model cases where no useful information could be retrieved for a given input. Here, if k documents\\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\\ndocument, before marginalizing over k + 1 predictions. We explored modelling this null document\\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\\n(iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 17}, page_content='the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\\ndocuments for questions that are less likely to beneﬁt from retrieval, suggesting that null document\\nmechanisms may not be necessary for RAG.\\nG\\nParameters\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 17}, page_content='DPR, with 110M parameters each (although we do not train the document encoder ourselves) and\\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 18}, page_content='Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\nTask\\nTrain\\nDevelopment\\nTest\\nNatural Questions\\n79169\\n8758\\n3611\\nTriviaQA\\n78786\\n8838\\n11314\\nWebQuestions\\n3418\\n362\\n2033\\nCuratedTrec\\n635\\n134\\n635\\nJeopardy Question Generation\\n97392\\n13714\\n26849\\nMS-MARCO\\n153726\\n12468\\n101093*\\nFEVER-3-way\\n145450\\n10000\\n10000\\nFEVER-2-way\\n96966\\n6666\\n6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 18}, page_content='with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 18}, page_content='728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating\\npoint precision to manage memory and disk footprints.\\nH\\nRetrieval Collapse\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\\nretrieval component would “collapse” and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 18}, page_content='and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\nI\\nNumber of instances per dataset'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'file_path': '../data/pdf_files/rag_for_nlp.pdf.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 18}, page_content='I\\nNumber of instances per dataset\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\\n19'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 0}, page_content='Understanding the approximate nearest neighbor \\n(ANN) algorithm \\nBy Elastic Platform Team \\n17 April 2024 \\n \\nIf you grew up in a time before the internet made its debut, you’ll remember it wasn’t \\nalways easy to find new things to like. We discovered new bands when we happened to \\nhear them on the radio, we’d see a new TV show by accident because we forgot to \\nchange the channel, and we’d find a new favorite video game based almost entirely on \\nthe picture on the cover.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 0}, page_content='the picture on the cover.  \\nNowadays, things are very different. Spotify will point me to artists that match my \\ntastes, Netflix will highlight movies and TV shows it knows we’ll enjoy, and Xbox knows \\nwhat we’ll probably want to play next. These recommendation systems make it so much \\neasier for us to find the things we’re actually looking for, and they’re powered by nearest \\nneighbor (NN) algorithms. NN looks at the extensive sea of information it has available'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 0}, page_content='and identifies the closest thing to something you like, or something you’re searching for. \\nBut NN algorithms have an inherent flaw. If the amount of data they’re analyzing gets \\ntoo big, crawling through every option takes forever. This is a problem, especially as \\nthese data sources get bigger and bigger every year. This is where approximate nearest \\nneighbor (ANN) grabs the baton from NN and changes the game. \\nIn this document, we’ll cover the following key topics about ANN: \\n•'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 0}, page_content=\"• \\nANN definition \\n• \\nHow ANN works \\n• \\nWhen to use ANN search \\n• \\nANN importance in vector search \\n• \\nVarious types of ANN algorithms \\n \\nApproximate nearest neighbor explained \\nApproximate nearest neighbor (ANN) is an algorithm that finds a data point in a data \\nset that's very close to the given query point, but not necessarily the absolute closest \\none. An NN algorithm searches exhaustively through all the data to find the perfect\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 0}, page_content='match, whereas an ANN algorithm will settle for a match that’s close enough. \\nThis might sound like a worse solution, but it’s actually the key to nailing fast similarity \\nsearch. ANN uses intelligent shortcuts and data structures to efficiently navigate the \\nsearch space. So instead of taking up huge amounts of time and resources, it can'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 1}, page_content='identify data points with much less effort that are close enough to be useful in most \\npractical scenarios. \\nEssentially, it’s a trade-off. If you absolutely need to find the one best match, you can do \\nthat at the expense of speed and performance with NN. But if you can tolerate a tiny \\ndrop in accuracy, ANN is almost always a better solution. \\n \\nHow approximate nearest neighbor algorithms work \\nThe first part of how ANN works is dimensionality reduction, where the goal is to turn a'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 1}, page_content='higher-dimensional data set into a lower-dimensional one. The aim is to make the \\npredictive model task less complicated and more efficient than having to analyze all the \\ndata. \\nThese algorithms rest on the mathematical concept of metric spaces — where data \\npoints reside and distances between them are defined. These distances must adhere to \\nspecific rules (non-negativity, identity, symmetry, triangle inequality), and common'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 1}, page_content='functions like Euclidean distance or cosine similarity are used to calculate them.  \\nTo better understand this, imagine you’re on holiday searching for the villa you’ve \\nrented. Instead of checking every single building one-by-one (higher-dimensional), \\nyou’d use a map, which reduces the problem into two dimensions (lower-dimensional). \\n(This is a deliberately simplistic example. Dimensionality reduction is not the sole \\nmethod employed by ANN algorithms to improve efficiency.)'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 1}, page_content='ANN algorithms also leverage clever data structures called indexes to improve \\nefficiency. By pre-processing the data into these indexes, ANN can navigate the search \\nspace much quicker. Think of these as street signs, helping you find where you are on \\nthe map to reach your holiday villa quicker. \\n \\nWhen to use approximate nearest neighbor search \\nIn the fast-paced world of data science, efficiency reigns supreme. While finding the'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 1}, page_content='true closest neighbor (exact nearest neighbor search) holds value, it often comes at a \\ncomputational cost, as we’ve already talked about. This is where ANN search shines, \\noffering a compelling trade-off: lightning speed with high, but not absolute, accuracy. \\nBut when exactly should you choose ANN over other search methods? \\nExact nearest neighbor might be slow, but it’s the best option when accuracy is your \\npriority or you’re using small data sets. k-nearest neighbors (kNN) sits between NN'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 1}, page_content='and ANN by giving you faster results while maintaining high accuracy. But it can be hard \\nto get right when deciding the value of k, and it also struggles with high-dimensional \\ndata.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 2}, page_content='ANN’s speed and efficiency combined with its high (but not absolute) accuracy makes it \\nperfect in a number of situations: \\n• \\nLarge data sets: When dealing with millions or even billions of data points, the \\nexhaustive nature of exact NN becomes sluggish. ANN excels in navigating vast \\ndata landscapes, delivering results swiftly. \\n• \\nHigh-dimensional data: As dimensions climb, exact NN computations explode. \\nANNs dimensionality reduction techniques effectively shrink the search space'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 2}, page_content='and boost efficiency in complex data like images or text. \\n• \\nReal-time applications: Need results instantly? Recommendation systems, \\nfraud detection, and anomaly detection rely on real-time insights. ANN’s speed \\nmakes it ideal for these scenarios. \\n• \\nAcceptable approximation: If your application can tolerate slight inaccuracies \\nin results, ANN’s speed becomes invaluable. For example, in image search, \\nfinding visually similar images — instead of the absolute closest one — might be'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 2}, page_content='sufficient. \\n \\nImportance of ANN in vector search \\nVector search deals with data encoded as dense vectors, capturing complex \\nrelationships and embedded meanings. This makes it ideal for searching content like \\nimages, text, and user preferences, where traditional keyword-based search often falls \\nshort. But the curse of dimensionality applies here, too. Because as the number of \\ndimensions representing these vectors increases, traditional search methods struggle,'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 2}, page_content='becoming slow and inefficient. \\nANN solves this problem by switching the focus from finding an exact match to “close \\nenough” matches. This enables fast retrieval, where your vector search can find similar \\nvectors in massive data sets lightning fast. It also gives you baked-in scalability, so you \\ncan grow your data set as much as you want without sacrificing speed. \\nThese real-time responses combined with the improved relevance and efficiency often'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 2}, page_content='mean that ANN can play a critical role in unlocking the true potential of your vector \\nsearch. \\n \\nTypes of approximate nearest neighbor algorithms \\nWhile the concept of ANN offers a compelling speed advantage in search, this term \\nactually covers a diverse toolbox of algorithms. They all have their own strengths and \\ntrade-offs, and understanding these nuances is critical when choosing the right tool for \\nyour specific data and search needs.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 3}, page_content=\"KD-trees \\nKD-trees organize data points in a hierarchical tree structure, partitioning the space \\nbased on specific dimensions. This enables fast and efficient search in low-\\ndimensional spaces and Euclidean distance-based queries. \\nBut while KD-trees excel at finding nearest neighbors in low dimensions, they suffer \\nfrom the “curse of dimensionality.” This is where, as the number of dimensions \\nincreases, the space between points explodes. In these high dimensions, KD-trees'\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 3}, page_content='strategy of splitting based on single axes becomes ineffective. This makes the search \\nexamine most of the data, losing the efficiency advantage and approaching the \\nslowness of a simple linear scan through all points. \\n \\nLocality-sensitive hashing (LSH) \\nLSH is a powerful ANN technique that works by \"hashing\" data points into lower-\\ndimensional spaces in a way that cleverly preserves their similarity relationships. This'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 3}, page_content='clustering makes them easier to find, and it allows LSH to excel in searching massive, \\nhigh-dimensional data sets like images or text with both speed and scalability. And it \\ndoes all this while still returning \"close enough\" matches with good accuracy. But keep \\nin mind that LSH might also occasionally produce false positives (finding non-similar \\npoints as similar), and its effectiveness can vary based on the distance metric and data'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 3}, page_content=\"type. There are various LSH families designed to work with different metrics (e.g., \\nEuclidean distance, Jaccard similarity), which means LSH remains versatile. \\n \\nAnnoy \\nAnnoy (Approximate Nearest Neighbors Oh Yeah) isn't a single algorithm, but an open-\\nsource C++ library that uses its own algorithms for building and querying trees, without \\ndirectly implementing LSH or KD-trees. It's designed for memory-efficient and fast\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 3}, page_content=\"search in high-dimensional spaces, making it suitable for real-time queries. Essentially, \\nit’s a user-friendly interface offering flexibility for different data types and search \\nscenarios. Annoy's strength lies in leveraging multiple ANN approaches under one roof, \\nallowing you to choose the best fit for your needs. While it simplifies the process, \\nremember that picking the right internal algorithm within Annoy is crucial for optimal\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 3}, page_content='performance, and its effectiveness still depends on factors like your data and accuracy \\nrequirements.  \\n \\nLinear scan algorithm \\nAlthough not typically classified as an ANN technique, it’s worth mentioning linear scan \\nbecause it’s a brute-force approach that gives you similar results to other ANN \\nalgorithms. It iterates through every data point sequentially, calculating the distances'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 4}, page_content='between records and keeping track of the best matches. Because of the simplistic \\nnature of the algorithm, it’s easy to implement and great for small data sets. The \\ndownside of the more basic approach is that it’s inefficient for large data sets, slow \\nwhen used with high-dimensional data, and impractical for real-time applications. \\n \\nChoosing the right ANN \\nBefore you dive into picking an ANN, there are a few things you should consider before \\ndeciding: \\n•'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 4}, page_content='deciding: \\n• \\nData set size and dimensionality: Consider using locality-sensitive hashing for \\nlarge and high-dimensional data and KD-trees for smaller and lower-dimensional \\ndata. \\n• \\nDesired accuracy level: If absolute precision is vital, linear scan is likely the \\nbest option — otherwise, look at LSH or Annoy for good accuracy with speed. \\n• \\nComputational resources: Annoy offers flexibility, but consider memory and \\nprocessing limitations before choosing an algorithm within it.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 4}, page_content=\"Remember – there's no one-size-fits-all solution. Experiment with different ANN \\nalgorithms and evaluate their performance on your specific data to find the perfect \\nmatch for your vector search needs. Beyond these options, the world of ANN algorithms \\nis constantly evolving, so it’s also worth keeping an ear to the ground so you don’t miss \\nsomething new that could improve your search. \\n \\nANN is the secret sauce for better search\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 4}, page_content='ANN is the secret sauce for better search \\nThe vast, complex world of data demands efficient tools to navigate its labyrinths. This \\nis where ANN can be the secret sauce that takes your similarity search from good to \\ngreat. It offers speed and scalability, albeit at the cost of a slight accuracy compromise. \\nAnd there is ongoing research with developments being made weekly, which will all \\ncontribute to the dynamic nature of ANN space. For instance, advancements in'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 4}, page_content=\"quantum computing and machine learning could lead to new types of ANN algorithms \\nthat are even faster and more efficient. \\nWe've explored different ANN algorithms, each with its unique strengths and \\nweaknesses. But ultimately, the optimal choice depends on your specific needs. \\nConsider factors like data size, dimensionality, accuracy requirements, and resources. \\nExperiment, explore, and choose the right algorithm to get the most out of ANNs. From\"),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 4}, page_content='image search to fraud detection, these algorithms can make a huge difference, \\nrevealing hidden connections and empowering data-driven insights fast.'),\n",
       " Document(metadata={'producer': '', 'creator': 'Microsoft Word', 'creationdate': '2025-09-12T17:16:35+00:00', 'source': '../data/pdf_files/anns_algorithm.pdf', 'file_path': '../data/pdf_files/anns_algorithm.pdf', 'total_pages': 6, 'format': 'PDF 1.7', 'title': '', 'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)', 'subject': '', 'keywords': '', 'moddate': '2025-09-12T17:16:35+00:00', 'trapped': '', 'modDate': \"D:20250912171635+00'00'\", 'creationDate': \"D:20250912171635+00'00'\", 'page': 5}, page_content='So, the next time you search for the next song, movie, or video game, remember the \\nsilent heroes behind the scenes — the ANN algorithms — joining the dots and making \\nconnections.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents_recursive(documents, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Splits each document in the list into smaller chunks using RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of Document objects to split.\n",
    "        chunk_size (int): Maximum number of characters per chunk.\n",
    "        chunk_overlap (int): Number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of chunked Document objects.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunked_docs = splitter.split_documents(documents)\n",
    "    print(f\"{len(documents)} documents split into {len(chunked_docs)} chunks.\")\n",
    "\n",
    "    if chunked_docs:\n",
    "        print(\"Sample chunk:\")\n",
    "        print(f\"Content: {chunked_docs[0].page_content}\")\n",
    "        print(f\"Metadata: {chunked_docs[0].metadata}\")\n",
    "\n",
    "    return chunked_docs\n",
    "\n",
    "chunked_docs = split_documents_recursive(pdf_documents)\n",
    "chunked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d868d",
   "metadata": {},
   "source": [
    "### Embedding and VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9577b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b51f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Model loaded successfully. \n",
      "Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x132f17aa0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initializes the embedding model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained SentenceTransformer model from Hugging Face.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model() # Load the embedding model (all-MiniLM-L6-v2)\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name, token=False)\n",
    "            print(f\"Model loaded successfully. \\nEmbedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): List of text strings to be embedded.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Array of embeddings.\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings of shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "# Initialize the embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a51c6f",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4166449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB client initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x152ac8290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Handles vector store operations using ChromaDB\"\"\"\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initializes the ChromaDB client and collection.\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_client()\n",
    "\n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initializes the ChromaDB client and collection.\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create the collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"Collection of PDF document embeddings\"}\n",
    "            )\n",
    "            print(f\"ChromaDB client initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing ChromaDB client: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Any]): List of Document objects.\n",
    "            embeddings (np.array): Corresponding embeddings.\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "\n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        doc_texts = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate a unique ID for each document\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document text\n",
    "            doc_texts.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to ChromaDB collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                metadatas=metadatas,\n",
    "                documents=doc_texts,\n",
    "                embeddings=embeddings_list\n",
    "            )\n",
    "            print(f\"Succesfully added {len(documents)} documents to vector store.\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "         \n",
    "vector_store = VectorStore()\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d20eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 428 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 14/14 [00:01<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings of shape: (428, 384)\n",
      "Adding 428 documents to the vector store...\n",
      "Succesfully added 428 documents to vector store.\n",
      "Total documents in collection: 428\n"
     ]
    }
   ],
   "source": [
    "# Convert text to embeddings\n",
    "texts = [doc.page_content for doc in chunked_docs]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Store in vector DB\n",
    "vector_store.add_documents(documents=chunked_docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1815bb5",
   "metadata": {},
   "source": [
    "## Query-based Context Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea70a70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x152d64e00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles retrieval of relevant documents from the vector store based on a query.\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initializes the retriever with a vector store and embedding manager.\n",
    "        \n",
    "        Args:\n",
    "            vector_store (VectorStore): Instance of the VectorStore class.\n",
    "            embedding_manager (EmbeddingManager): Instance of the EmbeddingManager class.\n",
    "        \"\"\"\n",
    "        self.vectore_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve the top_k most relevant documents for the given query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The input query string.\n",
    "            top_k (int): Number of top relevant documents to retrieve.\n",
    "            score_threshold (float): Minimum similarity score to consider a document relevant.\n",
    "        \n",
    "            Returns:\n",
    "                List[Dict[str, Any]]: List of retrieved documents with metadata and scores.\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving top {top_k} documents for query: {query}\")\n",
    "        print(f\"Similarity Score threshold: {score_threshold}\")\n",
    "        print(f\"Generating embedding for the query...\")\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in the vector store\n",
    "        try:\n",
    "            results = self.vectore_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            retrieved_docs = []\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "            \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i+1\n",
    "                        })\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever = RAGRetriever(vector_store=vector_store, embedding_manager=embedding_manager)\n",
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9aabb1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top 5 documents for query: How Approximate Nearest Neighbor Search algorithm works?\n",
      "Similarity Score threshold: 0.0\n",
      "Generating embedding for the query...\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings of shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_7c7761d7_401',\n",
       "  'content': 'and identifies the closest thing to something you like, or something you’re searching for. \\nBut NN algorithms have an inherent flaw. If the amount of data they’re analyzing gets \\ntoo big, crawling through every option takes forever. This is a problem, especially as \\nthese data sources get bigger and bigger every year. This is where approximate nearest \\nneighbor (ANN) grabs the baton from NN and changes the game. \\nIn this document, we’ll cover the following key topics about ANN: \\n•',\n",
       "  'metadata': {'title': '',\n",
       "   'trapped': '',\n",
       "   'file_path': '../data/pdf_files/anns_algorithm.pdf',\n",
       "   'content_length': 485,\n",
       "   'doc_index': 401,\n",
       "   'creationdate': '2025-09-12T17:16:35+00:00',\n",
       "   'keywords': '',\n",
       "   'creationDate': \"D:20250912171635+00'00'\",\n",
       "   'modDate': \"D:20250912171635+00'00'\",\n",
       "   'creator': 'Microsoft Word',\n",
       "   'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)',\n",
       "   'format': 'PDF 1.7',\n",
       "   'source': '../data/pdf_files/anns_algorithm.pdf',\n",
       "   'subject': '',\n",
       "   'total_pages': 6,\n",
       "   'producer': '',\n",
       "   'moddate': '2025-09-12T17:16:35+00:00',\n",
       "   'page': 0},\n",
       "  'similarity_score': 0.36714863777160645,\n",
       "  'distance': 0.6328513622283936,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_7355424f_414',\n",
       "  'content': 'mean that ANN can play a critical role in unlocking the true potential of your vector \\nsearch. \\n \\nTypes of approximate nearest neighbor algorithms \\nWhile the concept of ANN offers a compelling speed advantage in search, this term \\nactually covers a diverse toolbox of algorithms. They all have their own strengths and \\ntrade-offs, and understanding these nuances is critical when choosing the right tool for \\nyour specific data and search needs.',\n",
       "  'metadata': {'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)',\n",
       "   'trapped': '',\n",
       "   'format': 'PDF 1.7',\n",
       "   'creationdate': '2025-09-12T17:16:35+00:00',\n",
       "   'creator': 'Microsoft Word',\n",
       "   'file_path': '../data/pdf_files/anns_algorithm.pdf',\n",
       "   'doc_index': 414,\n",
       "   'producer': '',\n",
       "   'creationDate': \"D:20250912171635+00'00'\",\n",
       "   'title': '',\n",
       "   'moddate': '2025-09-12T17:16:35+00:00',\n",
       "   'page': 2,\n",
       "   'content_length': 445,\n",
       "   'keywords': '',\n",
       "   'source': '../data/pdf_files/anns_algorithm.pdf',\n",
       "   'subject': '',\n",
       "   'modDate': \"D:20250912171635+00'00'\",\n",
       "   'total_pages': 6},\n",
       "  'similarity_score': 0.3571075201034546,\n",
       "  'distance': 0.6428924798965454,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_bb5036e1_229',\n",
       "  'content': '97-D(12):3142–3154, 2014.\\n[46] R. Weber, H.-J. Schek, and S. Blott. A quantitative\\nanalysis and performance study for similarity-search\\nmethods in high-dimensional spaces. In Proc. International\\nConference on Very Large DataBases, pages 194–205, 1998.\\n[47] P. Wieschollek, O. Wang, A. Sorkine-Hornung, and\\nH. P. A. Lensch. Eﬃcient large-scale approximate nearest\\nneighbor search on the GPU. In Proc. IEEE Conference on\\nComputer Vision and Pattern Recognition, pages\\n2027–2035, June 2016.',\n",
       "  'metadata': {'author': '',\n",
       "   'source': '../data/pdf_files/vector_database.pdf',\n",
       "   'trapped': '',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'creationDate': 'D:20170301012702Z',\n",
       "   'subject': '',\n",
       "   'doc_index': 229,\n",
       "   'content_length': 487,\n",
       "   'keywords': '',\n",
       "   'format': 'PDF 1.5',\n",
       "   'file_path': '../data/pdf_files/vector_database.pdf',\n",
       "   'creationdate': '2017-03-01T01:27:02+00:00',\n",
       "   'total_pages': 12,\n",
       "   'moddate': '2017-03-01T01:27:02+00:00',\n",
       "   'page': 11,\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'title': '',\n",
       "   'modDate': 'D:20170301012702Z'},\n",
       "  'similarity_score': 0.34978556632995605,\n",
       "  'distance': 0.650214433670044,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_7b42f883_193',\n",
       "  'content': 'ditional pass through D′) is at least 25% slower.\\nEﬃcient k-selection is even more important in situations\\nwhere approximate methods are used to compute distances,\\nbecause the relative cost of k-selection with respect to dis-\\ntance computation increases.\\n6.4\\nBillion-scale approximate search\\nThere are few studies on GPU-based approximate nearest-\\nneighbor search on large datasets (ℓ≫106). We report a\\nfew comparison points here on index search, using standard',\n",
       "  'metadata': {'creator': 'LaTeX with hyperref package',\n",
       "   'modDate': 'D:20170301012702Z',\n",
       "   'moddate': '2017-03-01T01:27:02+00:00',\n",
       "   'producer': 'pdfTeX-1.40.17',\n",
       "   'author': '',\n",
       "   'content_length': 461,\n",
       "   'page': 8,\n",
       "   'creationDate': 'D:20170301012702Z',\n",
       "   'file_path': '../data/pdf_files/vector_database.pdf',\n",
       "   'doc_index': 193,\n",
       "   'total_pages': 12,\n",
       "   'format': 'PDF 1.5',\n",
       "   'subject': '',\n",
       "   'trapped': '',\n",
       "   'creationdate': '2017-03-01T01:27:02+00:00',\n",
       "   'source': '../data/pdf_files/vector_database.pdf',\n",
       "   'title': '',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.3488594889640808,\n",
       "  'distance': 0.6511405110359192,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_95894a04_408',\n",
       "  'content': 'true closest neighbor (exact nearest neighbor search) holds value, it often comes at a \\ncomputational cost, as we’ve already talked about. This is where ANN search shines, \\noffering a compelling trade-off: lightning speed with high, but not absolute, accuracy. \\nBut when exactly should you choose ANN over other search methods? \\nExact nearest neighbor might be slow, but it’s the best option when accuracy is your \\npriority or you’re using small data sets. k-nearest neighbors (kNN) sits between NN',\n",
       "  'metadata': {'total_pages': 6,\n",
       "   'doc_index': 408,\n",
       "   'creator': 'Microsoft Word',\n",
       "   'author': 'Neehanth Reddy Maramreddy (nmrmrdd1)',\n",
       "   'creationdate': '2025-09-12T17:16:35+00:00',\n",
       "   'file_path': '../data/pdf_files/anns_algorithm.pdf',\n",
       "   'content_length': 498,\n",
       "   'moddate': '2025-09-12T17:16:35+00:00',\n",
       "   'subject': '',\n",
       "   'keywords': '',\n",
       "   'source': '../data/pdf_files/anns_algorithm.pdf',\n",
       "   'page': 1,\n",
       "   'title': '',\n",
       "   'modDate': \"D:20250912171635+00'00'\",\n",
       "   'format': 'PDF 1.7',\n",
       "   'producer': '',\n",
       "   'creationDate': \"D:20250912171635+00'00'\",\n",
       "   'trapped': ''},\n",
       "  'similarity_score': 0.33150285482406616,\n",
       "  'distance': 0.6684971451759338,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(query=\"How Approximate Nearest Neighbor Search algorithm works?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95b0f1",
   "metadata": {},
   "source": [
    "## Integrating Vectordb context with LLM (RAG Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "520f1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG pipeline with GROQ LLM\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the GROQ LLM\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "chat_llm = ChatGroq(\n",
    "    groq_api_key=groq_api_key, \n",
    "    model='gemma2-9b-it', \n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "# RAG function\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "    # Retreive the context\n",
    "    results = retriever.retrieve(query=query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "\n",
    "    if not context:\n",
    "        return \"No relevant context found\"\n",
    "    \n",
    "    # Generate answer using GROQ LLM\n",
    "    prompt = f\"\"\"You are an expert Q&A assistant. Your task is to answer the user's query based exclusively on the provided context.\n",
    "        Context: {context}\n",
    "        User's Query: {query}\n",
    "        ANSWER:\n",
    "        \"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de703ab",
   "metadata": {},
   "source": [
    "`Test questions`\n",
    "1. How does the HNSW algorithm work?\n",
    "2. What is the role of a document index in retrieval?\n",
    "3. How can you make a language model's knowledge up-to-date without retraining it?\n",
    "4. What is the Continuous Bag-of-Words Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3777641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top 3 documents for query: How Approximate Nearest Neighbor Search algorithm works?\n",
      "Similarity Score threshold: 0.0\n",
      "Generating embedding for the query...\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings of shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While the provided text explains the benefits and importance of Approximate Nearest Neighbor (ANN) search, it doesn't delve into the specifics of *how* these algorithms work.  \n",
      "\n",
      "It does mention that ANN algorithms are a \"diverse toolbox\" with different strengths and trade-offs.  To understand how they work, you'd need to explore resources that go into detail about specific ANN algorithms like:\n",
      "\n",
      "* **Locality-Sensitive Hashing (LSH)**\n",
      "* **k-d trees**\n",
      "* **Ball trees**\n",
      "* **Product Quantization** \n",
      "\n",
      "\n",
      "Let me know if you'd like me to try and find some resources explaining these algorithms in more detail. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\n",
    "    query=\"How Approximate Nearest Neighbor Search algorithm works?\",\n",
    "    retriever=rag_retriever,\n",
    "    llm=chat_llm\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc4feb",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26d97736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top 3 documents for query: How Approximate Nearest Neighbor Search algorithm works?\n",
      "Similarity Score threshold: 0.1\n",
      "Generating embedding for the query...\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings of shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Answer: While the provided text explains that Approximate Nearest Neighbor (ANN) algorithms offer a speed advantage over traditional Nearest Neighbor (NN) algorithms, it doesn't delve into the specifics of *how* they work.  \n",
      "\n",
      "The text does mention that ANN is a \"diverse toolbox of algorithms\" with different strengths and trade-offs.  It also highlights that understanding these nuances is crucial for choosing the right ANN algorithm for a specific task. \n",
      "\n",
      "\n",
      "To get a detailed explanation of how ANN algorithms work, you would need to consult additional resources. \n",
      "\n",
      "\n",
      "------------------------------\n",
      "Sources: [{'source': '../data/pdf_files/anns_algorithm.pdf', 'page': 0, 'score': 0.36714863777160645, 'preview': 'and identifies the closest thing to something you like, or something you’re searching for. \\nBut NN algorithms have an inherent flaw. If the amount of data they’re analyzing gets \\ntoo big, crawling through every option takes forever. This is a problem, especially as \\nthese data sources get bigger and...'}, {'source': '../data/pdf_files/anns_algorithm.pdf', 'page': 2, 'score': 0.3571075201034546, 'preview': 'mean that ANN can play a critical role in unlocking the true potential of your vector \\nsearch. \\n \\nTypes of approximate nearest neighbor algorithms \\nWhile the concept of ANN offers a compelling speed advantage in search, this term \\nactually covers a diverse toolbox of algorithms. They all have their ...'}, {'source': '../data/pdf_files/vector_database.pdf', 'page': 11, 'score': 0.34978556632995605, 'preview': '97-D(12):3142–3154, 2014.\\n[46] R. Weber, H.-J. Schek, and S. Blott. A quantitative\\nanalysis and performance study for similarity-search\\nmethods in high-dimensional spaces. In Proc. International\\nConference on Very Large DataBases, pages 194–205, 1998.\\n[47] P. Wieschollek, O. Wang, A. Sorkine-Hornung...'}]\n",
      "Confidence: 0.36714863777160645\n",
      "Context preview: and identifies the closest thing to something you like, or something you’re searching for. \n",
      "But NN algorithms have an inherent flaw. If the amount of data they’re analyzing gets \n",
      "too big, crawling through every option takes forever. This is a problem, especially as \n",
      "these data sources get bigger and\n"
     ]
    }
   ],
   "source": [
    "# Enhanced RAG Pipeline with additional features\n",
    "def rag_advance(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    Enhanced RAG Pipeline with additional features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    # Retreive the context\n",
    "    results = retriever.retrieve(query=query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {\n",
    "            'answer': 'No relevant context found.',\n",
    "            'sources': [],\n",
    "            'confidence': 0.0,\n",
    "            'context': ''\n",
    "        }\n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unkown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer using GROQ LLM\n",
    "    prompt = f\"\"\"You are an expert Q&A assistant. Your task is to answer the user's query based exclusively on the provided context.\n",
    "        Context: {context}\n",
    "        User's Query: {query}\n",
    "        ANSWER:\n",
    "        \"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "\n",
    "    output ={\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "\n",
    "    return output\n",
    "\n",
    "# Test\n",
    "result = rag_advance(\n",
    "    query=\"How Approximate Nearest Neighbor Search algorithm works?\",\n",
    "    retriever=rag_retriever,\n",
    "    llm=chat_llm,\n",
    "    top_k=3,\n",
    "    min_score=0.1,\n",
    "    return_context=True\n",
    ")\n",
    "print('-'*30)\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print('-'*30)\n",
    "print(f\"Sources: {result['sources']}\")\n",
    "print(f\"Confidence: {result['confidence']}\")\n",
    "print(f\"Context preview: {result['context'][:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd8dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-qa-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
